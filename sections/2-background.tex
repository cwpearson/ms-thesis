\chapter{Background}


%
%
%
\section{Communication Links}

\subsection{PCI}
\subsection{NVLink}
\subsection{NVLink2}
\subsection{QPI}
\subsection{X bus}
\subsection{CAPI}

%
%
%
\section{Programming Systems}
\subsection{CUDA}
\label{sec:cuda}

\subsubsection{CUDA Unified Memory}

CUDA Unified Memory~\cite{harris2013cudaunifiedmemory} provides a single pool of memory that is accessible from the CPU and GPU by a single pointer.
CUDA automatically migrates data between the physically distinct CPU and GPU memory as needed, allowing GPU kernels to access the memory as if it were in the global memory, and CPU functions to access the memory as if it were in the system memory.
This simplifies the programming model.

\outline{
    write-combining
    pageable / pinned
    managed memory
    mapped memory (zero-copy)
    direct access
    peer access
    unified virtual addressing
}

% \subsection{HSA}
% \label{sec:hsa}


\section{Profiling Tooling}

\subsection{CUPTI}
\label{sec:cupti}

\cite{nvidia2017cupti}


\subsection{\texttt{LD\_PRELOAD}}
\label{sec:ldpreload}

\cite{kerrisk2017ld}

