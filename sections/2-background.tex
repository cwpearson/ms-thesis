\chapter{Background}


%
%
%
\section{Communication Links and Protocols}

\subsection{PCI}
\subsection{NVLink}
\subsection{NVLink2}
\subsection{QPI}
\subsection{X bus}
\subsection{CAPI}

Coherent Accelerator Processor Interface (CAPI) is a processor expansion bus standard.


\subsection{OpenCAPI}

%
%
%
\section{Programming Systems}
\subsection{CUDA}
\label{sec:cuda}

\outline{
    write-combining
    pageable / pinned
    managed memory
    mapped memory (zero-copy)
    direct access
    peer access
    unified virtual addressing
}




% \subsection{HSA}
% \label{sec:hsa}
NVIDIA's CUDA (Compute Unified Device Architecture) is a programming system for enabling general-purpose computation on NVIDIA GPUs (graphics processing units).
CUDA is a set of C extensions and libraries for interfacing with GPUs.
NVIDIA provides a compiler, \texttt{nvcc} for generating CUDA-enabled binaries.

CUDA provides a set of runtime and driver APIs for the developer to manage the allocation and movement of data between the host and device memory.
From its inception, CUDA provided comprehensive APIs for developers to mitigate application performance shortfalls stemming from the relatively limited performance of host-device communication links.
As the capabilities of GPUs and host systems have improved, CUDA has provided simpler, higher-level APIs that require less programmer effort.
This section describes CUDA memory-management capabilities and the historical context of their introduction.

\todo{CUDA Contexts and compute capability}

\subsection{CUDA Memory}

Prior to the introduction of Unified Virtual Addressing(\ref{sec:uva}), the CUDA memory space was composed of multiple address spaces: one for the host, and one for each GPU~\cite{schroeder2011peer}.
Data was explicitly allocated in and moved between those address spaces through the basic CUDA memory management runtime calls described in Table~\ref{tab:cuda-basic-apis}.
Standard C/C++ memory allocation techniques (\texttt{malloc}, \texttt{new}) are used for managing memory on the host.

\begin{table}[h]
    \centering
    \caption[Basic CUDA Memory-management APIs]{\todo{long caption}}
    \label{tab:cuda-basic-apis}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{API}    & \textbf{Description}                                    \\ \hline
    \texttt{cudaSetDevice()} & Associate a device with the host thread. \\ \hline
    \texttt{cudaMalloc()}    & Allocate memory on the device.                 \\ \hline
    \texttt{cudaFree()}      & Free memory on the device.                     \\ \hline
    \texttt{cudaMemcpy()}    & Copy data between to,from,and between GPUs.    \\ \hline
    \end{tabular}
\end{table}

\texttt{cudaMalloc} is used to allocate memory that may be used in CUDA kernels.
\texttt{cudaMemcpy} is used to move data between the host and device, or between devices.
Since each device has a separate address space, the programmer explicitly instructs \texttt{cudaMemcpy} how to move data with \texttt{cudaMemcpyHostToDevice}, \texttt{cudaMemcpyDeviceToHost}, or \texttt{cudaMemcpyDeviceToDevice}.

This API definition imposed the following basic structure on all CUDA programs, which remains essentially unchanged as of the writing of this document.
\begin{enumerate}
    \item allocate memory on the host with \texttt{new}/\texttt{malloc}.
    \item initialize memory on the host
    \item allocate memory on the device with \texttt{cudaMalloc}
    \item copy initialized data from the host to the device with \texttt{cudaMemcpy(..., cudaMemcpyHostToDevice)}
    \item launch CUDA kernels
    \item copy results back to the host with \texttt{cudaMemcpy(..., cudaMemcpyDeviceToHost)}
    \item free CUDA allocations with \texttt{cudaFree}
\end{enumerate}

\todo{streams to overlap computation with communication}

\subsubsection{Page-Locked Memory}

The GPU uses direct memory access (DMA) to copy data to and from the host.
When \texttt{cudaMemcpy} is invoked, the CPU instructs the GPU to copy a chunk of memory from the host memory to the device memory (or vis-versa), without the CPU being involved.
The host must guarantee that the memory to be accessed by the GPU will not be paged-out during the copy.
First, \texttt{cudaMemcpy} copies the data from the application address space to a piece of page-locked memory in the system DRAM managed by the CUDA driver, and then the GPU copies that data into the GPU memory.
CUDA allows the host to directly make page-locked memory regions visible to the application through the page-locked memory APIs summarized in Table~\ref{tab:cuda-pinned-apis}.
When the application uses these APIs, the first copy from pageable host memory to page-locked host memory can be elided.
Section{\ref{sec:todo}} demonstrates the performance improvement from skipping this first copy.
Overuse of page-locked memory on the host will degrade overall application performance or even impact system stability if the host system is not able to page as needed.

\begin{table}[h]
    \centering
    \caption[CUDA Pinned Memory-Management APIs]{\todo{long caption}}
    \label{tab:cuda-pinned-apis}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{API}                & \textbf{Description} \\ \hline
    \texttt{cudaMallocHost()}   & allocate page-locked memory on the host\\ \hline
    \texttt{cudaHostAlloc()}    & cudaMallocHost with additional options \\ \hline
    \texttt{cudaFreeHost()}     & free page-locked memory on the host\\ \hline
    \texttt{cudaHostRegister()} & Page-lock a range of host memory \\ \hline
    \end{tabular}
\end{table}

\texttt{cudaHostAlloc} allows even more options, including the \texttt{cudaHostAllocPortable}, \texttt{cudaHostAllocMapped}, and \texttt{cudaHostAllocWriteCombined} flags.

%3.2.4.2
\texttt{cudaHostAllocWriteCombined} causes a pinned allocation to be write-combined.
Writes to write-combined memory may be delayed and combined in a buffer to reduce the number of memory accesses.
Additionally, the host may not cache this data in L1 or L2 cache, freeing up those resources for other applications. 
\todo{why only L1 and L2?}
Furthermore, coherency is not enforced, so data is not snooped on the PCIe bus, which can increase bandwidth by up to 40\%.
This type of allocation makes sense for data that is not frequently read by the CPU, for example, data written once by the CPU before being sent to a GPU.

%3.2.4.3 
\texttt{cudaHostAllocMapped} introduces the ability to map the host allocation (in the host address space) into the device's address space.
By using \texttt{cudaHostGetDevicePointer}, the pointer in the device address space may be obtained and directly used by CUDA kernels.
In this case, the programmer does not need to explicitly allocate memory or copy data to the device, nor does the programmer need to use streams to overlap computation with communication.

\texttt{cudaHostAllocPortable} allows all CUDA contexts to treat the memory as pinned, not just the context that performed the allocation.
This became the default with the introduction of unified virtual addressing. \todo{why not have this from the start?}

\subsubsection{Unified Virtual Addressing}

Unified Virtual Addressing was introduced with compute capability 2.0 GPUs \todo{roughly, Fermi+} and CUDA 4.0 on 64-bit systems.
The host DRAM and the DRAM of each GPU are mapped into disjoint subsections of a single unified address space.

With this enhancement, the \texttt{cudaMemcpyDefault} flag for \texttt{cudaMemcpy} instructs the CUDA system to automatically determine how to move data.
By examining the address of the memory in question, CUDA can determine where the memory resides and move it accordingly.
This simplifies the programmer's use of \texttt{cudaMemcpy}, as \texttt{cudaMemcpyDefault} may be used everywhere.
Additionally, all mapped allocations are autmatically portable between GPUs, not restricted to the GPU that was active at the time of the allocation.

\subsubsection{Unified Memory with SM 3.0+}

CUDA Unified Memory~\cite{harris2013cudaunifiedmemory} provides a single pool of memory that is accessible from the CPU and GPU by a single pointer.
CUDA automatically migrates data between the physically distinct CPU and GPU memory as needed, allowing GPU kernels to access the memory as if it were in the global memory, and CPU functions to access the memory as if it were in the system memory.
Like mapped memory, this removes the need for an explicit allocation on the device and an explicit data transfer.
A summary of unified memory APIs are shown in Table~\ref{tab:cuda-um-apis}

\begin{table}[h]
    \centering
    \caption[CUDA Unified Memory-Management APIs]{\todo{long caption}}
    \label{tab:cuda-um-apis}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{API}                & \textbf{Description} \\ \hline
    \texttt{cudaMallocManaged()}   & allocate a unified memory region \\ \hline
    \texttt{cudaGetDeviceProperties()} \todo{what's the flag} \\ \hline
    \end{tabular}
\end{table}

The underlying data is only present in one location on the system, and in principle, unified memory allocations are automatically migrated towards their most recent use.
Data will migrate between the host and GPU as it is accessed by each.
Data will not migrate on a GPU peer access.

\subsubsection{Unified Memory with SM 6.0+}

On devices with compute capability 6.0+, the unified memory system operates at page-granularity rather than allocation-granularity.
Pages will migrate between GPUs on peer accesses, and \texttt{cudaMallocManaged} may be used to allocate more memory than is physically present on the GPU.

\todo{unified memory peer access}

\subsection{Peer Access}

\subsubsection{Texture memory and CUDNN}
\todo{3.2.11}

\subsection{HSA}
\label{sec:hsa}


\section{Profiling Tooling}

\subsection{CUDA Profiling Tools Interface}
\label{sec:cupti}

The CUDA Profiling Tools Interface~\cite{nvidia2017cupti} (CUPTI) ``provides...detailed information about how applications are using the GPU in a system.''
Users may inject code into the entry and exit point of every CUDA C Runtime and CUDA Driver API function call.
Additionally, users may configure and query hardware and software event counters to get insight into the operation of the GPU and CUDA stack.
The event counters include instruction count, instruction throughput, memory loads/stores, memory throughput, cache hits/misses, branches and custom profile triggers.
Chapter~\ref{ch:app-char} describes how \todo{hwcomm-apptracer} uses CUPTI to record memory allocations, kernel arguments, and timestamps to build a model of the application execution.

\subsection{\texttt{LD\_PRELOAD}}
\label{sec:ldpreload}

LD\_PRELOAD~\cite{kerrisk2017ld} is a mechanism by which the ld linker will load additional user-specific shared objects before any others.
If a function definition is present in a pre-loaded shared object, it will override the implementation present in later objects.
When combined with dlsym()~\cite{kerrisk2017dlysm}, it can be used to inject code into the entry of library calls in dynamically-linked binaries.
Chapter~\ref{ch:app-char} describes how \todo{hwcomm-apptracer} uses LD\_PRELOAD to record special information about cuBLAS and cuDNN calls.

\cite{kerrisk2017ld}

\subsection{ Communication Paths}

\todo{why are there different paths}
\todo{How do you use these paths}