\chapter{Background}

This section describes the hardware systems that connect NVIDIA GPUs to host systems, and the communication capabilities exposed by the CUDA API.

%
%
%
\section{Communication Links and Protocols}

\subsection{PCI Express}

Peripheral Component Interconnect Express is a widely-adopted computer bus standard.
It has gone through many revisions, from 1.0a, to PCIe 3.0, presently used today, up to PCIe 5.0, announced in June 2017.

\subsection{NVLink}
\subsection{NVLink2}

\todo{QPI?}

\todo{X bus?}

\todo{CAPI?}
%Coherent Accelerator Processor Interface (CAPI) is a processor expansion bus standard.
%\subsection{OpenCAPI}

%
%
%
\section{Programming Systems}
\subsection{CUDA}
\label{sec:cuda}

% \subsection{HSA}
% \label{sec:hsa}
NVIDIA's CUDA (Compute Unified Device Architecture) is a programming system for enabling general-purpose computation on NVIDIA GPUs (graphics processing units).
CUDA is a set of C extensions and libraries for interfacing with GPUs.
NVIDIA provides a compiler, \texttt{nvcc} for generating CUDA-enabled binaries.

CUDA provides a set of runtime and driver APIs for the developer to manage the allocation and movement of data between the host and device memory.
From its inception, CUDA provided comprehensive APIs for developers to mitigate application performance shortfalls stemming from the relatively limited performance of host-device communication links.
As the capabilities of GPUs and host systems have improved, CUDA has provided simpler, higher-level APIs that require less programmer effort.
This section describes CUDA memory-management capabilities and the historical context of their introduction.

The CUDA API has an associated version number that defines which CUDA actions are made available by that API.
CUDA-capable hardware advertises a specific compute capability (CC) that defines what CUDA actions are supported by that GPU.
Although the CUDA API may expose particular capability, the GPU may need a sufficiently high CC to take advantage of it.
This section references both these version systems when discussing CUDA features.

\subsection{CUDA Memory}

Prior to the introduction of Unified Virtual Addressing (see Section~\ref{sec:uva}), the CUDA memory space was composed of multiple address spaces: one for the host, and one for each GPU~\cite{schroeder2011peer}.
Data was explicitly allocated in and moved between those address spaces through the basic CUDA memory management runtime calls described in Table~\ref{tab:cuda-basic-apis}.
Standard C/C++ memory allocation techniques (\texttt{malloc}/\texttt{new}, \texttt{free}/\texttt{new}) are used for managing memory on the host.

\begin{table}[h]
	\centering
	\caption[Basic CUDA Memory-management APIs]{\todo{long caption}}
	\label{tab:cuda-basic-apis}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{API}             & \textbf{Description}                        \\ \hline
		\texttt{cudaSetDevice()} & Associate a device with the host thread.    \\ \hline
		\texttt{cudaMalloc()}    & Allocate memory on the device.              \\ \hline
		\texttt{cudaFree()}      & Free memory on the device.                  \\ \hline
		\texttt{cudaMemcpy()}    & Copy data between to,from,and between GPUs. \\ \hline
	\end{tabular}
\end{table}

\texttt{cudaMalloc} is used to allocate memory that may be used in CUDA kernels.
\texttt{cudaMemcpy} is used to move data between the host and device, or between devices.
Since each device has a separate address space, the programmer explicitly instructs \texttt{cudaMemcpy} how to move data with \texttt{cudaMemcpyHostToDevice}, \texttt{cudaMemcpyDeviceToHost}, or \texttt{cudaMemcpyDeviceToDevice}.

This API definition imposed the following basic structure on all CUDA programs, which remains essentially unchanged as of the writing of this document.
\begin{enumerate}
	\item allocate memory on the host with \texttt{new}/\texttt{malloc}.
	\item initialize memory on the host
	\item allocate memory on the device with \texttt{cudaMalloc}
	\item copy initialized data from the host to the device with \texttt{cudaMemcpy(..., cudaMemcpyHostToDevice)}
	\item launch CUDA kernels
	\item copy results back to the host with \texttt{cudaMemcpy(..., cudaMemcpyDeviceToHost)}
	\item free CUDA allocations with \texttt{cudaFree}
\end{enumerate}

\todo{streams to overlap computation with communication}

\subsubsection{Page-Locked Memory}

The GPU uses direct memory access (DMA) to copy data to and from the host.
When \texttt{cudaMemcpy} is invoked, the CPU instructs the GPU to copy a chunk of memory from the host memory to the device memory (or vis-versa), without the CPU being involved.
The host must guarantee that the memory to be accessed by the GPU will not be paged-out during the copy.
First, \texttt{cudaMemcpy} copies the data from the application address space to a piece of page-locked memory in the system DRAM managed by the CUDA driver, and then the GPU copies that data into the GPU memory.
CUDA allows the host to directly make page-locked memory regions visible to the application through the page-locked memory APIs summarized in Table~\ref{tab:cuda-pinned-apis}.
When the application uses these APIs, the first copy from pageable host memory to page-locked host memory can be elided.
Section{\ref{sec:todo}} demonstrates the performance improvement from skipping this first copy.
Overuse of page-locked memory on the host will degrade overall application performance or even impact system stability if the host system is not able to page as needed.

\begin{table}[h]
	\centering
	\caption[CUDA Pinned Memory-Management APIs]{\todo{long caption}}
	\label{tab:cuda-pinned-apis}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{API}                & \textbf{Description}                    \\ \hline
		\texttt{cudaMallocHost()}   & allocate page-locked memory on the host \\ \hline
		\texttt{cudaHostAlloc()}    & cudaMallocHost with additional options  \\ \hline
		\texttt{cudaFreeHost()}     & free page-locked memory on the host     \\ \hline
		\texttt{cudaHostRegister()} & Page-lock a range of host memory        \\ \hline
	\end{tabular}
\end{table}

\texttt{cudaHostAlloc} allows even more options, including the \texttt{cudaHostAllocPortable}, \texttt{cudaHostAllocMapped}, and \texttt{cudaHostAllocWriteCombined} flags.

%3.2.4.2
\texttt{cudaHostAllocWriteCombined} causes a pinned allocation to be write-combined.
Writes to write-combined memory may be delayed and combined in a buffer to reduce the number of memory accesses.
Additionally, the host may not cache this data in L1 or L2 cache, freeing up those resources for other applications. 
\todo{why only L1 and L2?}
Furthermore, coherency is not enforced, so data is not snooped on the PCIe bus, which can increase bandwidth by up to 40\%.
This type of allocation makes sense for data that is not frequently read by the CPU, for example, data written once by the CPU before being sent to a GPU.

%3.2.4.3 
\texttt{cudaHostAllocMapped} introduces the ability to map the host allocation (in the host address space) into the device's address space.
By using \texttt{cudaHostGetDevicePointer}, the corresponding pointer in the device address space may be obtained and directly used by CUDA kernels.
In this case, the programmer does not need to explicitly allocate memory or copy data to the device, nor does the programmer need to use streams to overlap computation with communication.

\texttt{cudaHostAllocPortable} allows all CUDA contexts to treat the memory as pinned, not just the context that performed the allocation.
This became the default with the introduction of unified virtual addressing. \todo{why not have this from the start?}

\subsubsection{Unified Virtual Addressing}
\label{sec:uva}

Unified Virtual Addressing was introduced with compute capability 2.0 GPUs \todo{roughly, Fermi+} and CUDA 4.0 on 64-bit systems.
The host memory and the memory of each GPU are mapped into disjoint subsections of a single unified address space.

With this enhancement, the \texttt{cudaMemcpyDefault} flag for \texttt{cudaMemcpy} instructs the CUDA system to automatically determine how to move data.
By examining the address of the pointers passed to \texttt{cudaMemcpy}, CUDA determines where which device the memory resides on and moves it accordingly.
This simplifies the programmer's use of \texttt{cudaMemcpy}, as \texttt{cudaMemcpyDefault} may be used everywhere.
There is also no need to call \texttt{cudaHostGetDevicePointer}~(\cite{nvidia2018cuda}, 4.9, cudaHostGetDevicePointer) for mapped allocations.
Furthermore, all mapped allocations are autmatically portable between GPUs, not restricted to the GPU that was active at the time of the allocation~(\cite{nvidia2018cuda}, 4.10).

\subsubsection{Peer Access}

\texttt{cudaDeviceEnablePeerAccess()} allows peer-to-peer transfers through \texttt{cudaMemcpyPeer()} (\todo{true?}).
This initiates a DMA copy from one GPU to another, without involving the host.
Peer access was introduced with CUDA 4.0.

When combined with UVA, \texttt{cudaDeviceEnablePeerAccess()} allows a GPU to directly access allocations on another GPU.
For example, if GPU1 loads through a pointer to data on GPU0,the data will be directly fetched from GPU0 memory, at the cost of one PCIe transasction and one globably memory load, and be cached in the L2 of GPU0.
Direct peer access requires compute capability 2.0, CUDA 4.0,Fermi+, and 64-bit system~\cite{schroeder2011peer}.

\subsubsection{Unified Memory with CC 3.0+ (Kepler+)}





No page fault support, limited virtual space
bulk migration of all pages attached to current stream when kernel is launched
~\cite{sakharnykh2017unified}

\texttt{cudaDeviceSynchronize} required before host program can use output from GPU~\cite{nvidia2014cuda60}.

The CUDA unified memory system was introduced with CUDA 6.0 and requires a GPU with SM architecture of 3.0 or higher~\cite{nvidia2014cuda60}.
CUDA Unified Memory~\cite{harris2013cudaunifiedmemory} provides a single pool of memory that is accessible from the CPU and GPU by a single pointer.
CUDA automatically migrates data between the physically distinct CPU and GPU memory as needed, allowing GPU kernels to access the memory as if it were in the global memory, and CPU functions to access the memory as if it were in the system memory.
Like mapped memory, this simplifies programming by removing the need for spearate host and device allocations and explicit data transfers.
A summary of unified memory APIs are shown in Table~\ref{tab:cuda-um-apis}

\begin{table}[h]
	\centering
	\caption[CUDA Unified Memory-Management APIs]{\todo{long caption}}
	\label{tab:cuda-um-apis}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{API}                 & \textbf{Description}             \\ \hline
		\texttt{\_\_managed\_\_} & Defines a global variable in managed memory \\ \hline
		\texttt{cudaMallocManaged()} & allocate a unified memory region. \\ \hline
		\texttt{cudaGetDeviceProperties()} \todo{what's the flag} &  \\ \hline
		\texttt{cudaStreamAttachMemAsync()} & Attach a managed allocation to a stream, instead of globally.
	\end{tabular}
\end{table}

The underlying data is only present in one location on the system, and in principle, unified memory allocations are automatically migrated towards their most recent use.
When a kernel is launched, all pages attached to that kernel's stream are bulk migrated to the destination GPU.
When the host program touches a page, that page is migrated back to the system memory.
In Multi-GPU systems, data does not migrate between GPUs - all other GPUs receive peer mappings to the data, which is accessed over the PCIe bus (\cite{nvidia2014cuda60}, J.1.3).

Unified memory maintains coherence (i.e., all GPUs the CPU have the same view of unified memory values) by disallowing concurrent accesses to managed data, including concurrent access to distinct managed allocations (\cite{nvidia2014cuda60}, J.2.2.1).
The CPU may access managed allocations after GPU execution has completed, where ``GPU execution'' refers to activity in a particular stream for stream-attached memory, or whole-GPU otherwise.
For stream-attached memory, completion of GPU execution can be guaranteed by any stream-synchronizing call.
For whole-GPU memory, completion is guaranteed by stream synchronization when only one stream is executing on the GPU\footnote{e.g., \texttt{cudaStreamSynchronize}}, or by any call that is fully synchronous with respect to the host\footnote{e.g. \texttt{cudaDeviceSynchronize()}}.
The GPU is considered to be active even if it is not accessing managed data.
Concurrent inter-GPU accesses are allowed, as are concurrently-executing kernels on a single GPU (\cite{nvidia2014cuda60}, J.2.2.2).

\todo{Hints: cudaMemAvise(setReadMostly, gpuId): prefetch creates copy instead of moving data. Simultaneous reads okay.
Writes are expensive but allowed.}

\subsubsection{Unified Memory with CC 6.0+ (Pascal+)}

With CUDA 8 and For GPUs with CC 6.0+, GPU page faulting provides a more fine-grained data transfer mechanism~\cite{nvidia2017cuda80}.
Instead of moving all managed allocations to the GPU prior to a kernel launch, the GPU will fault if it accesses a page that is not in its memory.
The page is either migrated to the GPU to serve that access, or the page is mapped into the GPU address space to be accessed over the host-device interconnect.
Unlike Unified Memory with CC 3.0, pages can migrate between GPUs on peer accesses (\cite{nvidia2017cuda80}, J.1.4).
The GPU page faulting mechanism lifts all restrictions on simultenous access to data (\cite{nvidia2017cuda80}, J.2.2.1).

CC 6.0+ also brings 49-bit virtual addressing to cover the 48-bit virtual addressing of modern CPUs and the GPU memory.
This allows CUDA to support managed allocations larger than the GPU memory.
The total amount of managed allocations still cannot be larger than the system memory (\cite{nvidia2017cuda80}, J.1.3).

Available since CUDA 8.
Page fault support.
Extended virtual address space (48-bit).
On-demand migration on first touch.
Oversubscription, system-wide atomics.
~\cite{sakharnykh2017unified}.

Hints:

\texttt{cudaMemAdvise(setAccessedBy, gpuId)}: GPU establish direct mapping of data in CPU memory, no page faults generated.
Access counters eventually trigger migration.

\subsubsection {Unified Memory with CC 7.0+ (Volta+) }

Access counters, hot pages moved to GPU.
CPU can directly access and cache GPU memory, native CPU-GPU atomics.
~\cite{sakharnykh2017unified}.

Prefetching.
prefetch whole regions if pages accesses densely.

Thrashing migrations (avoid frequent migration of shared pages).

Eviction (what pages to evict when new ones needed).

\texttt{cudaMemAdvise(setAccessedBy, gpuId)}: GPU establihs direct mapping of data in CPU memory, no page faults generated.
Access counters eventually trigger migration.

\todo{coherence traffic}

\todo{memory thrashing}

\todo{page throttling}

\todo{remote map}

On devices with compute capability 6.0+, the unified memory system operates at page-granularity rather than allocation-granularity.
Pages will migrate between GPUs on peer accesses, and \texttt{cudaMallocManaged} may be used to allocate more memory than is physically present on the GPU.


\subsubsection{Atomic Operations prior to CC 6.0}

Beginning with CC 1.1 and continuing through CC 2.0, CUDA introduced an increasingly complete set of atomic operations, culminating in support for common 32-bit and 64-bit operations on integers and floating-point types in global and shared memory (\cite{nvidia2008cuda20} C.1)
All of these atomic operations were atomic with respect only to the executing GPU.

\subsubsection{Atomic Operations with CC 6.0+}

Compute Capability 6.0 brings two new kind of atomics operations: system atomics (\texttt{atomic\*\_system}) and block atomics (\texttt{atomic\*\_block}) (\cite{nvidia2017cuda80}, B.12).
System atomics are atomic with respect to all CPUs and GPUs in the system.
Block atomics are atomic only with respect to other threads in the thread block.

\subsubsection{Parking Lot}

\todo{cudaMemcpy and managed memory (J2.2.1)?}

\todo{Texture memory?}
%\subsubsection{Texture memory and CUDNN}
%\todo{3.2.11}

\subsection{HSA}
\label{sec:hsa}


\section{Profiling Tooling}

\subsection{CUDA Profiling Tools Interface}
\label{sec:cupti}

The CUDA Profiling Tools Interface~\cite{nvidia2017cupti} (CUPTI) ``provides...detailed information about how applications are using the GPU in a system.''
Users may inject code into the entry and exit point of every CUDA C Runtime and CUDA Driver API function call.
Additionally, users may configure and query hardware and software event counters to get insight into the operation of the GPU and CUDA stack.
The event counters include instruction count, instruction throughput, memory loads/stores, memory throughput, cache hits/misses, branches and custom profile triggers.
Chapter~\ref{ch:app-char} describes how \todo{hwcomm-apptracer} uses CUPTI to record memory allocations, kernel arguments, and timestamps to build a model of the application execution.

\subsection{\texttt{LD\_PRELOAD}}
\label{sec:ldpreload}

LD\_PRELOAD~\cite{kerrisk2017ld} is a mechanism by which the ld linker will load additional user-specific shared objects before any others.
If a function definition is present in a pre-loaded shared object, it will override the implementation present in later objects.
When combined with dlsym()~\cite{kerrisk2017dlysm}, it can be used to inject code into the entry of library calls in dynamically-linked binaries.
Chapter~\ref{ch:app-char} describes how \todo{hwcomm-apptracer} uses LD\_PRELOAD to record special information about cuBLAS and cuDNN calls.

\cite{kerrisk2017ld}

\subsection{ Communication Paths}

\todo{why are there different paths}
\todo{How do you use these paths}