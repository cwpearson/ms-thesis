\chapter{System Characterization}
\label{ch:sys-char}

Chapter~\ref{ch:introduction} introduced the objective of characterizing the communication performance of heterogeneous systems.
Section~\ref{sec:sys-abstraction} described how applications interact with an abstraction presented by the system hardware, operating system, and software.
This chapter describes initial efforts towards a bottom-up approach to characterizing communication system performance that attempts to expose the communication capabilities exposed by the hardware.
Chapters~\ref{ch:explicit}~and~\ref{ch:unified} describe results obtained from a top-down approach, where communication abilities exposed to the application are characterized.

\section{Physical and Logical Communication Paths}

Consider the four different views of the IBM ``Minsky'' machine shown in Figure~\ref{fig:minsky-hardware-logical} (Section~\ref{sec:minsky} describes the machine in more detail).
Figure~\ref{fig:minsky-topo-hardware} shows an annotated topographical view of the hardware.
Figure~\ref{fig:minsky-topo-cuda} represents the connectivity of the components through the CUDA API.
Figure~\ref{fig:minsky-topo-numa} represents the Linux NUMA view of the system.
Figure~\ref{fig:minsky-topo-logical} shows more detailed communication capabilities between allocations on those components.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky-topo-hardware.pdf}
        \caption{}
        \label{fig:minsky-topo-hardware}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky-topo-cuda.pdf}
        \caption{}
        \label{fig:minsky-topo-cuda}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky-topo-numa.pdf}
        \caption{}
        \label{fig:minsky-topo-numa}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky-topo-logical.pdf}
        \caption{}
        \label{fig:minsky-topo-logical}
    \end{subfigure}
    \caption[\todo{short}]{
        Connectivity and components on Minsky at (a) the hardware level, (b) through the CUDA API, (c) through NUMA, and (d) when CUDA and numa are considered together.
    }
    \label{fig:minsky-hardware-logical}
\end{figure}


High-performance data movement in heterogeneous systems requires understanding properties of the logical communication capabilities exposed by the system.
The real-world properties of these communication capabilities depends on 
\begin{enumerate}
    \item Performance of the logical communication paths provided by the system.
    \item How the application uses those logical communication paths.
\end{enumerate}

With full knowledge of link properties it is possible to derive an accurate model of the first two elements on this list, that approach has three key barriers
\begin{itemize}
    \item Detailed link hardware properties are not available, e.g., when the link provides a competitive advantage for an OEM.
    \item Detailed link software properies are not available, e.g., when the drivers are proprietary.
    \item Even if a link is pysically present on the system, it may not be available to the application {e.g., due to bugs in the system configuration}
\end{itemize}
Instead of deriving a model of link performance from the ``first principles'' of link properties, this work attempts to generate an empirical model of performance of data movement in the system.

Though data movement between many different system components is possible, this work focuses on CPU-CPU and CPU-GPU data transfers.
Section~\ref{sec:hardware-model} describes an overview of the system model.
Section~\ref{sec:hardware-enumeration} describes a method for discovering data sources, sinks, and communication paths in a system.

\section{Hardware Model}
\label{sec:hardware-model}

The hardware is represented by a graph $G_s = \{E,V\}$ where $E$ is a set of edges representing communication links, and $V$ is a set of vertices representing communication endpoints, or data routing elements.
Sections~\ref{sec:system-vertices} and \ref{sec:system-edges} describe the specific system components explored.
Associated with each edge is a performance model function $M: C \rightarrow P$ that maps communication $C$ to an achievable performance $P$.
For example, a simple model $M$ would have $C$ parameterized by transfer size, and produce $P$ by considering link latency and bandwidth.
A more complicated model might also consider contention on the link.

Each vertex in $V$ is a data routing element.
These vertices are able to receive and re-transmit data on any of their links.
A PCIe hub \todo{cite PCIe discussion} is an example of a pure data-routing vertex.
Optionally, the vertex may serve as a communication endpoint; a source or a sink for data.
Processing elements and data storage elements serve as communication endpoints.

\texttt{hwcomm}~\cite{pearson2018hwcomm} is an open-source tool developed for automated hardware topology enumeration.
This tool can be executed on a target system to generate $G_s$ for that system.
Presently, the tool has only been tested on Linux operating systems.



%
% SECTION
%
\section{\texttt{hwcomm} Topology Enumeration}
\label{sec:hardware-enumeration}

The first step in generating $G_s$ is to discovere the topology.
hwcomm builds an in-memory representation of $G_s$, with sufficient information to later access the device during characterization.
Enumeration is done in several phases.

\textbf{Enumerate and Link CPU Sockets:}

First, the Portable Hardware Locality~\cite{broquedis2010hwloc} (hwloc) library is used to enumerate the present CPU sockets.
As the test systems only have two sockets, all discovered sockets are considered to be directly connected by an SMP bus for the appropriate system type.
The sockets and SMP buses are added to $G_s$.

\textbf{Enumerate PCI devices:}

Next, hwloc is used to traverse the PCI device tree.
All PCI devices are added to $G_s$ and connected with PCI links of the appropriate type.
Most attached storage, networking, and computing components are assigned an address in the PCI system and are discoverable in this step.

\textbf{Update GPUs to NVIDIA GPUs as appropriate:}

Next, the NVIDIA Management Library~\cite{nvidia2017nvml} (NVML) is used to enumerate all NVIDIA GPUs.
Those GPUs are matched by PCI address with existing PCI devices previously added to $G_s$, and NVML is used to discover whether NVLink is supported on each GPU and which other device that NVLink terminates at.
THis information is not provided by hwloc.
The edges associated with those NVLinks are added to $G_s$.


\textbf{Enumerate Linux block devices:}

Linux block devices are enumerated through \todo{more detail} and added to $G_s$.

\textbf{Enumerate Linux network devices:}

Finally, network devices are enumerated through \todo{more detail} and added to $G_s$.

\subsection{Vertex Types}
\label{sec:system-vertices}

Table~\ref{tab:topology-vertices} summarizes the types of data routers discovered by hwcomm.
These make up the vertices of $G_s$.

\begin{table}[]
    \centering
    \caption[Discoverable vertex types]{
        A summary of the types of data routers that can be discovered by \texttt{hwcomm}.
        Some components may also serve as data endpoints.
        }
    \label{tab:topology-vertices}
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hardware}       & \textbf{Data Endpoint} & \textbf{Description} \\ \hline
    CPU Socket              & \checkmark            &                \\ \hline
    PCI Device              & \checkmark            &                \\ \hline
    PCIe Hostbridge         &                       &                \\ \hline
    PCIe Bridge             &                       &                \\ \hline
    CUDA GPU                & \checkmark            &                \\ \hline
    Linux Block Device      & \checkmark            &                \\ \hline
    Linux Network Interface & \checkmark            &                \\ \hline
    \end{tabular}
\end{table}

\subsection{Edge Types}
\label{sec:system-edges}

In $G_s$, the vertices are connected by the discoverable edge types shown in Table~\ref{tab:topology-edges}.

\begin{table}[]
    \centering
    \caption[Discoverable edge types]{
        A summary of the types of communication links that can be discovered by \texttt{hwcomm}.
        Some components may also serve as data endpoints.
    }
    \label{tab:topology-edges}
    \begin{tabularx}{\linewidth}{ |c | >{\centering\arraybackslash}X |}
    \hline
    \textbf{Edge Type} & \textbf{Description} \\ \hline
    SMP Bus            & A symmetric multiprocessing bus connecting two CPU sockets. \\ \hline
    PCIe Bus           & A PCIe link connecting a PCIe Bidge and PCIe device or PCIe Hostbridge and PCIe bridge. \\ \hline
    NVLink1            & A first-generation NVLink connecting two NVIDIA GPUs or an NVIDIA GPU and CPU \\ \hline
    NVLink2            & A second-generation NVLink connecting two NVIDIA GPUs or an NVIDIA GPU and CPU \\ \hline
    SATA bus           & A Serial AT Attachment link conneting a host bus to a mass storage device. \\ \hline
    \end{tabularx}
\end{table}



