\chapter{System Characterization}
\label{ch:sys-char}

This chapter describes and approach to produce an empirical performance model of hardware when an application invokes communication activities through the CUDA API functions.
This performance model is needed for understanding the measured performance results to be presented in the rest of this thesis.
In particular, this chapter motivates a joint performance model of software abstractions and underlying hardware.
It then describes an approach for enumerating hardware components and connections.

\section{Joint Abstraction and Hardware Model}
\label{sec:abstraction-hardware}

Figure~\ref{fig:minsky-abstraction} shows two different communication abstractions of S822LC.
Figure~\ref{fig:minsky-topo-numa} represents the Linux NUMA view of the system.
As described in Section~\ref{sec:numa}, this view is accessible to the application through the libnuma library.
Figure~\ref{fig:minsky-topo-cuda} represents the connectivity of the components through the CUDA API.
Nodes in the graphs represent data storage locations or compute elements, and edges in the graph represent logical communication paths considered in this work.
Both NUMA and CUDA present a different abstraction, which is again different from the system layout in Section~\ref{sec:s822lc}.
In practice, the logical communication paths available to the system is the union of these abstractions (and any other abstraction made available by the system).

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky-topo-numa.pdf}
        \caption{}
        \label{fig:minsky-topo-numa}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky-topo-cuda.pdf}
        \caption{}
        \label{fig:minsky-topo-cuda}
    \end{subfigure}
    \caption[Communication topologies exposed to application]{
        Communication Topologies exposed to the application.
        (a) shows the abstraction presented by NUMA.
        (b) shows the abstraction presented by CUDA.
    }
    \label{fig:minsky-abstraction}
\end{figure}

Despite many of the logical communication paths using the same physical links, they achieve different performance on those links.
As demonstrated in Chapters~\ref{ch:explicit}~and~\ref{ch:unified}, some aspects of the empirical performance are determined by properties of the hardware links, while others are not.
A communication performance model must therefore not only be based off of the empirical performance of the logical links, but also incorporate understanding of the underlying hardware, if the model is to be applicable to more than just the system it was developed on.

Ideally, while empirical communication performance is characterized, the mapping to underlying hardware should be automatically established.
This work takes that mapping as a-priori, but the following section presents initial efforts to automate the process.

\section{Topology Enumeration}
\label{sec:hardware-enumeration}

This work proposes a two-step approach to establishing a mapping from logical communication paths to underlying hardware.
\begin{enumerate}
    \item Generate a graph $G_s$ of the hardware
    \item Observe hardware utilization while exercising logical communication paths
\end{enumerate}
The hardware is represented by a graph $G_s = \{E,V\}$ where $E$ is a set of edges representing communication links, and $V$ is a set of vertices representing communication endpoints, or data routing elements.
Sections~\ref{sec:system-vertices} and \ref{sec:system-edges} describe the specific system components explored.
Each vertex in $V$ is a data routing element.
These vertices are able to receive and re-transmit data on any of their links.
A PCIe switch is an example of a pure data-routing vertex.
Optionally, the vertex may serve as a communication endpoint; a source or a sink for data.
Processing elements and data storage elements serve as communication endpoints.

\texttt{hwcomm}~\cite{pearson2018hwcomm} is an open-source tool developed for automated hardware topology enumeration.
This tool can be executed on a target system to generate $G_s$ for that system.
Thought \texttt{hwcomm} relies largely on exploring the PCIe device tree, it also uses information provided by the Nvidia Management Library~\cite{nvidia2017nvml} (NVML) to discover NVLink devices, and build a graph of hardware components.
The first step of generating $G_s$ is to discover the hardware components and connections through a multi-stage process.

\textbf{Stage 1: Enumerate and Link CPU Sockets:}

The Portable Hardware Locality~\cite{broquedis2010hwloc} (hwloc) library is used to enumerate the present CPU sockets.
As the test systems only have two sockets, all discovered sockets are considered to be directly connected by an SMP bus for the appropriate system type.
The sockets and SMP buses are added to $G_s$.

\textbf{Stage 2: Enumerate PCI devices:}

The hwloc library is used to traverse the PCI device tree.
All PCI devices are added to $G_s$ and connected with PCI links of the appropriate type.
Most attached storage, networking, and computing components are assigned an address in the PCI system and are discoverable in this step.

\textbf{Stage 3: Update GPUs to Nvidia GPUs as appropriate:}

Next, NVML is used to enumerate all Nvidia GPUs.
The GPUs are matched by PCI address with existing PCI devices previously added to $G_s$, and NVML is used to discover whether NVLink is supported on each GPU and which other devices are connected to the GPU through NVLinks.
This information is not provided by hwloc.
The edges associated with the NVLinks are added to $G_s$.


\subsection{Vertex Types}
\label{sec:system-vertices}

Table~\ref{tab:topology-vertices} summarizes the types of data routers discovered by hwcomm.
These make up the vertices of $G_s$.

\begin{table}[ht]
    \centering
    \caption[Discoverable vertex types]{
        A summary of the types of data routers that can be discovered by \texttt{hwcomm}.
        Some components may also serve as data endpoints.
        }
    \label{tab:topology-vertices}
    \begin{tabular}{ccc}
    \hline
    \textbf{Hardware}       & \textbf{Data Router} & \textbf{Data Endpoint} \\ \hline
    CPU Socket              & \checkmark             & \checkmark             \\ \hline
    PCI Device              & \checkmark             & \checkmark             \\ \hline
    PCIe Hostbridge         & \checkmark             & $\times$               \\ \hline
    PCIe Bridge             & \checkmark             & $\times$               \\ \hline
    CUDA GPU                & \checkmark             & \checkmark             \\ \hline
    Linux Block Device      & \checkmark             & \checkmark             \\ \hline
    Linux Network Interface & \checkmark             & \checkmark             \\ \hline
    \end{tabular}
\end{table}

\subsection{Edge Types}
\label{sec:system-edges}

In $G_s$, the vertices are connected by the discoverable edge types shown in Table~\ref{tab:topology-edges}.

\begin{table}[ht]
    \centering
    \caption[Discoverable edge types]{
        A summary of the types of communication links that can be discovered by \texttt{hwcomm}.
        Some components may also serve as data endpoints.
    }
    \label{tab:topology-edges}
    \begin{tabularx}{\linewidth}{ c  >{\centering\arraybackslash}X }
    \hline
    \textbf{Edge Type} & \textbf{Description} \\ \hline
    SMP Bus            & A symmetric multiprocessing bus connecting two CPU sockets. \\ \hline
    PCIe Bus           & A PCIe link connecting a PCIe Bidge and PCIe device or PCIe Hostbridge and PCIe bridge. \\ \hline
    NVLink1            & A first-generation NVLink connecting two Nvidia GPUs or an Nvidia GPU and CPU \\ \hline
    NVLink2            & A second-generation NVLink connecting two Nvidia GPUs or an Nvidia GPU and CPU \\ \hline
    SATA bus           & A Serial AT Attachment link conneting a host bus to a mass storage device. \\ \hline
    \end{tabularx}
\end{table}

\subsection{Discovered Topologies}

The topologies of the S822LC, AC922, and DGX-1 systems are show in Appendix~\ref{ch:full-topos}.


\subsection{Logical Path to Hardware Link Mapping}
\label{sec:logical-hardware-mapping}

Once the system graph is established, the mapping between logical communication paths and system graph vertices and edges can be established by observing performance counters while benchmarking logical paths.
For example, NVML provides access to NVLink performance counters.
As known quantities of data are moved across the logical connections, the hardware link performance counters can be observed to associate logical transfers with traffic across physical links.

In this work, automatic determination of the mapping is not considered, instead, the mapping for the case study systems is known ahead of time.