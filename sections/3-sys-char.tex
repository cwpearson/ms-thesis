\chapter{System Characterization}
\label{ch:sys-char}

Section~\ref{sec:sys-abstraction} described how applications interact with an abstraction of the system hardware presented by the operating system, and software.
This chapter describes the approach for producing an empirical performance model of that performance.
Chapters~\ref{ch:explicit}~and~\ref{ch:unified} present the results of a ``top-down'' characterization the logical communication capabilities presented by the system abstraction.
Understanding the results of that characterization requires information about how the logical communication exercises the underlying hardware.
To that end, this chapter is organized as follows:
\begin{itemize}
    \item Section~\ref{sec:abstraction-hardware} motivates a joint performance model of software abstractions and underlying hardware.
    \item Section~\ref{sec:hardware-enumeration} describes an approach for enumerating hardware components and connections.
\end{itemize}

\section{Joint Abstraction and Hardware Model}
\label{sec:abstraction-hardware}

Figure~\ref{fig:minsky-abstraction} shows two different communication abstractions of S822LC.
Figure~\ref{fig:minsky-topo-numa} represents the Linux NUMA view of the system.
Figure~\ref{fig:minsky-topo-cuda} represents the connectivity of the components through the CUDA API.
Nodes in the graphs represent data storage locations or compute elements, and edges in the graph represent logical communication paths considered in this work.
Both NUMA and CUDA present a different abstraction, which is again different from the system layout in Section~\ref{sec:s822lc}.
In practice, the logical communication paths available to the system is the union of these abstractions (and any other abstraction made available by the system).

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky-topo-numa.pdf}
        \caption{}
        \label{fig:minsky-topo-numa}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky-topo-cuda.pdf}
        \caption{}
        \label{fig:minsky-topo-cuda}
    \end{subfigure}
    \caption[Communication Topologies Exposed to Application]{
        Communication Topologies exposed to the application.
        (a) shows the abstraction presented by NUMA.
        (b) shows the abstraction presented by CUDA.
    }
    \label{fig:minsky-abstraction}
\end{figure}

Despite many of the logical communication paths using the same physical links, they achieve different performance on those links.
As demonstrated in Chapters~\ref{ch:explicit}~and~\ref{ch:unified}, some aspects of the empirical performance are determined by properties of the hardware links, while others are not.
A communication performance model must therefore be based off of the empirical performance of the logical links, but also incorporate understanding of the underlying hardware, if the model is to be applicable to more than just the system it was developed on.

Ideally, while empirical communication performance is characterized, the mapping to underlying hardware should be automatically established.
This work takes that mapping as a-priori, but the following section presents initial efforts to automate the process.

\section{Topology Enumeration}
\label{sec:hardware-enumeration}

This work proposes a two-step approach to establishing a mapping from logical communication paths to underlying hardware.
\begin{enumerate}
    \item Generate a graph $G_s$ of the hardware
    \item Observe hardware utilization while exercising logical communication paths
\end{enumerate}
The hardware is represented by a graph $G_s = \{E,V\}$ where $E$ is a set of edges representing communication links, and $V$ is a set of vertices representing communication endpoints, or data routing elements.
Sections~\ref{sec:system-vertices} and \ref{sec:system-edges} describe the specific system components explored.
Each vertex in $V$ is a data routing element.
These vertices are able to receive and re-transmit data on any of their links.
A PCIe switch is an example of a pure data-routing vertex.
Optionally, the vertex may serve as a communication endpoint; a source or a sink for data.
Processing elements and data storage elements serve as communication endpoints.

\texttt{hwcomm}~\cite{pearson2018hwcomm} is an open-source tool developed for automated hardware topology enumeration.
This tool can be executed on a target system to generate $G_s$ for that system.
Thought \texttt{hwcomm} relies largely on exploring the PCIe device tree, it also uses information provided by the NVIDIA Management Library~\cite{nvidia2017nvml} (NVML) to discover NVLink devices, and build a graph of hardware components.
The first step of generating $G_s$ is to discover the hardware components and connections through a multi-stage process.

\textbf{Enumerate and Link CPU Sockets:}

First, the Portable Hardware Locality~\cite{broquedis2010hwloc} (hwloc) library is used to enumerate the present CPU sockets.
As the test systems only have two sockets, all discovered sockets are considered to be directly connected by an SMP bus for the appropriate system type.
The sockets and SMP buses are added to $G_s$.

\textbf{Enumerate PCI devices:}

Next, hwloc is used to traverse the PCI device tree.
All PCI devices are added to $G_s$ and connected with PCI links of the appropriate type.
Most attached storage, networking, and computing components are assigned an address in the PCI system and are discoverable in this step.

\textbf{Update GPUs to NVIDIA GPUs as appropriate:}

Next, NVML is used to enumerate all NVIDIA GPUs.
Those GPUs are matched by PCI address with existing PCI devices previously added to $G_s$, and NVML is used to discover whether NVLink is supported on each GPU and which other device that NVLink terminates at.
THis information is not provided by hwloc.
The edges associated with those NVLinks are added to $G_s$.


\subsection{Vertex Types}
\label{sec:system-vertices}

Table~\ref{tab:topology-vertices} summarizes the types of data routers discovered by hwcomm.
These make up the vertices of $G_s$.

\begin{table}[ht]
    \centering
    \caption[Discoverable vertex types]{
        A summary of the types of data routers that can be discovered by \texttt{hwcomm}.
        Some components may also serve as data endpoints.
        }
    \label{tab:topology-vertices}
    \begin{tabular}{cc}
    \hline
    \textbf{Hardware}       & \textbf{Data Endpoint} \\ \hline
    CPU Socket              & \checkmark             \\ \hline
    PCI Device              & \checkmark             \\ \hline
    PCIe Hostbridge         & $\times$               \\ \hline
    PCIe Bridge             & $\times$               \\ \hline
    CUDA GPU                & \checkmark             \\ \hline
    Linux Block Device      & \checkmark             \\ \hline
    Linux Network Interface & \checkmark             \\ \hline
    \end{tabular}
\end{table}

\subsection{Edge Types}
\label{sec:system-edges}

In $G_s$, the vertices are connected by the discoverable edge types shown in Table~\ref{tab:topology-edges}.

\begin{table}[ht]
    \centering
    \caption[Discoverable edge types]{
        A summary of the types of communication links that can be discovered by \texttt{hwcomm}.
        Some components may also serve as data endpoints.
    }
    \label{tab:topology-edges}
    \begin{tabularx}{\linewidth}{ c  >{\centering\arraybackslash}X }
    \hline
    \textbf{Edge Type} & \textbf{Description} \\ \hline
    SMP Bus            & A symmetric multiprocessing bus connecting two CPU sockets. \\ \hline
    PCIe Bus           & A PCIe link connecting a PCIe Bidge and PCIe device or PCIe Hostbridge and PCIe bridge. \\ \hline
    NVLink1            & A first-generation NVLink connecting two NVIDIA GPUs or an NVIDIA GPU and CPU \\ \hline
    NVLink2            & A second-generation NVLink connecting two NVIDIA GPUs or an NVIDIA GPU and CPU \\ \hline
    SATA bus           & A Serial AT Attachment link conneting a host bus to a mass storage device. \\ \hline
    \end{tabularx}
\end{table}

\subsection{Discovered Topologies}

The topologies of the S822LC, AC922, and DGX-1 systems are show in Appendix~\ref{ch:full-topos}.


\subsection{Logical Path to Hardware Link Mapping}
\label{sec:logical-hardware-mapping}

Once the system graph is established, the mapping between logical communication paths and system graph vertices and edges can be established by observing performance counters while benchmarking logical paths.
For example, NVML provides access to NVLink performance counters.
As known quantities of data are moved across the logical connections, hardware link activity can be observed to determine which links are used.
This work takes the mapping as a-priori knowledge, but automatically determining this mapping is future work.