\chapter{System Characterization}
\label{ch:sys-char}

High-performance data movement in heterogeneous systems requires information about the properties of the communication links between system storage and compute components.
Although specifications of system components are often available\todo{cite some examples}, the real-world properties of these links depends on how applications use the links, and whether or not the links are shared between system components.
\todo{For example, Figure~\ref{fig:actual-perf} shows modeled and achieved cuda memcpy bandwidth.}
With full knowledge of link properties it is possible to derive an accurate model of link performance, that approach has two key barriers
\begin{itemize}
    \item Detailed link hardware properties are not available, e.g., when the link provides a competitive advantage for an OEM.
    \item Detailed link software properies are not available, e.g., when the drivers are proprietary.
    \item Even if a link is pysically present on the system, it may not be available to the application {e.g., due to bugs in the system configuration}
\end{itemize}
Instead of deriving a model of link performance from the ``first principles'' of link properties, this work attempts to generate an empirical model of performance of data movement in the system.
Section~\ref{sec:system-model} describes an overview of the system model.
Section~\ref{sec:topology-exploration} describes a method for discovering data sources, sinks, and communication paths in a system.
Section~\ref{sec:link-char} describes the approach to characterize communication links.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth,draft]{../figures/actual-cuda-memcpy.png}
    \caption[\todo{short}]{\todo{long}}
    \label{fig:actual-cuda-memcpy}
\end{figure}

\section{System Model}
\label{sec:system-model}

The harware system is represented by a graph $G_s = \{E,V\}$ where $E$ is a set of edges representing communication links, and $V$ is a set of vertices representing data sources/sinks.
Sections~\ref{sec:system-vertices} and \ref{sec:system-edges} describe the specific system components explored.
Associated with each edge is a performance model function $M: C,U \rightarrow P$ that maps a communication pattern $C$ and a link utilization $U$ to an achievable performance $P$.

The communication pattern $P$ has the following parameters:
\begin{itemize}
    \item The communication API or method used (e.g., \texttt{fread()}, CUDA unified memory page transfer).
    \item The number, size, and priority of pending transfers on the link.
\end{itemize}

The link utilization $U$ a set of extant communication patterns already sharing the link, separate from the communication of interest $C$.



Each vertex in $V$ represents a data source/sink.



%
% SECTION
%
\section{Topology Exploration}
\label{sec:topology-exploration}

The topology exploration is done in several phases:

\begin{minipage}[ht]{\textwidth}
\begin{enumerate}
    \item Enumerate and link CPU sockets
    \item Enumerate PCI devices
    \item Update GPUs to NVIDIA GPUs as appropriate
    \item Enumerate Linux block devices
\end{enumerate}
\end{minipage}

First, the Portable Hardware Locality~\cite{broquedis2010hwloc} (hwloc) library is used to enumerate the present CPU sockets.
As the test systems only have two sockets, all discovered sockets are considered to be directly connected by an SMP bus.
Next, hwloc is used to descend through the PCI device tree and connect all PCI devices with PCI links of the appropriate type.
Next, the NVIDIA Management Library~\cite{nvidia2017nvml} (NVML) is used to enumerate all NVIDIA GPUs.
Those GPUs are matched by PCI address with previously-discovered PCI devices and information about those GPUs is added to $G_s$.
NVML is then used to discover whether NVLink is supported on each GPU, and which device the NVLink terminates at.
Finally, linux block devices are enumerated through \todo{more detail} and added to $G_s$.
Where applicable, enough information about the device is stored within the vertex to be able to access the device later.

\subsection{Vertex Types}
\label{sec:system-vertices}

Table~\ref{tab:topology-vertices} summarizes the discoverable types of data sources and sinks investigated by this work.


\begin{table}[]
    \centering
    \caption[Discoverable vertex types]{\todo{long caption}}
    \label{tab:topology-vertices}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Vertex Type}    & \textbf{Description} \\ \hline
    CPU Socket              &                      \\ \hline
    PCI Device              &                      \\ \hline
    PCIe Hostbridge         &                      \\ \hline
    PCIe Bridge             &                      \\ \hline
    CUDA GPU                &                      \\ \hline
    Linux Block Device      &                      \\ \hline
    Linux Network Interface &                      \\ \hline
    \end{tabular}
\end{table}

\subsection{Edge Types}
\label{sec:system-edges}

In $G_s$, the vertices are connected by the discoverable edge types shown in Table~\ref{tab:topology-edges}.

\begin{table}[]
    \centering
    \caption[Discoverable edge types]{\todo{long caption}}
    \label{tab:topology-edges}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Edge Type} & \textbf{Description} \\ \hline
    SMP Bus            &                      \\ \hline
    PCIe Bus           &                      \\ \hline
    NVLink             &                      \\ \hline
    SATA Bus           &                      \\ \hline
    \end{tabular}
\end{table}



%
% SECTION
%
\section{Link Characterization}
\label{sec:link-char}

After the system graph $G_s$ has been generated, the next task is to characterize the communication capabilities of the system.
The goal of this characterization is to determine the rate at which data of a particular size can be moved between devices.
Ideally, this characterization would occur on a per-link basis along each available path between two communicating devices.
In practice, the communication between many devices is mediated by APIs exposed by the operating system or vendor library.
These APIs abstract away some complexity from the data movement.

\begin{figure}
    \centering
    \begin{tikzpicture}[
        cpunode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=7mm},
        gpunode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
        blocknode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
        ]
        %Nodes
        \node[cpunode]   (s0)                  {Socket0};
        \node[blocknode] (b0)    [below=of s0] {Disk0};
        \node[cpunode]   (s1)    [right=of s0] {Socket1};
        \node[gpunode]   (g0)    [below=of s1] {GPU0};

        %Lines
        \path[-] (s0.east)  edge node [above] {SMP}    (s1.west);
        \path[-] (s0.south) edge node [left]  {PCIe0}  (b0.north);
        \path[-] (s1.south) edge node [right] {PCIe1}  (g0.north);
    \end{tikzpicture}
    \caption[A simple example topology]{\todo{clean this up}\todo{long caption}}
    \label{fig:simple-topology}
\end{figure}

For example, consider the simple example system topology in Figure~\ref{fig:simple-topology}.
If a CPU thread running on CPU1 calls \texttt{fread()} to move a block of data from Disk0 to the memory associated with CPU1, the OS will transparently move that data along the PCIe0 and SMP0 links.
Since this capability is exposed to applications, it is useful to characterize it as well, not just the intermediate PCIe0 and SMP links.

An overview of the characterization algorithm is shown in Algorithm~\ref{alg:link-char}.

\begin{algorithm}[ht]
    \SetAlgoLined
    \KwResult{Characterization of all links between all vertices in $G_s$ }
     Build $G_s$ as described in Section~\ref{sec:topology-exploration}\;
     \For{$v_1$ in $V$}{
         \For{$v_2$ in $V$}{
             \If{$v_1 \ne v_2$}{
                Chars $\gets$ SupportedCharacterizers($v_1$,$v_2$)\;
                \For{c in Chars} {
                    c($v_1$, $v_2$)\;
                }
             }
         }
     }
     \caption{Link characterization.}
     \label{alg:link-char}
\end{algorithm}

For each pair of vertices, \todo{hwcomm} determines whether direct communication between those vertices is supported by the operating system or vendor libraries.
For vertices with a path of more than one link between them (e.g. Disk0 to Socket1 in Figure~\ref{fig:simple-topology}) those individual links will be characterized separately.
For vertices with multiple paths between them, the characterized path will be implicitly chosen by the applied characterization method.

\subsubsection{CUDA \texttt{cudaMemcpy} with Pinned Memory}

This characterization method outlined in Algorithm~\ref{alg:cuda-h2d} is available for paths terminated by a CPU socket and a CUDA GPU.
It uses pinned memory allocations on the host to have the best chance of fully utilizing the link.

\begin{algorithm}[ht]
    \SetAlgoLined
    \KwResult{bandwidth vs. transfer size between socket $s$ and CUDA GPU $g$}
    bind CPU thread to $s$\;
    bind memory allocation to $s$\;
    poolSize $\gets$ $\frac{gpuMemory}{2}$\;
    socketPool $\gets$ \texttt{cudaMallocHost(poolSize)}\;
    gpuPool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    \For{transferSize := $1$ to poolSize}{
        start $\gets$ wall\_time()\;
        \eIf{direction == deviceToHost} {
            \texttt{cudaMemcpy(socketPool, gpuPool, transferSize, cudaMemcpyHostToDevice)}\;
        }{
            \texttt{cudaMemcpy(gpuPool, socketPool, transferSize, cudaMemcpyHostToDevice)}\;
        }
        stop $\gets$ wall\_time()\;
        bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
    }
    \caption{CUDA cudaMemcpy with pinned memory.}
    \label{alg:cuda-h2d}
\end{algorithm}

\subsubsection{CUDA \texttt{cudaMemcpy} between CUDA GPUs}

This characterization method outlined in Algorithm~\ref{alg:cuda-d2d} is available for paths terminated by a CUDA GPU on both ends.

\begin{algorithm}[ht]
    \SetAlgoLined
    \KwResult{bandwidth vs. transfer size between CUDA GPUs $g_0$ and $g_1$}
    poolSize $\gets$ $\frac{gpuMemory}{2}$\;
    gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    \For{transferSize := $1$ to poolSize}{
        start $\gets$ wall\_time()\;
        \texttt{cudaMemcpy(gpu1Pool, gpu0Pool, transferSize, cudaMemcpyDeviceToDevice)}\;
        stop $\gets$ wall\_time()\;
        bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
    }
    \caption{CUDA cudaMemcpy between CUDA GPUs}
    \label{alg:cuda-d2d}
\end{algorithm}

\subsubsection{CUDA \texttt{memcpyPeer}}

This characterization method outlined in Algorithm~\ref{alg:cuda-d2d} is available for paths terminated by a CUDA GPU on both ends.

\begin{algorithm}[ht]
    \SetAlgoLined
    \KwResult{bandwidth vs. transfer size between CUDA GPUs $g_0$ and $g_1$}
    poolSize $\gets$ $\frac{gpuMemory}{2}$\;
    gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    \For{transferSize := $1$ to poolSize}{
        start $\gets$ wall\_time()\;
        \texttt{cudaMemcpyPeer(gpu1Pool, gpu0Pool, transferSize)}\;
        stop $\gets$ wall\_time()\;
        bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
    }
    \caption{CUDA cudaMemcpy between CUDA GPUs}
    \label{alg:cuda-p2p}
\end{algorithm}

\todo{The difference between this and cudaMemcpy(cudaMemcpyDeviceToDevice)}

\subsubsection{CUDA Unified Memory}

CUDA Unified Memory~\cite{harris2013cudaunifiedmemory} provides a single pool of memory that is accessible from the CPU and GPU by a single pointer.
CUDA automatically migrates data between the physically distinct CPU and GPU memory as needed, allowing GPU kernels to access the memory as if it were in the global memory, and CPU functions to access the memory as if it were in the system memory.
This simplifies the programming model.

\subsubsection{OpenMP Socket to Socket Bandwidth}

The symmetric multi-processing links between CPU sockets are characterized by a synthetic workload generating using OpenMP~\cite{openmp2013}.
This workload creates multiple threads reading remote data in an attempt to saturate the socket memory controllers and SMP bus.
Algorithm~\ref{alg:h2h} describes the approach.

\begin{algorithm}[ht]
    \SetAlgoLined
    \KwResult{bandwidth vs. transfer size between CPU sockets $src$ and $dst$}
    numThreads $\gets$ 10\;
    \todo{Choice of elemSize}\;
    \For{poolSize $\gets$ 128 to sweepFinish} {
        bind process to $src$\;
        bind memory allocation to $src$\;
        \For{tid $\gets$ 0 := numThreads}{
            regions[tid] $\gets$ alloc(poolSize)\;
        }
        bind process to $dst$\;
        start OpenMP parallel region\;
        start $\gets$ wall\_time()\;
        \texttt{sum\_array(region[omp\_get\_thread\_num(), elemSize])}\;
        stop $\gets$ wall\_time()\;
        bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
    }
    \caption{Synthetic workload for testing SMP bus.}
    \label{alg:h2h}
\end{algorithm}

\texttt{sum\_array()} is a function used to ensure that every data element is accessed, and the compiler does not optimize out the otherwise unused remote data read.
\texttt{sum\_array()} adds the elements in the region using loads and accumulates of a desired size.

%
% SECTION
%
\section{System Characterization Case Studies}

The IMB ``Minsky'' machine consists of two symmetric sections, connected by an IBM X bus between two 10-core Power8+ CPUs with 8-way simultaneous multithreading (SMT).
Each CPU is connected to 256GB of \todo{DDR4} RAM.
Each CPU is also connected to two NVIDIA Tesla P100 GPUs by two NVLink blocks.
Those P100s within the isomorphic section are also connected to each other by two NVLink blocks.
The first CPU socket hosts the majority of the PCI devices on the system, including the network interfaces and the disks.
Figures~\ref{fig:topology-minsky-actual} and \ref{fig:topology-minsky-simple} shows the topology discovered by \todo{hwcomm}.

\subsection{IBM S822LC ``Minsky''}
\label{sec:topology-minsky}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/topo-minsky-actual.png}
    \caption{IBM ``Minsky'' discovered topology.}
    \label{fig:topology-minsky-actual}
\end{figure}

\subsection{IBM S822LC ``Minsky''}
\label{sec:topology-minsky}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth,draft]{figures/topo-minsky-simple.png}
    \caption{IBM ``Minsky'' simplified topology.}
    \label{fig:topology-minsky-simplified}
\end{figure}


\subsection{NVIDIA DGX-1}
\label{sec:topology-dgx1}

The NVIDIA DGX-1~\cite{nvidia2016dgx1} machine consists of two symmetric sections.
Each section consists of one 20-core Intel Xeon E5-2698v4 CPUs with 2-way SMT.
Each CPU is connected to 256GB of DDR4 RAM.
Each section has 4 NVIDIA Tesla P100 GPUs coupled by single NVLinks.
The sections are connected by an Intel QPI bus between the CPUs, as well as NVLinks between corresponding GPUs.
The first CPU socket hosts the majority of the PCI devices on the system, including the network interfaces and the disks.
Figures~\ref{fig:topo-dgx-actual} and \ref{fig:topo-dgx-simple} show the topology discovered by \todo{hwcomm}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/topo-dgx1-actual.png}
    \caption{NVIDIA DGX-1 discovered toplogy.}
    \label{fig:topo-dgx-actual}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth,draft]{figures/topo-dgx1-simple.png}
    \caption{NVIDIA DGX-1 simple topology.}
    \label{fig:topo-dgx-simple}
\end{figure}