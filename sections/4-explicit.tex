\chapter{Explicit Memory Performance}
\label{ch:explicit}

\section{CPU / GPU Transfers}

Algorithm~\ref{alg:explicit} is used to evaluate the achievable bandwidth for the logical endpoints and methods shown in Table~\ref{tab:explicit}.

\begin{table}[ht]
    \centering
    \caption[\todo{short}]{\todo{long}}
    \label{tab:explicit}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Source} & \textbf{Transfer Methods} \\ \hline 
    \makecell{Host Pinned Allocation \\ Host Pageable Allocation \\ Host Write-Combining Allocation \\ Device Allocation} & cudaMemcpy \\ \hline
    \end{tabular}
\end{table}

\begin{algorithm}
    \caption{Measuring explicit \texttt{cudaMemcpy} performance}
    \label{alg:explicit}
    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$}
        \If{$src$ is GPU}
            \State \texttt{cudaSetDevice($src$}
        \Else \Comment{$src$ is CPU}
            \State \texttt{numa\_bind($src$}
        \EndIf
        \If{$dst$ is GPU}
        \State \texttt{cudaSetDevice($dst$)}
        \Else \Comment{$dst$ is CPU}
        \State \texttt{numa\_bind($dst$)}
        \EndIf

        \State $devPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{device allocation}
        \State $srcPtr \gets$ \texttt{malloc($transfer\_size$)} \Comment{or cudaHostAlloc()}

        \State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
        \For{$i \gets 1 \textrm{ to } num\_iters$}
            \State $start \gets$ walltime()
            \State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor

        \Return $elapsed$
    \EndFunction


    \end{algorithmic}
\end{algorithm}

\subsection{Comparison of Pageable, Pinned, and Write-Combining Host Allocations}

Figure~\ref{fig:pageable-pinned-wc} shows the transfer performance from CPU0 to GPU0 for pageable, pinned, and write-combined allocations.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/pageable_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/pinned_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/wc_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    \caption[\todo{short}]{
        IBM Minsky \texttt{cudaMemcpy} performance with pageable host allocations from (a) CPU0 to all GPUs and (b) CPU1 to all GPUs.
    }
    \label{fig:pageable-pinned-wc}
\end{figure}

\subsection{Effect of Affinity and Direction}

There is a distinct performance difference in transfers between components within a triad (CPU0-GPU0-GPU1 or CPU1-GPU2-GPU3) and components across triads.
Figure~\ref{fig:minsky-pinned} shows cudaMemcpy performance between pinned allocations and GPU allocations on Minsky.
Figures~\ref{fig:minsky_pinned_cpu0-gpu}~and~\ref{fig:minsky_pinned_cpu1-gpu} show that pinned transfers to/from local GPUs are 3GB/s faster than remote GPUs.
At large transfer sizes, figure~\ref{fig:minsky_pinned_cpu0-gpu} shows a \mytilde 5 GB/s difference, regardless of the source CPU socket.
Figure~\ref{fig:minsky-pinned-gpu} shows an even larger performance difference of up to 10GB/s.
Additionally, GPU $\rightarrow$ CPU transfers in the (CPU0-GPU0-GPU1) triad have erratic performance as transfer sizes change.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_cpu0-gpu.pdf}
        \caption{}
        \label{fig:minsky_pinned_cpu0-gpu}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_cpu1-gpu.pdf}
        \caption{}
        \label{fig:minsky_pinned_cpu1-gpu}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_gpu-cpu0.pdf}
        \caption{}
        \label{fig:minsky_pinned_gpu-cpu0}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_gpu-cpu1.pdf}
        \caption{}
        \label{fig:minsky_pinned_gpu-cpu1}
    \end{subfigure}
    \caption[\todo{short}]{
        IBM Minsky \texttt{cudaMemcpy} performance with pinned host allocations from 
        (a) CPU0 to all GPUs
        (b) CPU1 to all GPUs
        (c) All GPUs to CPU0
        (d) All GPUs to CPU1
    }
    \label{fig:minsky-pinned}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by affinity]{Does affinity affect transfer rate?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind} & \textbf{Minsky} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\rightarrow$ GPU        & \checkmark & & \\ \hline
    Pageable $\leftarrow$ GPU         & \checkmark & & \\ \hline
    Pinned $\rightarrow$ GPU          & \checkmark & & \\ \hline
    Pinned $\leftarrow$ GPU           & \checkmark & & \\ \hline
    Write-Combining $\rightarrow$ GPU & \checkmark & & \\ \hline
    Write-Combining $\leftarrow$ GPU  & \checkmark & & \\ \hline
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by direction]{Does direction affect transfer rate?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind} & \textbf{Minsky} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\leftrightarrow$ GPU        & depends on affinity & & \\ \hline
    Pinned $\leftrightarrow$ GPU          & depends on affinity & & \\ \hline
    Write-Combining $\leftrightarrow$ GPU & $\times$ & & \\ \hline
    \end{tabular}
\end{table}

\subsection{Effect of Transfer Direction}

Remote CPU $\rightarrow$ GPU transfers may have different performance than GPU $\rightarrow$ CPU transfers.
        In Figures~\ref{fig:minsky-pinned-cpu}~and~\ref{fig:minsky-pinned-gpu}, CPU $\rightarrow$ GPU remote transfers achieve nearly 30 GB/s, while GPU $\rightarrow$ CPU transfers peak at around 20 GB/s.

\subsection{Differences between Logically Identical Transfers}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pageable_cpu0-gpus.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pageable_cpu1-gpus.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    \caption[\todo{short}]{
        IBM Minsky \texttt{cudaMemcpy} performance with pageable host allocations from (a) CPU0 to all GPUs and (b) CPU1 to all GPUs.
    }
    \label{fig:topo-asymmetry}
\end{figure}

Figure~\ref{fig:topo-asymmetry} shows the performance of \texttt{cudaMemcpy} moving pageable host allocations from CPU0 or CPU1 to all GPUs on the IBM Minsky.
For transfers originating at CPU0, the achievable bandwidth does not depend on the destination device, but for transfers originating from CPU1, transfers 1-10MB in size are much slower to remote GPUs than local GPUs.



\section{GPU / GPU Transfers}


\subsubsection{CUDA \texttt{cudaMemcpy} between CUDA GPUs}

This characterization method outlined in Algorithm~\ref{alg:cuda-d2d} is available for paths terminated by a CUDA GPU on both ends.
A GPU memory allocation is established on each GPU.
Bandwidth achieved at various transfer sizes between GPUs is established by copying various amounts of data between the memory allocations.

% \begin{algorithm}[ht]
%     \SetAlgoLined
%     \KwResult{bandwidth vs. transfer size between CUDA GPUs $g_0$ and $g_1$}
%     poolSize $\gets$ $\frac{gpuMemory}{2}$\;
%     gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     \For{transferSize := $1$ to poolSize}{
%         start $\gets$ wall\_time()\;
%         \texttt{cudaMemcpy(gpu1Pool, gpu0Pool, transferSize, cudaMemcpyDeviceToDevice)}\;
%         stop $\gets$ wall\_time()\;
%         bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
%     }
%     \caption{CUDA cudaMemcpy between CUDA GPUs}
%     \label{alg:cuda-d2d}
% \end{algorithm}

\subsubsection{CUDA \texttt{memcpyPeer}}

This characterization method outlined in Algorithm~\ref{alg:cuda-d2d} is available for paths terminated by a CUDA GPU on both ends.
A GPU memory allocation is established on each GPU.
Bandwidth between GPUs achieved at various transfer sizes is established by copying various amounts of data between the memory allocations.

% \begin{algorithm}[ht]
%     \SetAlgoLined
%     \KwResult{bandwidth vs. transfer size between CUDA GPUs $g_0$ and $g_1$}
%     poolSize $\gets$ $\frac{gpuMemory}{2}$\;
%     gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     \For{transferSize := $1$ to poolSize}{
%         start $\gets$ wall\_time()\;
%         \texttt{cudaMemcpyPeer(gpu1Pool, gpu0Pool, transferSize)}\;
%         stop $\gets$ wall\_time()\;
%         bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
%     }
%     \caption{CUDA cudaMemcpy between CUDA GPUs}
%     \label{alg:cuda-p2p}
% \end{algorithm}


This characterization method outlined in Algorithm~\ref{alg:gpu-gpu-peer} is available for paths terminated by a CUDA GPU on both ends.
If the GPUs support peer access

A GPU memory allocation is established on each GPU.
Bandwidth achieved at various transfer sizes between GPUs is established by copying various amounts of data between the memory allocations.


% \begin{algorithm}[ht]
%     \SetAlgoLined
%     \SetKwInOut{Input}{input}
%     \SetKwInOut{Output}{output}
%     \Input{$b$, whether peer transfers between CUDA GPUs should be enabled}
%     \Output{a set of (bandwidth, transfer size) tuples $t$ for transfers between CUDA GPUs $g_0$ and $g_1$}
%     $t$ $\gets$ \{\}\;
%     poolSize $\gets$ $\frac{gpuMemory}{2}$\;
%     gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     \If{$b$}{
%         \texttt{cudaSetDevice($g_0$)\;}
%         \texttt{cudaDeviceEnablePeerAccess($g_1$)\;}
%         \texttt{cudaSetDevice($g_1$)\;}
%         \texttt{cudaDeviceEnablePeerAccess($g_0$)\;}
%     }
%     \For{transferSize := $1$ to poolSize}{
%         start $\gets$ wall\_time()\;
%         \texttt{cudaMemcpyPeer(gpu1Pool, gpu0Pool, transferSize)}\;
%         stop $\gets$ wall\_time()\;
%         t $\gets$ t $\cup$ \{ ( transferSize, $\frac{transferSize}{stop - start}$ ) \}\;
%     }
%     \caption{Characterizing data transfer performance between CUDA GPUs.}
%     \label{alg:gpu-gpu-peer}
% \end{algorithm}