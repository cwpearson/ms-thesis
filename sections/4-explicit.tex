\chapter{Explicit Memory Performance}
\label{ch:explicit}

\section{CPU / GPU Transfers}

Algorithm~\ref{alg:explicit} is used to evaluate the achievable bandwidth for the logical endpoints and methods shown in Table~\ref{tab:explicit}.

\begin{table}[ht]
    \centering
    \caption[\todo{short}]{\todo{long}}
    \label{tab:explicit}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Source} & \textbf{Transfer Methods} \\ \hline 
    \makecell{Host Pinned Allocation \\ Host Pageable Allocation \\ Host Write-Combining Allocation \\ Device Allocation} & cudaMemcpy \\ \hline
    \end{tabular}
\end{table}

\begin{algorithm}
    \caption{Measuring explicit \texttt{cudaMemcpy} performance}
    \label{alg:explicit}
    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$}
        \If{$src$ is GPU}
            \State \texttt{cudaSetDevice($src$}
        \Else \Comment{$src$ is CPU}
            \State \texttt{numa\_bind($src$}
        \EndIf
        \If{$dst$ is GPU}
        \State \texttt{cudaSetDevice($dst$)}
        \Else \Comment{$dst$ is CPU}
        \State \texttt{numa\_bind($dst$)}
        \EndIf

        \State $devPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{device allocation}
        \State $srcPtr \gets$ \texttt{malloc($transfer\_size$)} \Comment{or cudaHostAlloc()}

        \State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
        \For{$i \gets 1 \textrm{ to } num\_iters$}
            \State $start \gets$ walltime()
            \State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor

        \Return $elapsed$
    \EndFunction


    \end{algorithmic}
\end{algorithm}

\subsection{Comparison of Pageable, Pinned, and Write-Combining Host Allocations}

Figure~\ref{fig:pageable-pinned-wc} shows the transfer performance from CPU0 to GPU0 for pageable, pinned, and write-combined allocations.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/pageable_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/pinned_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/wc_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    \caption[\todo{short}]{
        IBM Minsky \texttt{cudaMemcpy} performance with pageable host allocations from (a) CPU0 to all GPUs and (b) CPU1 to all GPUs.
    }
    \label{fig:pageable-pinned-wc}
\end{figure}

\subsection{Effect of Affinity and Direction}

There is a distinct performance difference in transfers between components within a triad (CPU0-GPU0-GPU1 or CPU1-GPU2-GPU3) and components across triads.
Figure~\ref{fig:minsky-pinned} shows cudaMemcpy performance between pinned allocations and GPU allocations on Minsky.
Figures~\ref{fig:minsky_pinned_cpu0-gpu}~and~\ref{fig:minsky_pinned_cpu1-gpu} show that pinned transfers to/from local GPUs are 3GB/s faster than remote GPUs.
At large transfer sizes, figure~\ref{fig:minsky_pinned_cpu0-gpu} shows a \mytilde 5 GB/s difference, regardless of the source CPU socket.
Figure~\ref{fig:minsky-pinned-gpu} shows an even larger performance difference of up to 10GB/s.
Additionally, GPU $\rightarrow$ CPU transfers in the (CPU0-GPU0-GPU1) triad have erratic performance as transfer sizes change.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_cpu0-gpu.pdf}
        \caption{}
        \label{fig:minsky_pinned_cpu0-gpu}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_cpu1-gpu.pdf}
        \caption{}
        \label{fig:minsky_pinned_cpu1-gpu}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_gpu-cpu0.pdf}
        \caption{}
        \label{fig:minsky_pinned_gpu-cpu0}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_gpu-cpu1.pdf}
        \caption{}
        \label{fig:minsky_pinned_gpu-cpu1}
    \end{subfigure}
    \caption[\todo{short}]{
        IBM Minsky \texttt{cudaMemcpy} performance with pinned host allocations from 
        (a) CPU0 to all GPUs
        (b) CPU1 to all GPUs
        (c) All GPUs to CPU0
        (d) All GPUs to CPU1
    }
    \label{fig:minsky-pinned}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by affinity]{Does affinity affect transfer rate?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind} & \textbf{Minsky} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\rightarrow$ GPU        & \checkmark & $\times$   & \\ \hline
    Pageable $\leftarrow$ GPU         & \checkmark & \checkmark & \\ \hline
    Pinned $\rightarrow$ GPU          & \checkmark & \checkmark & \\ \hline
    Pinned $\leftarrow$ GPU           & \checkmark & \checkmark & \\ \hline
    Write-Combining $\rightarrow$ GPU & \checkmark & \checkmark & \\ \hline
    Write-Combining $\leftarrow$ GPU  & \checkmark & \checkmark & \\ \hline
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by direction]{Does direction affect transfer rate?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                & \textbf{Minsky}     & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\leftrightarrow$ GPU        & depends on affinity & \checkmark     & \\ \hline
    Pinned $\leftrightarrow$ GPU          & depends on affinity & for some sizes & \\ \hline
    Write-Combining $\leftrightarrow$ GPU & $\times$            & for some sizes & \\ \hline
    \end{tabular}
\end{table}


\subsection{Differences between Identical Transfers}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/generated/minsky_pageable_cpu1-gpu01.pdf}
    \caption[\todo{short}]{\todo{long}}
    \label{fig:minsky_pageable_cpu1-gpu01}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by direction]{Does trasnfer rate vary on identical links?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                & \textbf{Minsky}     & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\rightarrow$ GPU        & $\times$   & $\times$ & \\ \hline
    Pageable $\leftarrow$ GPU         & \checkmark & $\times$ & \\ \hline
    Pinned $\rightarrow$ GPU          & $\times$   & $\times$ & \\ \hline
    Pinned $\leftarrow$ GPU           & $\times$   & $\times$ & \\ \hline
    Write-Combining $\rightarrow$ GPU & $\times$   & $\times$ & \\ \hline
    Write-Combining $\leftarrow$ GPU  & $\times$   & $\times$ & \\ \hline
    \end{tabular}
\end{table}

Figure~\ref{fig:minsky_pageable_cpu1-gpu01} shows the performance of \texttt{cudaMemcpy} moving data between pageable host allocations on CPU1 to GPU1 on Minsky.

\section{GPU / GPU Transfers}


\subsubsection{CUDA \texttt{cudaMemcpy} between CUDA GPUs}

This characterization method outlined in Algorithm~\ref{alg:cuda-d2d} is available for paths terminated by a CUDA GPU on both ends.
A GPU memory allocation is established on each GPU.
Bandwidth achieved at various transfer sizes between GPUs is established by copying various amounts of data between the memory allocations.

% \begin{algorithm}[ht]
%     \SetAlgoLined
%     \KwResult{bandwidth vs. transfer size between CUDA GPUs $g_0$ and $g_1$}
%     poolSize $\gets$ $\frac{gpuMemory}{2}$\;
%     gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     \For{transferSize := $1$ to poolSize}{
%         start $\gets$ wall\_time()\;
%         \texttt{cudaMemcpy(gpu1Pool, gpu0Pool, transferSize, cudaMemcpyDeviceToDevice)}\;
%         stop $\gets$ wall\_time()\;
%         bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
%     }
%     \caption{CUDA cudaMemcpy between CUDA GPUs}
%     \label{alg:cuda-d2d}
% \end{algorithm}

\subsubsection{CUDA \texttt{memcpyPeer}}

This characterization method outlined in Algorithm~\ref{alg:cuda-d2d} is available for paths terminated by a CUDA GPU on both ends.
A GPU memory allocation is established on each GPU.
Bandwidth between GPUs achieved at various transfer sizes is established by copying various amounts of data between the memory allocations.

% \begin{algorithm}[ht]
%     \SetAlgoLined
%     \KwResult{bandwidth vs. transfer size between CUDA GPUs $g_0$ and $g_1$}
%     poolSize $\gets$ $\frac{gpuMemory}{2}$\;
%     gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     \For{transferSize := $1$ to poolSize}{
%         start $\gets$ wall\_time()\;
%         \texttt{cudaMemcpyPeer(gpu1Pool, gpu0Pool, transferSize)}\;
%         stop $\gets$ wall\_time()\;
%         bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
%     }
%     \caption{CUDA cudaMemcpy between CUDA GPUs}
%     \label{alg:cuda-p2p}
% \end{algorithm}


This characterization method outlined in Algorithm~\ref{alg:gpu-gpu-peer} is available for paths terminated by a CUDA GPU on both ends.
If the GPUs support peer access

A GPU memory allocation is established on each GPU.
Bandwidth achieved at various transfer sizes between GPUs is established by copying various amounts of data between the memory allocations.


% \begin{algorithm}[ht]
%     \SetAlgoLined
%     \SetKwInOut{Input}{input}
%     \SetKwInOut{Output}{output}
%     \Input{$b$, whether peer transfers between CUDA GPUs should be enabled}
%     \Output{a set of (bandwidth, transfer size) tuples $t$ for transfers between CUDA GPUs $g_0$ and $g_1$}
%     $t$ $\gets$ \{\}\;
%     poolSize $\gets$ $\frac{gpuMemory}{2}$\;
%     gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
%     \If{$b$}{
%         \texttt{cudaSetDevice($g_0$)\;}
%         \texttt{cudaDeviceEnablePeerAccess($g_1$)\;}
%         \texttt{cudaSetDevice($g_1$)\;}
%         \texttt{cudaDeviceEnablePeerAccess($g_0$)\;}
%     }
%     \For{transferSize := $1$ to poolSize}{
%         start $\gets$ wall\_time()\;
%         \texttt{cudaMemcpyPeer(gpu1Pool, gpu0Pool, transferSize)}\;
%         stop $\gets$ wall\_time()\;
%         t $\gets$ t $\cup$ \{ ( transferSize, $\frac{transferSize}{stop - start}$ ) \}\;
%     }
%     \caption{Characterizing data transfer performance between CUDA GPUs.}
%     \label{alg:gpu-gpu-peer}
% \end{algorithm}