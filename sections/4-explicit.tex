\chapter{Explicit Memory Performance}
\label{ch:explicit}

This chapter examines the performance of explicit data transfers over logical communication links presented in a system with NUMA and CUDA interfaces.
In particular, it focuses on CPU/CPU transfers, CPU/GPU transfers, and GPU/GPU transfers.
It highlights cases where the observed logical communication performance deviates significantly from the symmetries present in the CUDA API, numactl API, and hardware.
Those deviations take the form of different performance on identical links, anisotropic link performance, or performance affected by device affinity.

The microbenchmarks developed for this section are available in the \texttt{microbench} project~\cite{pearson2018microbench}.
That project also includes benchmarks of other aspects of CUDA performance, including CUDA primitives like kernel launches, and CUDA libraries such as cuBLAS and cuDNN.

\section{CPU / CPU Transfers}
\label{sec:explicit-cpu-cpu}

This section begins by examining CPU-CPU transfer performance through \texttt{cudaMemcpy}.
This attempts to provide insight into the CUDA performance when sending data from one CPU socket to another.
Such a transfer would occur when data is sent from a CPU A to a GPU attached to another CPU B.
The data would traverse the SMP bus between CPU A and CPU B, and the bandwidth of that bus could limit the overall performance of the transfer.
Algorithm~\ref{alg:explicit-cpu-cpu} describes the measurement approach.
During the setup phase, an allocation is created on the source CPU $src$ and destination CPU $dst$.
During the benchmark iterations, the $dst$ cache is invalidated (if the $src$ is different from the $dst$) by accessing that data from the $src$.
Then, \texttt{cudaMemcpy} is invoked to transfer data between those allocations.
CUDA events are used to measure the time of the memory copy.

\begin{algorithm}

	
	\begin{algorithmic}[1]
        \Statex
		\Function{bandwidth}{$dst$, $src$, $transfer\_size$}
		\State numa\_bind\_node($src$)
        \State $srcPtr \gets$ AllocPageable($transfer\_size$)
		\State memset($srcPtr$, 0, $transfer\_size$)
		\State numa\_bind\_node($dst$)
        \State $dstPtr \gets$ AllocPinned($transfer\_size$)
		\State memset($dstPtr$, 0, $transfer\_size$)

		\State $start \gets$ cudaEventCreate()
		\State $stop \gets$ cudaEventCreate()

		\Statex
		\For{state} \Comment Benchmark library loop
		\State numa\_bind\_node($src$)
		\State memset($srcPtr$, 0, $transfer\_size$) \Comment invalidate $dst$ cache
		\State numa\_bind\_node($dst$)
		\State cudaEventRecord($start$)
		\State \begin{varwidth}[t]{\linewidth}
			cudaMemcpy(\par
			\hskip\algorithmicindent $dstPtr$, $srcPtr$, $transfer\_size$, \par 
			\hskip\algorithmicindent cudaMemcpyHostToHost)
		\end{varwidth}
		\State cudaEventRecord($stop$)
		\State cudaEventSynchronize($stop$)
		\State $millis \gets$ cudaEventElapsedTime($start$, $stop$)
		\State state.SetIterationTime($\frac{millis}{1000}$)
        \EndFor
		\EndFunction

	\end{algorithmic}
	\caption[Measure \texttt{cudaMemcpy} CPU-CPU bandwidth]{
		Algorithm to measure \texttt{cudaMemcpy} CPU-CPU Bandwidth.
		AllocPinned and AllocPageable are defined in Algorithm~\ref{alg:host-allocators}.
		\texttt{numa\_bind\_node} is defined in Listing~\ref{lst:numa-bind-node}.
	}
	\label{alg:explicit-cpu-cpu}
\end{algorithm}

Algorithm~\ref{alg:host-allocators} shows different kinds of host allocation strategies used in microbenchmarks that need CUDA host allocations on particular NUMA nodes.
\texttt{AllocPageable} simply defers to \texttt{malloc}, which will return memory allocation on a previously-pinned NUMA node.
\texttt{AllocPinned} defers to \texttt{malloc} to get a NUMA allocations, and then uses \texttt{cudaHostRegister} to pin that memory.
\texttt{AllocWriteCombined} uses the \texttt{cudaHostAlloc} CUDA library call with the \texttt{cudaHostAllocWriteCombined} flag to request that CUDA allocate write-combining memory.

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Statex
		\Function{AllocPageable}{$bytes$}
		\State $ptr \gets 0$
		\State malloc($ptr$, $bytes$)
		\State \Return $ptr$
		\EndFunction
		\Statex
		\Function{AllocPinned}{$bytes$}
		\State $ptr \gets 0$
		\State malloc($ptr$, $bytes$)
		\State cudaHostRegister($ptr$, $bytes$, cudaHostRegisterPortable)
		\State \Return $ptr$
		\EndFunction
		\Statex
		\Function{AllocWriteCombined}{$bytes$}
		\State $ptr \gets 0$
		\State cudaHostAlloc($ptr$, $bytes$, cudaHostAllocWriteCombined)
		\State \Return $ptr$
		\EndFunction
		
	\end{algorithmic}
	\caption[Pageable, pinned, and write-combining host allocators]{
        Pageable, pinned, and write-combining host allocators.
	}
	\label{alg:host-allocators}
\end{algorithm}

Figure~\ref{fig:explicit-cpu-cpu} shows intra- and inter-CPU \texttt{cudaMemcpy} performance on S822LC, AC922, and DGX-1.
In all cases, for small transfers, the bandwidth is limited by the overhead of invoking the transfer.
For intermediate and larger sizes, that overhead ceases to be the performance-limiter.
At large sizes, intra-CPU bandwidth is higher, presumably since data transfer over the SMP bus is not required.
The bandwidth saturates at the rate that a single thread can generate loads and stores.

S822LC and AC922 have similar intra-CPU performance except for intermediate sizes, where the S822LC performance peaks (presumably due to transfers happening in cache) and AC922 performance drops.
On DGX-1, inter-CPU transfers are actually faster than intra-CPU transfers for intermediate sizes.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_cpu-cpu.pdf}
		\caption{}
		\label{fig:s822lc-cpu0-cpu1-dst}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_cpu-cpu.pdf}
		\caption{}
		\label{fig:ac922-cpu0-cpu1-dst}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_cpu-cpu.pdf}
		\caption{}
		\label{fig:dgx-cpu0-cpu1-dst}
	\end{subfigure}
	\caption[CPU-CPU transfer bandwidth]{
		CPU-to-CPU transfer bandwidth vs. transfer size.
		Each transfer is measured using Algorithm~\ref{alg:explicit-cpu-cpu}.
		Whiskers at each point show the standard deviation measured over 5 repetitions.
		}
		\label{fig:explicit-cpu-cpu}
\end{figure}

\section{CPU / GPU Transfers}
\label{sec:explicit-cpu-gpu}

Explicit CPU-GPU transfers are caused by the \texttt{cudaMemcpy} family of functions being invoked on one pointer to a host allocation and one pointer to a device allocation.
In this work, the host allocation is created by one of three methods shown in Algorithm~\ref{alg:host-allocators}.
The device allocation is created by \texttt{cudaMalloc}.
This section compares bandwidth achievable from pinned, pageable, and write-combining host allocations, with particular emphasis on how device affinity affects transfer-performance and cases where transfers are anisotropic.

Algorithm~\ref{alg:explicit-cpu-gpu} is used to evaluate the achievable bandwidth for \texttt{cudaMemcpy} transfers between a GPU allocation and a pageable, pinned, or write-combining host allocation. 
The same algorithm can be used for these cases, because the same \texttt{cudaMemcpy} CUDA API call to transfer data can be used on a pointer pointing to any of the allocation types.
Depending on the source and destination types $src$ and $dst$, and the desired host allocation type, the corresponding CUDA or numactl APIs are called to bind later activities to the desired GPU or CPU.
Then, the CUDA or host allocators are invoked to produce $devPtr$ (a pointer to the device allocation) and $hostPtr$ (a pointer to the CPU allocation).
The main benchmark loop uses the \texttt{cudaMemcpy} time as the iteration time that should be reported.

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $bytes$, $num\_iters$}
		\If{$src$ is GPU}
		\State cudaSetDevice($src$)
		\Else \Comment{$src$ is CPU}
		\State numa\_bind\_node($src$)
		\EndIf
		\If{$dst$ is GPU}
		\State cudaSetDevice($dst$)
		\Else \Comment{$dst$ is CPU}
		\State numa\_bind\_node($dst$)
		\EndIf
		
		\State $devPtr \gets$ cudaMalloc($bytes$) \Comment{device allocation}
		\State $hostPtr \gets$ hostAllocate($bytes$) \Comment{appropriate host allocator}

		\If{$src$ is GPU}
		\State $srcPtr \gets devPtr$
		\State $dstPtr \gets hostPtr$
		\Else \Comment{$src$ is CPU}
		\State $srcPtr \gets hostPtr$
		\State $dstPtr \gets devPtr$
		\EndIf

		\State $start \gets$ cudaEventCreate()
		\State $end \gets$ cudaEventCreate()

		\For{state}
		\State cudaEventRecord($start$)
		\State cudaMemcpy($dstPtr$, $srcPtr$, $bytes$, cudaMemcpyDefault)
		\State cudaEventRecord($stop$)
		\State $millis \gets$ cudaEventElapsedTime($start$, $stop$)
		\State state.SetIterationTime($millis$ / $1000$)
        \EndFor
		
		\Return $elapsed$
		\EndFunction
		
	\end{algorithmic}
	\caption[Measuring CPU/GPU bandwidth with \texttt{cudaMemcpy}]{
		Measuring CPU/GPU bandwidth with \texttt{cudaMemcpy}.
		Host allocators are described in Algorithm~\ref{alg:host-allocators}.
		\texttt{numa\_bind\_node} is defined in Listing~\ref{lst:numa-bind-node}.
	}
	\label{alg:explicit-cpu-gpu}
\end{algorithm}

\subsection{Comparison of Pageable, Pinned, and Write-Combining Host Allocations}
\label{sec:explicit-pageable-pinned-wc}

To contextualize other results presented in this chapter, Figure~\ref{fig:pageable-pinned-wc} shows the transfer performance between pageable, pinned, and write-combined allocations on CPU0 and a device allocation on GPU0.
On all tested systems, this is a local transfer between a directly-connected CPU and GPU.
These performance curves exhibit features common throughout this chapter:
\begin{itemize}
	\item For small transfer sizes, the time is dominated by overhead of invoking the transfer.
	\item For large transfer sizes, the performance is dominated by the exercised pysical link (in pinned or write-combining transfers) or some part of the abstraction layer (pageable transfers).
	\item The performance may vary smoothly across intermediate transfer sizes, or exhibit more complicated behavior.
	For example, in the AC922 transfer shown in Figure~\ref{fig:pageable-cpu0-gpu0}, bandwidth peaks and then drops for intermediate transfer sizes before recovering for larger transfers.
\end{itemize}

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pageable_cpu0-to-gpu0.pdf}
		\caption{}
		\label{fig:pageable-cpu0-gpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pinned_cpu0-to-gpu0.pdf}
		\caption{}
		\label{fig:pinned-cpu0-gpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/wc_cpu0-to-gpu0.pdf}
		\caption{}
		\label{fig:wc-cpu0-gpu0}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pageable_gpu0-to-cpu0.pdf}
		\caption{}
		\label{fig:pageable-gpu0-cpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pinned_gpu0-to-cpu0.pdf}
		\caption{}
		\label{fig:pinned-gpu0-cpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/wc_gpu0-to-cpu0.pdf}
		\caption{}
		\label{fig:wc-gpu0-cpu0}
	\end{subfigure}
	\caption[\texttt{CudaMemcpy} bandwidth for CPU0-GPU0 transfers]{
		\texttt{CudaMemcpy} bandwidth vs. transfer size for CPU0 to GPU0 transfers from 
		(a) pageable allocations,
		(b) pinned allocations, and
		(c) write-combining allocations and 
		GPU0 to CPU0 transfers of the same kind (d-f).
		Results for S822LC, AC922, and DGX-1 systems are shown.
		Whiskers show standard deviations of benchmark measurements over five repetitions.
	}
	\label{fig:pageable-pinned-wc}
\end{figure}


The curves for transfers involving pinned or write-combined allocations on CPU0 (Figures~\ref{fig:pageable-pinned-wc}(b,c,e,f)) share a similar shape: the transfer bandwidth is low for small sizes, and eventually saturates once transfers become large enough.
S822LC and DGX-1 achieve \mytilde $75$\% of the theoretical $40$ and $20$ GB/s bandwidths of the NVLink 1.0 x2 and PCIe 3.0 x16 links, respectively.
AC922 achieves nearly $100$\% of the theoretical $75$ GB/s unidirectional NVLink 2.0 x3 bandwidth.

In contrast, Figures~\ref{fig:pageable-pinned-wc}a and \ref{fig:pageable-pinned-wc}d show transfers involving pageable allocations.
The achievable bandwidth for large transfer sizes on S822LC is reduced to approximately $25\%$ of the theoretical $40$ GB/s bandwidth provided by the link.
On AC922 the performance is nearly $50\%$ of the theoretical bandwidth.
AC922 CPU-to-GPU transfers also show a high bandwidth achieved for large CPU-to-GPU transfers, but not for GPU-to-CPU transfers.
DGX-1 bandwidth peaks at $50\%$ of the theoretical $20$ GB/s of one-lane NVLink 1.0, but drops substantially for large transfer sizes.

Section~\ref{sec:pinned-memory} describes how \texttt{cudaMemcpy} from a pageable allocation to the GPU actually causes two data copies: one from the pageable allocation application to a pinned buffer, and a second copy, a DMA from the pinned buffer to the GPU.
When pinned memory transfers are faster than pageable memory, we can infer that the CPU memory copy from pageable allocation to pinned buffer is limiting the performance.
For comparison, consider Figure~\ref{fig:explicit-cpu-cpu}, which shows the performance of using \texttt{cudaMemcpy} to only do a copy from a pageable allocation to a pinned allocation.
For S822LC, the pageable-to-GPU transfer shown in Figure~\ref{fig:pageable-cpu0-gpu0} is approximately the same performance as the pageable-to-pinned transfer shown in Figure~\ref{fig:explicit-cpu-cpu}.

Surprisingly, on AC922, the pageable-to-GPU transfer for large transfer sizes is substantially faster than the pageable-to-pinned transfer that it should be limited by.
This suggests that there is a different implementation for the two cases.
Bandwidth spikes at intermediate sizes suggest that the GPU DMA may directly access data from the CPU cache when the transfer can fit in the cache.
This is further reinforced by the lack of difference between pinned and write-combining transfer bandwidth, which suggests that caching or lack thereof on these systems does not influence the DMA engine.

\subsection{CPU/GPU Bandwidth Measurements}

Figure~\ref{fig:cpu-gpu-affinity-direction} shows CPU/GPU bandwidth on a variety of logical paths for S822LC, AC922, and DGX-1.
Transfers involving pinned and pageable allocations are shown.
Write-combined results are omitted as they match the pinned performance.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_pageable_affinity.pdf}
		\caption{}
		\label{fig:minsky_pageable_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_pageable_affinity.pdf}
		\caption{}
		\label{fig:hal_pageable_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_pageable_affinity.pdf}
		\caption{}
		\label{fig:dgx_pageable_affinity}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_pinned_affinity_cpu0.pdf}
		\caption{}
		\label{fig:minsky_pinned_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_pinned_affinity.pdf}
		\caption{}
		\label{fig:hal_pinned_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_pinned_affinity.pdf}
		\caption{}
		\label{fig:dgx_pinned_affinity}
		
	\end{subfigure}
	\caption[CPU-GPU affinity and \texttt{cudaMemcpy} bandwidth]{
		Transfer bandwidth vs. transfer size for local and remote transfers from pageable and pinned host allocations on S822LC, AC922, and DGX-1.
		(a-c) Transfers from pageable allocations to GPUs.
		(d-f) Transfers from pinned allocations to GPUs.
		(a) and (d) are for S822LC, (b) and (e) for AC922, and (c) and (f) for DGX-1.
	}
	\label{fig:cpu-gpu-affinity-direction}
\end{figure}

In general, the bandwidth follows the same outline described in Section~\ref{sec:explicit-pageable-pinned-wc}, with overhead-dominated time for small transfers, bandwidth-dominated time for large transfers, and some other behavior between.
There are some distinctive reoccurring patterns in Figures~\ref{fig:cpu-gpu-affinity-direction} (a)-(c).

CPU-to-GPU pageable transfers on the IBM systems exhibit peaks in transfer bandwidth at intermediate transfer sizes.
The shape of this curve suggests some insight into the copy implementation.
For example, consider the S822LC CPU0 to GPU0 curve.
The expected process is that a pageable allocation on CPU0 is copied to one or more pinned allocations on CPU0, which are then accessed by GPU0's DMA engine.
The fact that the peak bandwidth at intermediate transfer sizes surpasses the measured bandwidth for single-threaded inter-CPU transfers (Figure~\ref{fig:explicit-cpu-cpu}) suggests that the pageable-to-pinned copy is indeed occurring within a single CPU, and not across CPUs.
The same shape is even present in the CPU0 to GPU2 curve, where it would be plausible for the pageable allocation to be on CPU0 and the pinned allocation on CPU1.
For S822LC as the transfer grows larger, the bandwidth reaches a steady-state value that is approximately the same as the single-threaded CPU-CPU memory access bandwidth.
In AC922, we see a large change in the transfer bandwidth, suggesting that some other implementation is chosen at those sizes.
The drop before reaching that bandwidth suggests some system performance bug, like imbalance in the number and size of the pinned transfer buffers that prevents good overlapping of the host-device DMA and the host-host memory copy.
Finally, on AC922, the local GPU-to-CPU transfer is actually faster than the corresponding intra-CPU \texttt{cudaMemcpy}, again suggesting some difference in implementation when a GPU is involved.
On DGX-1, no similar intermediate spike is observed.
The total transfer bandwidth is capped by the lower interconnect bandwidth, clipping that shape.
Also on DGX-1, the transfer bandwidth at large sizes is seriously degraded, falling well below even the CPU-to-CPU \texttt{cudaMemcpy} bandwidth.
This also may be a performance bug.

Figures~\ref{fig:cpu-gpu-affinity-direction} (d)-(f) show the same transfers, but from pinned allocations.
These transfers are all ultimately limited by the interconnect bandwidth, and do not show the same peaks.
On S822LC, local transfers achieve around 75\% of the NVLink 1.0 bandwidth, while remote transfers achieve $50$-$75$\% of the lower $38.4$ GB/s SMP bus bandwidth.
On Ac922, local transfers achieve around 95\% of the NVLink 2.0 bandwidth, while remote transfers again are limited to around $50$-$75$\% of the 64 GB/s SMP bus.
On DGX-1, both transfers achieve around $60$-$70$ of the PCIe 3.0 bandwidth.

\subsection{Affinity}
\label{sec:explicit-cpu-gpu-affinity}

On systems with high-performance interconnects, transfers from GPU allocations to pageable CPU allocations are strongly correlated with device affinity.
Transfers involving pinned CPU allocations demonstrate a strong effect in both CPU-to-GPU and GPU-to-CPU directions.
The presence of high-performance interconnects further exposes any performance differences, since bandwidth is less likely to be limited by underlying link bandwidth and more likely to be limited by performance bugs or single-threaded memory copies.
Table~\ref{tab:cpu-gpu-affinity} summarizes the effects.

Figure~\ref{fig:minsky_pageable_affinity} shows that affinity has a small effect on bandwidth for pageable transfers on S822LC.
For transfers larger than 4 MB, the GPU-to-CPU remote transfer is faster than the GPU-to-CPU local transfer.
Figure~\ref{fig:hal_pageable_affinity} shows local GPU-to-CPU pageable transfers on AC922 are much faster than their remote counterparts, except for large transfers, where remote performance is slightly higher.
Figure~\ref{fig:dgx_pageable_affinity} shows that GPU-to-CPU pageable transfers on DGX are not affected by affinity.
Figure~\ref{fig:minsky_pinned_affinity} shows on S822LC, pinned bandwidth is correlated with affinity, particularly in the GPU-to-CPU direction.
Figure~\ref{fig:hal_pinned_affinity} shows for pinned transfers on AC922, affinity has an even stronger effect.
Figure~\ref{fig:dgx_pinned_affinity} shows no effect from affinity on pinned transfers for DGX-1.
Without the additional CPU-CPU copy, pinned bandwidth is highly dependent on the bandwidth of the underlying hardware links.
For remote transfers on the IBM machines, the SMP bandwidth is less than the NVLink bandwidth, and limits performance.
On the DGX system, remote and local CPU-GPU transfers all must traverse PCIe 3.0 links, so there is no performance effect from affinity.

\begin{table}[ht]
	\centering
	\caption[Affinity and logical communication bandwidth]{Effect of device affinity on logical transfer bandwidth.}
	\label{tab:cpu-gpu-affinity}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}     & \textbf{S822LC}                                      & \textbf{AC922}                                    & \textbf{DGX-1}                                  \\ \hline 
		Pageable $\rightarrow$ GPU & $\times$   (Fig.~\ref{fig:minsky_pageable_affinity}) & $\times$   (Fig.~\ref{fig:hal_pageable_affinity}) & $\times$ (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pageable $\leftarrow$ GPU  & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & $\times$ (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pinned $\rightarrow$ GPU   & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & $\times$ (Fig.~\ref{fig:dgx_pinned_affinity})  \\ \hline
		Pinned $\leftarrow$ GPU    & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & $\times$ (Fig.~\ref{fig:dgx_pinned_affinity})   \\ \hline
	\end{tabular}
\end{table}

\subsection{Anisotropy}
\label{sec:explicit-cpu-gpu-anisotropy}
\textit{Anisotropy} refers to the property of being directionally-dependent, e.g., CPU/GPU transfer bandwidth is anisotropic if CPU-to-GPU bandwidth is different than GPU-to-CPU bandwidth. 
Figure~\ref{fig:cpu-gpu-affinity-direction} highlights that link bandwidth exhibits significant anisotropy in systems with high-performance interconnects.
Table~\ref{tab:explicit-anisotropy} summarizes the effects.
Particularly for the pageable transfers shown in Figures~\ref{fig:minsky_pageable_affinity} and \ref{fig:hal_pageable_affinity}, corresponding transfers are shown to be highly anisotropic.
In the pinned transfers on S822LC (Fig.~\ref{fig:minsky_pinned_affinity}), all transfers show some degree of anisotropy, with a larger effect over remote transfers.
For transfers where there is a difference, CPU $\rightarrow$ GPU transfers tend to be faster.
For AC922 (Fig.~\ref{fig:hal_pinned_affinity}), remote transfers generally show anisotropy, and local transfers show anisotropy only for intermediate transfer sizes.
For DGX-1, pinned transfers all show 2 GB/s of anisotropy (Fig.~\ref{fig:dgx_pinned_affinity}), while the degree of anisotropy for transfers involving pageable allocations depends on the transfer size (Fig.~\ref{fig:dgx_pageable_affinity}).
For pageable transfers, CPU-to-GPU transfers tend to be faster than GPU-to-CPU transfers.

\begin{table}[H]
	\centering
	\caption[Host-device transfer anisotropy]{Host-device transfer anisotropy.}
	\label{tab:explicit-anisotropy}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}                  & \textbf{S822LC}                                      & \textbf{AC922}                                    & \textbf{DGX-1}                                    \\ \hline 
		Pageable $\leftrightarrow$ GPU (local)  & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pageable $\leftrightarrow$ GPU (remote) & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pinned $\leftrightarrow$ GPU (local)    & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & \checkmark (Fig.~\ref{fig:dgx_pinned_affinity}) \\ \hline
		Pinned $\leftrightarrow$ GPU (remote)   & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & \checkmark (Fig.~\ref{fig:dgx_pinned_affinity})   \\ \hline
	\end{tabular}
\end{table}

\subsection{Differences between Identical Transfers}
\label{sec:explicit-cpu-gpu-identical}

Figure~\ref{fig:explicit-identical} shows cases of different performance on two topologically- or logically-identical links.
Figures~\ref{fig:explicit-identical-s822lc-local}~and~\ref{fig:explicit-identical-s822lc-remote} show transfer bandwidth between a pageable allocation and local or remote GPU on S822LC, respectively.
Figures~\ref{fig:explicit-identical-ac922-local}~and~\ref{fig:explicit-identical-ac922-remote} show transfer bandwidth from a local GPU to a pageable allocation, and a pageable allocation to a remote GPU on AC922, respectively.
Each of these four scenarios involve identical logical and topological links, and yet a substantial transfer bandwidth difference is observed.
Table~\ref{tab:explicit-identical} summarizes scenarios where the transfer performance differs on identical links.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_identical_pageable-to-gpu_local.pdf}
		\caption{}
		\label{fig:explicit-identical-s822lc-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_identical_pageable-to-gpu_remote.pdf}
		\caption{}
		\label{fig:explicit-identical-s822lc-remote}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_identical_gpu-to-pageable_local.pdf}
		\caption{}
		\label{fig:explicit-identical-ac922-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_identical_pageable-to-gpu_remote.pdf}
		\caption{}
		\label{fig:explicit-identical-ac922-remote}
	\end{subfigure}
	\caption[CPU/GPU \texttt{cudaMemcpy} bandwidth on identical links]{
		Cases of CPU/GPU \texttt{cudaMemcpy} bandwidth on identical links.
		(a-b) Transfer bandwidth from pageable allocations to GPUs on S822lc.
		(c) Bandwidth from local GPU to pageable CPU allocation on AC922.
		(d) Bandwidth from pageable CPU allocation to remote GPU on AC922.
	}
	\label{fig:explicit-identical}
\end{figure}

These discrepancies only manifest on systems with high-bandwidth interconnects and pageable transfers.
This suggests that the lower-performance PCIe 3.0 buses mask any similar effects that appear on that system.
Furthermore, it suggests that the causes of these discrepancies are performance bugs in the CUDA system relating to how internal buffers are allocated, or performance bugs in the system firmware related to low-level data transfer.

\begin{table}[ht]
	\centering
	\caption[Transfer rate on identical CPU-GPU links]{Transfer rate variability on identical CPU-GPU links.}
	\label{tab:explicit-identical}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}     & \textbf{S822LC}                                                                                              & \textbf{AC922}                                               & \textbf{DGX-1} \\ \hline 
		Pageable $\rightarrow$ GPU & \checkmark (Figs. \ref{fig:explicit-identical-s822lc-local} and ~\ref{fig:explicit-identical-s822lc-remote}) & \checkmark (Figs. \ref{fig:explicit-identical-ac922-remote}) & $\times$       \\ \hline
		Pageable $\leftarrow$ GPU  & $\times$                                                                                                     & \checkmark (Figs. \ref{fig:explicit-identical-ac922-local})  & $\times$       \\ \hline
		Pinned $\rightarrow$ GPU   & $\times$                                                                                                     & $\times$                                                     & $\times$       \\ \hline
		Pinned $\leftarrow$ GPU    & $\times$                                                                                                     & $\times$                                                     & $\times$       \\ \hline
	\end{tabular}
\end{table}

\section{GPU / GPU Transfers}
\label{sec:explicit-gpu-gpu}

Explicit GPU-GPU transfers are caused by the \texttt{cudaMemcpy} family of functions being invoked on pointers to device allocations created with \texttt{cudaMalloc}.
Unlike the different types of host allocations in Section~\ref{sec:explicit-cpu-gpu}, this section only refers to a single kind of device allocation.
Device allocations come with the concept of peer access, discussed in Section~\ref{sec:cuda-peer}.
This section focuses on the effect of peer access on transfer bandwidth, and cases where transfers are anisotropic or have different performance on identical links.

Algorithms~\ref{alg:explicit-gpu-gpu-peer} and \ref{alg:explicit-gpu-gpu-nopeer} are used to evaluate the achievable GPU-GPU transfer bandwidth with and without peer access enabled.
When peer access is disabled, \texttt{numa\_bind\_node} is used to pin the executing thread to a specific node.
On systems where the CUDA driver does not make NUMA-aware allocations, this may help control for NUMA performance effects.
Then, peer access is enabled or disabled depending on the experimental configuration.
Then, \texttt{cudaMalloc} is used to create allocations of $transfer\_size$ bytes pointed to by $srcPtr$ and $dstPtr$.
The achievable bandwidth is measured during the Benchmark loop using \texttt{cudaEvent}s and \texttt{cudaMemcpy}.

\begin{algorithm}[H]
	\caption[Measuring GPU-GPU \texttt{cudaMemcpy} peer bandwidth]{Measuring GPU-GPU \texttt{cudaMemcpy} bandwidth with peer access enabled.}
	\label{alg:explicit-gpu-gpu-peer}
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $transfer\_size$}
		\State \texttt{cudaSetDevice($src$)}
		\State $srcPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{Source allocation}
		\State \texttt{cudaMemset($srcPtr$, $transfer\_size$, 0)}
		\State \texttt{cudaDeviceEnablePeerAccess($dst$)}
		\State \texttt{cudaSetDevice($dst$)}
		\State $dstPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{Destination allocation}
		\State \texttt{cudaMemset($dstPtr$, $transfer\_size$, 0)}
		\State \texttt{cudaDeviceEnablePeerAccess($src$)}
		
		\State $start \gets$ \texttt{cudaEventCreate()}
		\State $end \gets$ \texttt{cudaEventCreate()}

		\For{state}
		\State \texttt{cudaEventRecord($start$)}
		\State \texttt{cudaMemcpy($dstPtr$, $srcPtr$, $bytes$, cudaMemcpyDefault)}
		\State \texttt{cudaEventRecord($stop$)}
		\State $millis \gets$ cudaEventElapsedTime($start$, $stop$)
		\State state.SetIterationTime($millis$ / $1000$)
        \EndFor
		
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption[Measuring GPU-GPU \texttt{cudaMemcpy} non-peer bandwidth]{
		Measuring GPU-GPU \texttt{cudaMemcpy} bandwidth with peer access disabled.
		\texttt{numa\_bind\_node} is defined in Listing~\ref{lst:numa-bind-node}.}
	\label{alg:explicit-gpu-gpu-nopeer}
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $numa$, $transfer\_size$}

		\State \texttt{numa\_bind\_node($numa$)}
		\State \texttt{cudaSetDevice($src$)}
		\State $srcPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{Source allocation}
		\State \texttt{cudaMemset($srcPtr$, $transfer\_size$, 0)}
		\State \texttt{cudaDeviceDisablePeerAccess($dst$)}
		\State \texttt{cudaSetDevice($dst$)}
		\State $dstPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{Destination allocation}
		\State \texttt{cudaMemset($dstPtr$, $transfer\_size$, 0)}
		\State \texttt{cudaDeviceDisablePeerAccess($src$)}
		
		\State $start \gets$ \texttt{cudaEventCreate()}
		\State $end \gets$ \texttt{cudaEventCreate()}

		\For{state}
		\State \texttt{cudaEventRecord($start$)}
		\State \texttt{cudaMemcpy($dstPtr$, $srcPtr$, $bytes$, cudaMemcpyDefault)}
		\State \texttt{cudaEventRecord($stop$)}
		\State $millis \gets$ cudaEventElapsedTime($start$, $stop$)
		\State state.SetIterationTime($millis$ / $1000$)
        \EndFor
		
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}

\subsection{Transfer Rate and Peer Access}
\label{sec:explicit-peer-bandwidth}

Figure~\ref{fig:explicit-peer} shows the performance of a variety of GPU-GPU transfers with and without peer access.
Generally, peer access has a large effect on the bandwidth of local GPU-GPU transfers.
With peer access enabled, GPUs may do DMAs directly with their peer memory instead of copying through the host.
Without peer access, the execution is pinned to a particular CPU to control the location for the CUDA system making memory allocations.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_memcpy_local.pdf}
		\caption{}
		\label{fig:explicit-s822lc-gpu-gpu-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_memcpy_local.pdf}
		\caption{}
		\label{fig:explicit-ac922-gpu-gpu-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_memcpy_local.pdf}
		\caption{}
		\label{fig:explicit-dgx-gpu-gpu-local}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_memcpy_remote_80.pdf}
		\caption{}
		\label{fig:explicit-s822lc-gpu-gpu-remote}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_memcpy_remote.pdf}
		\caption{}
		\label{fig:explicit-ac922-gpu-gpu-remote}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_memcpy_remote.pdf}
		\caption{}
		\label{fig:explicit-dgx-gpu-gpu-remote}
	\end{subfigure}
	\caption[GPU-GPU \texttt{cudaMemcpy} bandwidth and peer access]{
		GPU-GPU \texttt{cudaMemcpy} transfer bandwidth vs. transfer size for various scenarios with peer access enabled or disabled.
		(a) GPU0 to GPU1 transfer on S822LC with and without peer access.
		(b) GPU-to-GPU local transfers on AC922.
		(c) GPU-to-GPU local transfers on DGX-1.
		(d-f) GPU-to-GPU remote transfers on the same systems.
		S822LC (d) and DGX-1 (f) do not support peer access for remote GPU-GPU transfers.
		S822LC (d) is annotated with the CPU that the non-peer benchmark is pinned to.
		The same is not shown in (e-f) because there is no significant performance effect.
	}
	\label{fig:explicit-peer}
\end{figure}

Figure~\ref{fig:explicit-s822lc-gpu-gpu-local} shows that enabling peer access on S822LC improves the performance of local GPU-GPU transfers by \mytilde$40$\%, to over $90$\% of the theoretical link bandwidth.
Likewise, Figure~\ref{fig:explicit-dgx-gpu-gpu-local} shows peer access roughly doubling performance on DGX-1 and Figure~\ref{fig:explicit-ac922-gpu-gpu-local} shows similar behavior for AC922, with the much higher performance ceiling due to the increased bandwidth of NVLink 2.0.

On S822LC remote transfers (Figure~\ref{fig:explicit-s822lc-gpu-gpu-remote}), pinning the benchmark to the NUMA node with affinity to the sending GPU improves the performance by around $25$\%.
On DGX-1 (Figure~\ref{fig:explicit-dgx-gpu-gpu-remote}), no similar effect is seen.
Any performance differences may be masked by the limited PCIe CPU-GPU bandwidth relative to the CPU-CPU and GPU-GPU bandwidth, so the specific route that the data takes through the system matters less than whether or not the data travels over a slow PCIe bus.
The performance difference may be masked by the fact that GPU-CPU interconnects are slower than CPU-CPU interconnects on that system, so when the data transfers back.
On AC922 (Figure~\ref{fig:explicit-ac922-gpu-gpu-remote}), there is no substantial effect for large transfers for disabling peer access.

\subsection{Transfer Rate on Identical Transfers}
\label{sec:explicit-peer-identical}

Different performance is observed on identical GPU-GPU transfers when peer access is disabled.
Figure~\ref{fig:explicit-nopeer-identical} shows some example scenarios.
In isolation, there is no reason to disable peer access for local transfers, as it always reduces performance.
Therefore, it is unlikely that a practical program would ever exercise this scenario.
During cases of contention, peer access could be utilized to divert data along a different hardware path, so these results are presented for completeness.

Figure~\ref{fig:explicit-nopeer-identical-s822lc-local} compares the bandwidth of transfers from GPU0 to GPU1 (pinned to local CPU0) and GPU2 to GPU3 (pinned to local CPU1) on S822LC.
These are identical transfers on different CPU-GPU-GPU triads that make up S822LC (Figure~\ref{fig:topo-minsky-simple}).
The transfer between GPU0 and GPU1 is around $30$\% slower than the same transfer between GPU2 and GPU3.

Similarly, Figure~\ref{fig:explicit-nopeer-identical-dgx-local} shows variability in transfer bandwidths observed on DGX-1.
As shown in Figure~\ref{fig:topo-minsky-actual}, there are three PCIe bridges between GPU0 and GPU1, and there are seven PCIe bridges between GPU2 and GPU3.
The bandwidth between GPU0 and GPU2/GPU3 is identical, as are the topologies between them.
Although the topology between GPU0 and GPU1 is shorter, the performance is also lower.
It is possible that the different bridges have different performance characteristics.

Table~\ref{tab:explicit-identical} summarizes the cases where differing performance is observed.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_nopeer_identical-local.pdf}
		\caption{}
		\label{fig:explicit-nopeer-identical-s822lc-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_nopeer_identical.pdf}
		\caption{}
		\label{fig:explicit-nopeer-identical-dgx-local}
	\end{subfigure}
	\caption[GPU-GPU \texttt{cudaMemcpy} Bandwidth on Identical Links]{
		Transfer bandwidth vs. transfer size for S822LC and DGX-1.
		(a) Differing transfer bandwidth on logically-identical local transfers on S822LC.
		(b) The same for DGX-1.
		AC922 is omitted as no performance variability is present when controlling for NUMA pinning during non-peer transfers.
	}
	\label{fig:explicit-nopeer-identical}
\end{figure}

\begin{table}[ht]
	\centering
	\caption[Transfer rate on identical GPU-GPU links]{Transfer rate on identical GPU-GPU links}
	\label{tab:explicit}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}                                           & S822LC                                                              & AC922     & DGX-1                                                           \\ \hline 
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer enabled)  } & $\times$                                                            & $\times$  & $\times$                                                        \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer enabled)  } & N/A                                                                 & $\times$  & N/A                                                             \\ \hline
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-s822lc-local})  & $\times$  & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-dgx-local}) \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer disabled) } & $\times$                                                            & $\times$  & $\times$                                                        \\ \hline
	\end{tabular}
\end{table}

\section{Summary}

The performance of \texttt{cudaMemcpy} transfers is highly dependent on device affinity, CPU allocation type, transfer direction, and underlying hardware performance.
Transfers involving pageable host allocations are particularly unpredictable, probably due to the performance of combined intra-CPU communication, inter-CPU communication, cache effects, and DMA being difficult to tune for all cases.
In general, constraining communication to pinned buffers and local devices offers the best performance, though direction of the transfer still has a large impact.
Some CPU-GPU pageable transfers exhibit different performance even when the logical communication is the same.
This is also present in GPU-GPU transfers, but only for local transfers when peer-access is disabled.
