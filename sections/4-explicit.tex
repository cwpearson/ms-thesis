\chapter{Explicit Memory Performance}
\label{ch:explicit}

This chapter examines the performance of explicit data transfers over logical communication links presented in a system with NUMA and CUDA interfaces.
\begin{itemize}
	\item Section~\ref{sec:explicit-cpu-cpu} describes CPU-to-CPU transfers.
	\item Section~\ref{sec:explicit-cpu-gpu} describes transfers between CPUs and GPUs.
	\item Section~\ref{sec:explicit-gpu-gpu} describes GPU-to-GPU transfers.
\end{itemize}
It highlights cases where the observed logical communication performance deviates significantly from the symmetries present in the CUDA API, numactl API, and hardware.
Those deviances take the form of different performance on identical links, anisotropic link performance, or performance affected by device affinity.

\section{CPU / CPU Transfers}
\label{sec:explicit-cpu-cpu}

This section begins by examining CPU-CPU transfer performance by using multiple threads to read remote data in an attempt to saturate the SMP bus and memory controllers.
This attempts to simulate the maximum possible performance that the CUDA API could achieve while sending data between sockets for CPU-to-GPU transfers.
Algorithm~\ref{alg:explicit-cpu-cpu} describes the approach.

Two types of traffic are measured, in \texttt{read\_bandwidth} and \texttt{write\_bandwidth}.
In the \texttt{read} operation (Listing~\ref{lst:explicit-cpu-cpu-cpuread}), source-to-destination bandwidth is measured by a set of worker threads on the destination CPU generating loads for an allocation on the source CPU.
In the \texttt{write} operation (Listing~\ref{lst:explicit-cpu-cpu-cpuwrite}), bandwidth is measured by worker threads on the source CPU writing to an allocation on the destination CPU.
First, an allocation is created on the source CPU
Then, the worker threads are bound to the desired CPU, and the \texttt{cpu\_read} or \texttt{cpu\_write} workload is executed.
The read and write operations are executed on data members 8 bytes in size, to reflect the 8-byte payload on the X bus~\cite{caldeira2016s822lc}.

\begin{lstlisting}[language=C++, caption=CPU-CPU Read Function, label=lst:explicit-cpu-cpu-cpuread]
void cpu_read(double *__restrict__ dummy,
                double *__restrict__ ptr,
                const size_t count,
                const size_t stride) {
    const size_t numElems = count / sizeof(double);
    const size_t elemsPerStride = stride / sizeof(double);
    double acc = 0;
#pragma omp parallel for schedule(static)
    for (size_t i = 0; i < numElems; i += elemsPerStride) {
        acc += ptr[i];
    }
    *dummy += acc;
}
\end{lstlisting}


\begin{lstlisting}[language=C++, caption=CPU-CPU Write Function, label=lst:explicit-cpu-cpu-cpuwrite]
void cpu_write(double *__restrict__ ptr, 
                 const size_t count, 
                 const size_t stride) {
    const size_t numElems = count / sizeof(double);
    const size_t elemsPerStride = stride / sizeof(double);
#pragma omp parallel for schedule(static)
    for (size_t i = 0; i < numElems; i += elemsPerStride) {
        ptr[i] = i * 31ul + 7ul;
    }
}
\end{lstlisting}

For both the \texttt{cpu\_read} and \texttt{cpu\_write} operations, \texttt{ptr} is the allocation that is being read or written,
\texttt{count} is the number of bytes in that allocation, and \texttt{stride} is the number of bytes between every read or write.
Since memory loads in isolation are side-effect free, \texttt{cpu\_read} is designed to avoid being optimized away by the compiler.
The array values are accumulated and the sum is written out through the \texttt{dummy} argument.
This takes a small amount of time that is incorrectly attributed to the elapsed bandwidth time.
For both operations, OpenMP is used to evenly distribute the read or write operations amongst the cores of the executing CPU.
Entering and exiting the parallel region incurs some cost, which is counted in the bandwidth computation.

\begin{algorithm}
	\caption[test]{
		Algorithm to measure CPU-CPU Bandwidth.
		\texttt{cpu\_read} is defined in Listing~\ref{lst:explicit-cpu-cpu-cpuread} and \texttt{cpu\_write} is defined in Listing~\ref{lst:explicit-cpu-cpu-cpuwrite}.
		\texttt{bind\_cpu} binds the executing thread to the provided NUMA node.
		\texttt{walltime} returns the current wall time.
	}
	\label{alg:explicit-cpu-cpu}
	\begin{algorithmic}[1]
		\Statex
        
        \Function{numa\_bind}{$dev$}
        \State \texttt{bind\_cpu($dev$)}
        \For{all worker threads}
        \State \texttt{bind\_cpu($dev$)}
        \EndFor
        \EndFunction

        \Statex
		\Function{read\_bandwidth}{$dst$, $src$, $transfer\_size$, $num\_workers$}
		\State \texttt{omp\_set\_num\_threads($num\_workers$)}
		\State \texttt{numa\_bind($src$)}
        \State $srcPtr \gets$ \texttt{malloc($transfer\_size$)}
        \State \texttt{memset($srcPtr$, 0, $transfer\_size$)}
		
		\State \texttt{numa\_bind($dst$)}
		\Statex

		\State $start \gets$ walltime()
		\State \texttt{cpu\_read($dst$, $transfer\_size$, $8$)}
		\State $end \gets$ walltime()
        \State $elapsed \gets$ $end-start$
		\State \Return $elapsed$
		\EndFunction
        
        \Statex
		\Function{write\_bandwidth}{$dst$, $src$, $transfer\_size$, $num\_workers$}
		\State \texttt{numa\_bind($dst$)}
		\State $srcPtr \gets$ \texttt{malloc($transfer\_size$)}
		\State \texttt{memset($srcPtr$, 0, $transfer\_size$)}
		
		\State \texttt{numa\_bind($src$)}
		\State \texttt{omp\_set\_num\_threads($num\_workers$)}
		\Statex
		
		\State $start \gets$ walltime()
		\State \texttt{cpu\_write($src$, $transfer\_size$, 8)}
		\State $end \gets$ walltime()
        \State $elapsed \gets$ $end-start$
		\State \Return $elapsed$
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}


Figure~\ref{fig:cpu0-cpu1} shows the performance achieved by the \texttt{cpu\_read} and \texttt{cpu\_write} workloads.
The measurement is taken multiple times with varying $num\_workers$, for each $transfer\_size$, and the maximum observed bandwidth at each transfer size is reported.
The highest performing number of workers is highlighted at each transfer size.
For smaller transfers, fewer workers may result in a faster transfer time as the overhead of entering the parallel region is not amortized effectively.

For both workloads, the achievable bandwidth for low transfer sizes is limited by the overhead introduced by the workload function calls and OpenMP runtime.
For large transfer sizes, the performance flattens out as the elapsed time is dominated by the actual data transfer over the SMP bus, achieving $30$ GB/s in the read workload and $25$ GB/s in the write workload on S822LC.
This is consistent with the theoretical 38.4 GB/s performance of the X bus on S822LC.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_cpu0-cpu1-dst.pdf}
		\caption{}
		\label{fig:s822lc-cpu0-cpu1-dst}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_cpu0-cpu1-src.pdf}
		\caption{}
		\label{fig:s822lc-cpu0-cpu1-src}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_cpu0-cpu1-dst.pdf}
		\caption{}
		\label{fig:ac922-cpu0-cpu1-dst}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_cpu0-cpu1-src.pdf}
		\caption{}
		\label{fig:ac922-cpu0-cpu1-src}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_cpu0-cpu1-dst.pdf}
		\caption{}
		\label{fig:dgx-cpu0-cpu1-dst}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_cpu0-cpu1-src.pdf}
		\caption{}
		\label{fig:dgx-cpu0-cpu1-src}
	\end{subfigure}
	\caption[CPU0 to CPU1 transfer bandwidth]{
		CPU0 to CPU1 transfer bandwidth vs. transfer size.
		The bandwidth achieved by one through 16 simultaneous worker threads is shown, with the maximum observed bandwidth across the number of threads highlighted.
		(a,c,e) show multi-threaded reads from CPU1 of an allocation on CPU0's NUMA node on S822LC, AC922, and DGX-1.
		(b,d,f) show multi-threaded writes from CPU0 to an allocation on CPU1's NUMA Node on S822LC, AC922, and DGX-1.
	}
	\label{fig:cpu0-cpu1}
\end{figure}


\section{CPU / GPU Transfers}
\label{sec:explicit-cpu-gpu}

Explicit CPU-GPU transfers are caused by the \texttt{cudaMemcpy} family of functions being invoked on one pointer to a host allocation and one pointer to a device allocation.
The host allocation may be pageable (created by \texttt{malloc} or \texttt{new}), or be created by \texttt{cudaMallocHost} or \texttt{cudaHostAlloc}.
The device allocation is created by \texttt{cudaMalloc}.
This section covers
\begin{itemize}
	\item Comparing bandwidth achievable from pinned, pageable, and write-combining host allocations (Section~\ref{sec:explicit-pageable-pinned-wc})
	\item The effect of device affinity on transfer performance (Section~\ref{sec:explicit-cpu-gpu-affinity})
	\item Cases of observed anisotropic transfer properties (Section~\ref{sec:explicit-cpu-gpu-anisotropy})
	\item Cases of differing performance on identical links (Section~\ref{sec:explicit-cpu-gpu-identical})
\end{itemize}

Algorithm~\ref{alg:explicit-cpu-gpu} is used to evaluate the achievable bandwidth for \texttt{cudaMemcpy} transfers between a GPU allocation and a pageable, pinned, or write-combining host allocation. 
The same algorithm can be used for these cases, because the same \texttt{cudaMemcpy} CUDA API call to transfer data can be used on a pointer pointing to any of the allocation types.
Depending on the source and destination types $src$ and $dst$, and the desired host allocation type, the corresponding CUDA or numactl APIs are called to bind later activities to the desired GPU or CPU.
Then, the CUDA and system allocators are invoked to produce $devPtr$ (a pointer to the device allocation) and $srcPtr$ (a pointer to the CPU allocation).
Then, \texttt{cudaMemcpy} is invoked $num\_iters$ times, and the fastest result is reported.
This helps remove any jitter from the results.
Time is recorded by wrapping \texttt{cudaMemcpy} in a call to the operating system clock.
Consequently, the reported time includes the actual transfers, as well as any overhead for invoking the transfer.
For pageable transfers, a \texttt{cudaDeviceSynchronize} is also included in the time, as cudaMemcpy may return before the device DMA is finished.


\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$}
		\If{$src$ is GPU}
		\State \texttt{cudaSetDevice($src$}
		\Else \Comment{$src$ is CPU}
		\State \texttt{numa\_bind($src$}
		\EndIf
		\If{$dst$ is GPU}
		\State \texttt{cudaSetDevice($dst$)}
		\Else \Comment{$dst$ is CPU}
		\State \texttt{numa\_bind($dst$)}
		\EndIf
		
		\State $devPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{device allocation}
		\State $srcPtr \gets$ \texttt{malloc($transfer\_size$)} \Comment{or cudaHostAlloc()}
		
		\State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
		\For{$i \gets 1 \textrm{ to } num\_iters$}
		\State $start \gets$ walltime()
		\State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
		\State \texttt{cudaDeviceSynchronize()}
		\State $end \gets$ walltime()
		\State $elapsed \gets$ min($elapsed$, $end-start$)
		\EndFor
		
		\Return $elapsed$
		\EndFunction
		
	\end{algorithmic}
	\caption[Measuring CPU/GPU bandwidth with \texttt{cudaMemcpy}]{
        Measuring CPU/GPU bandwidth with \texttt{cudaMemcpy}.
	}
	\label{alg:explicit-cpu-gpu}
\end{algorithm}

\subsection{Comparison of Pageable, Pinned, and Write-Combining Host Allocations}
\label{sec:explicit-pageable-pinned-wc}

To contextualize other results presented in this chapter, Figure~\ref{fig:pageable-pinned-wc} shows the transfer performance from pageable, pinned, and write-combined allocations on CPU0 to device allocations on GPU0.
These performance curves exhibit features common throughout this chapter:
\begin{itemize}
	\item For small transfer sizes, the time is dominated by overhead introduced by the abstraction layer.
	\item For large transfer sizes, the time is dominated by bandwidth limits on the exercised physical link.
	\item The performance may vary smoothly across intermediate transfer sizes, or exhibit more complicated behavior.
	For example, in the AC922 transfer show in Figure~\ref{fig:pageable-cpu0-gpu0}, there is a peak in bandwidth for intermediate transfer sizes before the performance drops for larger transfers.
	Other curves do not show this behavior.
\end{itemize}

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pinned_cpu0-gpu0.pdf}
		\caption{}
		\label{fig:pinned-cpu0-gpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pageable_cpu0-gpu0.pdf}
		\caption{}
		\label{fig:pageable-cpu0-gpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/wc_cpu0-gpu0.pdf}
		\caption{}
		\label{fig:wc-cpu0-gpu0}
	\end{subfigure}
	\caption[\texttt{cudaMemcpy} bandwidth for CPU0-GPU0 transfers.]{
		\texttt{cudaMemcpy} bandwidth vs transfer size for CPU0 to GPU0 transfers from 
		(a) pageable allocations,
		(b) pinned allocations, and
		(c) write-combining allocations.
		Results for S822LC, AC922, and DGX-1 systems are shown.
	}
	\label{fig:pageable-pinned-wc}
\end{figure}


The curves for transfers from a pinned allocation on CPU0 to GPU0 shown in Figure~\ref{fig:pinned-cpu0-gpu0} share a similar shape: the transfer bandwidth is low for small sizes, and eventually saturates once transfers become large enough.
For small transfer sizes, the elapsed time is dominated by overhead from invoking the transfer (and the device synchronization for the pageable transfer).
This causes the upward slope as transfer size increases.
For larger transfers, the time is dominated by the actual cost of moving the data, and the performance saturates.
Both S822LC and AC922 achieve \mytilde 75\% of the theoretical performance allowed by the relevant interconnects, and DGX-1 achieves around 50\%.

In contrast, Figure~\ref{fig:pageable-cpu0-gpu0} shows transfers from pageable host allocations to GPU0.
The achievable bandwidth for large transfer sizes on S822LC is reduced to approximately 50\% of the theoretical $40$ GB/s bandwidth provided by the link.
On AC922 the performance is actually even lower, at only 15 GB/s or 20\% of the theoretical bandwidth.
AC922 also shows a transfer bandwidth peak at itermediate sizes that is not present on the other machines.
Similar peaks show up during other pageable transfters later in this section, 
DGX-1 remains around the same performance level at $10$ GB/s out of a theoretical maximum $20$ GB/s for one-lane NVLink 1.0.
Figure~\ref{fig:wc-cpu0-gpu0} shows that write-combining pinned allocations do not have a measurable performance impact on the testest systems.

Section~\ref{sec:pinned-memory} describes how \texttt{cudaMemcpy} from a pageable allocation to the GPU actually causes two data copies: one from the pageable allocation application to a pinned buffer, and a second copy, a DMA from the pinned buffer to the GPU.
When pinned memory transfers are faster than pageable memory, we can infer that the CPU memory copy from pageable allocation to pinned buffer is limiting the performance.
Figure~\ref{fig:explicit-pageable-cpu} tries to isolate the effect of the pageable-to-pinned copy by using \texttt{cudaMemcpy} to only do a copy from a pageable allocation to a pinned allocation.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{figures/generated/page-to-pin.pdf}
	\caption[Intra-CPU \texttt{cudaMemcpy} Bandwidth]{
        \texttt{cudaMemcpy} Bandwidth between same-CPU Pageable and Pinned Allocation
    }
	\label{fig:explicit-pageable-cpu}
\end{figure}

On S822LC and AC922, the pageable-to-GPU transfer for large transfer sizes is actually faster than the pageable-to-pinned transfer that is should be limited by.
This suggests that there is a different implementation for the two cases.
There is a spike at itermediate sizes, which suggests that the copy can occur solely within the CPU cache.
On DGX-1, there is a less pronounced spike.

\subsection{Measurements}

Figure~\ref{fig:cpu-gpu-affinity-direction} shows CPU/GPU bandwidth on a variety of logical paths for S822LC, AC922, and DGX-1.
Transfers involving pinned and pageable allocations are shown.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/minsky_pageable_affinity.pdf}
		\caption{}
		\label{fig:minsky_pageable_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_pageable_affinity.pdf}
		\caption{}
		\label{fig:hal_pageable_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_pageable_affinity.pdf}
		\caption{}
		\label{fig:dgx_pageable_affinity}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/minsky_pinned_affinity_cpu0.pdf}
		\caption{}
		\label{fig:minsky_pinned_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_pinned_affinity.pdf}
		\caption{}
		\label{fig:hal_pinned_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_pinned_affinity.pdf}
		\caption{}
		\label{fig:dgx_pinned_affinity}
		
	\end{subfigure}
	\caption[CPU-GPU affinity and \texttt{cudaMemcpy} bandwidth]{
		Transfer bandwidth vs transfer size for local and remote transfers from pageable and pinned host allocations on S822LC, AC922, and DGX-1.
		(a-c) show transfers from pageable allocations to GPUs.
		(d-f) show transfers from pinned allocations to GPUs.
		(a) and (d) are for S822LC, (b) and (e) for AC922, and (c) and (f) for DGX-1.
	}
	\label{fig:cpu-gpu-affinity-direction}
\end{figure}

In general, the bandwidth follows the same outline described in Section~\ref{sec:explicit-pageable-pinned-wc}, with overhead-dominated time for small transfers, bandwidth-dominated time for large transfers, and some other behavior between.
There are some distinctive reocurring patterns in Figures~\ref{fig:cpu-gpu-affinity-direction} (a)-(c).
For CPU-to-GPU pageable transfers on all systems, remote GPU-to-CPU transfers on S822LC, and local CPU-to-GPU transfers on AC922, intermediate transfer bandwidths show peaks of varying strengths.
This may be due to the transfer from the pageable allocation to pinned buffer occuring within the cache, so the ultimate transfer bandwidth is limited by the interconnect bandwidth instead of the CPU single-threaded memory bandiwdth.
There may also be some imbalance in the number and size of the pinned transfer buffers that prevents good overlapping of the host-device DMA and the host-host memory copy.
AC922 also exhibits a strong peak, drop, and return to increased performance.

Figures~\ref{fig:cpu-gpu-affinity-direction} (d)-(f) show the same transfers, but from pinned allocations.
These transfers are all limited by the interconnect bandwidth, and do not show the same peaks.

\subsection{Affinity}
\label{sec:explicit-cpu-gpu-affinity}

On systems with high-performance interconnects, CPU/GPU transfer performance is strongly correlated with device affinity.
Furthermore, the high performance interconnects do tend to allow higher bandwidth transfers, but can also expose some surprising behavior.
Table~\ref{tab:cpu-gpu-affinity} summarizes the effects.
Figure~\ref{fig:minsky_pageable_affinity} show that affinity has a negligible effect on bandwidth for pagable transfers on S822LC.
Figure~\ref{fig:hal_pageable_affinity} shows a large bandiwdth difference for small and large transfers from pageable allocations to the GPU on AC922.
Figure~\ref{fig:dgx_pageable_affinity} shows that GPU-to-CPU pageable transfers on DGX are affected by affinity.
Figure~\ref{fig:minsky_pinned_affinity} shows on S822LC, pinned bandwidth is affected both by affinity with local transfers at least $5$ GB/s faster than remote transfers at large sizes.
Figure~\ref{fig:hal_pinned_affinity} shows for pinned transfers on AC922, affinity has a much stronger effect.
Figure~\ref{fig:dgx_pinned_affinity} shows no effect from affinity on pinned transfers for DGX-1.

\begin{table}[ht]
	\centering
	\caption[Affinity and Logical Communication Bandwidth]{Effect of Device Affinity on Logical Transfer Bandwidth}
	\label{tab:cpu-gpu-affinity}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}     & \textbf{S822LC}                                      & \textbf{AC922}                                    & \textbf{DGX-1}                                  \\ \hline 
		Pageable $\rightarrow$ GPU & $\times$   (Fig.~\ref{fig:minsky_pageable_affinity}) & $\times$   (Fig.~\ref{fig:hal_pageable_affinity}) & $\times$ (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pageable $\leftarrow$ GPU  & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pinned $\rightarrow$ GPU   & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & $\times$  (Fig.~\ref{fig:dgx_pinned_affinity})  \\ \hline
		Pinned $\leftarrow$ GPU    & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & $\times$ (Fig.~\ref{fig:dgx_pinned_affinity})   \\ \hline
	\end{tabular}
\end{table}

\subsection{Anisotropy}
\label{sec:explicit-cpu-gpu-anisotropy}
\textit{Anisotropy} is the property of being directionally-dependent, e.g., CPU/GPU link bandwidth is anisotropic if CPU-to-GPU bandwidth is different than GPU-to-CPU bandwidth. 
Figure~\ref{fig:cpu-gpu-affinity-direction} highlights that systems with high-performance interconnects, link bandwidth exhibits significant anisotropy.
Table~\ref{tab:explicit-anisotropy} summarizes the effects.
Particularly for the pageable transfers shown in Figures~\ref{fig:minsky_pageable_affinity} and \ref{fig:hal_pageable_affinity}, corresponding transfers are shown to be highly anisotropic.
In the pinned transfers on S822LC (Fig.~\ref{fig:minsky_pinned_affinity}), all transfers show some degree of anisotropy, with a larger effect over remote transfers.
For AC922, (Fig.~\ref{fig:hal_pinned_affinity}), local transfers show anisotropy only for intermediate transfer sizes, and remote transfers do generally.
For DGX-1, pinned transfers all show 2 GB/s of anisotropy (Fig.~\ref{fig:dgx_pinned_affinity}), while the degree of anisotropy for transfers involving pageable allocations depends on the affinity (Fig.~\ref{fig:dgx_pageable_affinity}).

\begin{table}[H]
	\centering
	\caption[Host-Device Transfer Anisotropy]{Host-Device Transfer Anisotropy}
	\label{tab:explicit-anisotropy}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}                  & \textbf{S822LC}                                      & \textbf{AC922}                                    & \textbf{DGX-1}                                    \\ \hline 
		Pageable $\leftrightarrow$ GPU (local)  & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pageable $\leftrightarrow$ GPU (remote) & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pinned $\leftrightarrow$ GPU (local)    & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & \checkmark (Fig.~\ref{fig:dgx_pinned_affinity}) \\ \hline
		Pinned $\leftrightarrow$ GPU (remote)   & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & \checkmark (Fig.~\ref{fig:dgx_pinned_affinity})   \\ \hline
	\end{tabular}
\end{table}

\subsection{Differences between Identical Transfers}
\label{sec:explicit-cpu-gpu-identical}

Figure~\ref{fig:minsky_pageable_cpu1-gpu01} shows transfer performance on two different identical links: CPU1-GPU0 and CPU1-GPU1.
Both of the scenarios presented involve identical hardware and logical links, yet there is a 10\% bandwidth different at intermediate sizes.
Table~\ref{tab:explicit-identical} summarizes scenarios where the transfer performance differs on identical links.
S822LC is the only system where this behavior is observed for CPU-GPU transfers.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/generated/minsky_pageable_cpu1-gpu01.pdf}
	\caption[Different Performance on Identical S822LC Links]{
        S822LC transfer bandwidth vs. transfer size for a pageable allocation on CPU1 to an allocation on GPU0 or GPU1.
	}
	\label{fig:minsky_pageable_cpu1-gpu01}
\end{figure}

\begin{table}[ht]
	\centering
	\caption[Transfer Rate Variability on Identical Links]{Transfer Rate Variability on Identical Links}
	\label{tab:explicit-identical}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}     & \textbf{S822LC}                                        & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
		Pageable $\rightarrow$ GPU & \checkmark (Fig.~\ref{fig:minsky_pageable_cpu1-gpu01}) & $\times$       & $\times$       \\ \hline
		Pageable $\leftarrow$ GPU  & $\times$                                               & $\times$       & $\times$       \\ \hline
		Pinned $\rightarrow$ GPU   & $\times$                                               & $\times$       & $\times$       \\ \hline
		Pinned $\leftarrow$ GPU    & $\times$                                               & $\times$       & $\times$       \\ \hline
	\end{tabular}
\end{table}

\section{GPU / GPU Transfers}
\label{sec:explicit-gpu-gpu}

Explicit GPU-GPU transfers are caused by the \texttt{cudaMemcpy} family of functions being invoked on pointers to device allocations created with \texttt{cudaMalloc}.
Unlinke the different types of host allocations CPU-GPU transfers described in Section~\ref{sec:explicit-cpu-gpu}, this section only refers to a single kind of device allocation.
Device allocations come with the concept of peer access, discussed in Section~\ref{sec:cuda-peer}.
This section covers
\begin{itemize}
	\item The effect of peer access on transfer bandwidth (Section~\ref{sec:explicit-peer-bandwidth})
	\item Cases of observed anisotropic transfer properties (Section~\ref{sec:explicit-peer-direction})
	\item Cases of differing performance on identical links (Section~\ref{sec:explicit-peer-identical})
\end{itemize}

Algorithm~\ref{alg:explicit-gpu-gpu} is used to evaluate the achievable GPU-GPU transfer bandwidth.
First, peer access is enabled or disabled depending on the experimental configuration.
Then, \texttt{cudaMalloc} is used to create allocations of $transfer\_size$ bytes pointed to by $srcPtr$ and $dstPtr$.
Finally, the achievable bandwidth is measured using the wall time for $num\_iters$ iterations, and the minimum elapsed time is reported, to help remove jitter from the results.

\begin{algorithm}[H]
	\caption[Measuring GPU-GPU \texttt{cudaMemcpy} Bandwidth]{Measuring GPU-GPU \texttt{cudaMemcpy} Bandwidth}
	\label{alg:explicit-gpu-gpu}
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$, $peer\_access$}
		\If{$peer\_access$}
		\State \texttt{cudaSetDevice($src$)}
		\State \texttt{cudaDeviceEnablePeerAccess($dst$)}
		\State \texttt{cudaSetDevice($dst$)}
		\State \texttt{cudaDeviceEnablePeerAccess($src$)}
		\Else
		\State \texttt{cudaSetDevice($src$)}
		\State \texttt{cudaDeviceDisablePeerAccess($dst$)}
		\State \texttt{cudaSetDevice($dst$)}
		\State \texttt{cudaDeviceDisablePeerAccess($src$)}        
		\EndIf
		
		\State \texttt{cudaSetDevice($src$)} \Comment{Source allocation}
		\State $srcPtr \gets$ \texttt{cudaMalloc($transfer\_size$)}
		
		\State \texttt{cudaSetDevice($dst$)} \Comment{Destination allocation}
		\State $dstPtr \gets$ \texttt{cudaMalloc($transfer\_size$)}
		
		\State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
		\For{$i \gets 1 \textrm{ to } num\_iters$}
		\State $start \gets$ walltime()
		\State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
		\State $end \gets$ walltime()
		\State $elapsed \gets$ min($elapsed$, $end-start$)
		\EndFor
		
		\Return $elapsed$
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}

\subsection{Transfer Rate and Peer Access}
\label{sec:explicit-peer-bandwidth}

Figure~\ref{fig:explicit-peer} shows the performance of a variety of GPU-GPU transfers with and without peer access.
Generally, peer access has a large effect on the bandwidth of local GPU-GPU transfers.
With peer access enabled, GPUs may do DMAs directly with their peer memory instead of copying through the host.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/minsky_memcpy_local.pdf}
		\caption{}
		\label{fig:explicit-s822lc-peer}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_peer_local.pdf}
		\caption{}
		\label{fig:explicit-hal-peer-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_peer_remote.pdf}
		\caption{}
		\label{fig:explicit-hal-peer-remote}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_peer_nopeer_local.pdf}
		\caption{}
		\label{fig:explicit-dgx-peer-nopeer-local}
	\end{subfigure}
	
	\caption[GPU-GPU \texttt{cudaMemcpy} bandwidth and peer access]{
		GPU-GPU \texttt{cudaMemcpy} transfer bandwidth vs. transfer size for various scenarios with peer access enabled or disabled.
		(a) shows a GPU0 to GPU1 transfer on S822LC with and without peer access,
		(b) shows GPU to GPU local transfers on AC922.
		(c) shows GPU to GPU remote transfers on AC922.
		(d) shows GPU to GPU local transfers on DGX-1.
		S822LC and DGX-1 do not support peer access for remote GPU-GPU transfers.
	}
	\label{fig:explicit-peer}
\end{figure}

Figure~\ref{fig:explicit-s822lc-peer} shows that enabling peer access on S822LC improves the performance of local GPU-GPU transfers by \mytilde40\%, to over 75\% of the theoretical link bandwidth.
Likewise, Figure~\ref{fig:explicit-dgx-peer-nopeer-local} shows peer access roughly doubling performance on DGX-1 and Figure~\ref{fig:explicit-hal-peer-local} shows similar behavior for AC922, with the much higher performance ceiling due to the increased bandwidth of NVLink 2.0.
On AC922, GPU2 and GPU3 show lower performance than GPU0 and GPU1 when peer access is disabled.
This may be because the CUDA staging buffers needed during this transfer are allocated on a NUMA node remote to the GPUs, so the transfer must copy over the X bus unecessarily.
For remote transfers on AC922, shown in Figure~\ref{fig:explicit-hal-peer-remote}, enabling peer access actually reduces the performance, though the effect is not very large.
Table~\ref{tab:explicit-peer-rate} summarizes the scenarios where peer access affects transfer bandwidth.

\begin{table}[H]
	\centering
	\caption[Effect of Peer Access on Transfer Rate]{Effect of Peer Access on Transfer Rate}
	\label{tab:explicit-peer-rate}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}       & \textbf{S822LC}                                   & \textbf{AC922}                                       & \textbf{DGX-1}                                             \\ \hline 
		GPU $\rightarrow$ Local GPU  & \checkmark  (Fig.~\ref{fig:explicit-s822lc-peer}) & \checkmark (Fig.~\ref{fig:explicit-hal-peer-local})  & \checkmark (Fig.~\ref{fig:explicit-dgx-peer-nopeer-local}) \\ \hline
		GPU $\rightarrow$ Remote GPU & N/A                                               & \checkmark (Fig.~\ref{fig:explicit-hal-peer-remote}) & N/A                                                        \\ \hline
	\end{tabular}
\end{table}


\subsection{Transfer Anisotropy with Peer Access Disabled}
\label{sec:explicit-peer-direction}

Disabling peer access causes anisotropy to be observed in GPU-to-GPU transfers on S822LC and AC922.
Figure~\ref{fig:explicit-peer-anisotropy} highlights anisotropic remote GPU-GPU transfers on S822LC and AC922.
No anisotropy is observed on DGX-1.
This anisotropy is not consistent with anisotropy observed on the intervening links.
For example, consider Figure~\ref{fig:explicit-peer-anisotropy}a-c.
Figure~\ref{fig:minsky-explicit-nopeer-remote} shows anisotropy along the remote GPU0-GPU2 transfer.
Figures~\ref{fig:minsky-explicit-path-gpu0-gpu2}~and~\ref{fig:minsky-explicit-path-gpu2-gpu0} show pinned transfer speeds along GPU0-CPU0-CPU1-GPU2 and GPU2-CPU1-CPU0-GPU0 paths.
The observed GPU0-GPU2 bandwidth is sometimes higher than the higher observed bandwidth on the path components, and sometimes lower than the lowest path component.


\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_remote.pdf}
		\caption{}
		\label{fig:minsky-explicit-nopeer-remote}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/minsky_path_gpu0-gpu2.pdf}
		\caption{}
		\label{fig:minsky-explicit-path-gpu0-gpu2}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/minsky_path_gpu2-gpu0.pdf}
		\caption{}
		\label{fig:minsky-explicit-path-gpu2-gpu0}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_nopeer_remote.pdf}
		\caption{}
		\label{fig:explicit-ac922-nopeer-remote}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_path_gpu0-gpu2.pdf}
		\caption{}
		\label{fig:ac922-explicit-path-gpu0-gpu2}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_path_gpu2-gpu0.pdf}
		\caption{}
		\label{fig:ac922-explicit-path-gpu2-gpu0}
	\end{subfigure}
	\caption[Peer Access and GPU-GPU Transfer Bandwidth]{
		Transfer bandwidth vs transfer size when peer access is disabled.
		(a) shows anisotropy on transfers between GPU0 and GPU2 on S822LC.
		(b) and (c) show transfer rates along the GPU0-CPU0 and CPU1-GPU2 components of the links.
		Although (b) and (c) are the same, the anisotropy still exists in the aggregated logical link.
		(d-c) show the same for AC922.
	}
	\label{fig:explicit-peer-anisotropy}
\end{figure}

\begin{table}[ht]
	\centering
	\caption[GPU-GPU Transfer Anisotropy]{GPU-GPU Transfer Anisotropy}
	\label{tab:explicit-peer-direction}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}                           & \textbf{S822LC}                                           & \textbf{AC922}                                           & \textbf{DGX-1} \\ \hline 
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer enabled)  } & $\times$                                                  & $\times$                                                 & $\times$       \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer enabled)  } & N/A                                                       & $\times$                                                 & N/A            \\ \hline
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer disabled) } & $\times$                                                  & $\times$                                                 & $\times$       \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:minsky-explicit-nopeer-remote}) & \checkmark (Fig.~\ref{fig:explicit-ac922-nopeer-remote}) & $\times$       \\ \hline
	\end{tabular}
\end{table}

\subsection{Transfer Rate on Identical Transfers}
\label{sec:explicit-peer-identical}

Like CPU-GPU transfers, different performance is observed on identical GPU-GPU transfers when peer access is disabled.
Figure~\ref{fig:explicit-nopeer-identical} show some example scenarios.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_identical-local.pdf}
		\caption{}
		\label{fig:explicit-nopeer-identical-s822lc-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_identical-remote.pdf}
		\caption{}
		\label{fig:explicit-nopeer-identical-s822lc-remote}
	\end{subfigure}
    \\
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_nopeer_identical-local.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-ac922-local}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_nopeer_identical-remote.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-ac922-remote}
    \end{subfigure}
	\\
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_nopeer_identical.pdf}
		\caption{}
		\label{fig:explicit-nopeer-identical-dgx}
	\end{subfigure}
	\caption[GPU-GPU \texttt{cudaMemcpy} Bandwidth on Identical Links]{
		Transfer bandwidth vs transfer size for AC922 and S822LC.
		(a) and (b) show differing transfer bandwidth on logically-identical local and remote transfers for S822LC.
		(c) and (d) show the same for AC922.
		(e) shows the same for DGX-1.
	}
	\label{fig:explicit-nopeer-identical}
\end{figure}

Figure~\ref{fig:explicit-nopeer-identical-ac922-local} compares the bandwidth of transfers from GPU0 to GPU1 and GPU2 to GPU3 on AC922.
These transfers are logically and physically identical: they have the same direction, between identical sub-topologies, and use the same CUDA APIs.
Surprisingly, the achieved bandwidth varies by nearly a factor of two.
This may be due to intermediate buffers being allocated on a CPU remote to the GPU pair, forcing transfers to cross the X-bus.
Figure~\ref{fig:explicit-nopeer-identical-ac922-remote} shows similar behavior for remote transfers, though with a smaller magnitude of effect.
Figures~\ref{fig:explicit-nopeer-identical-s822lc-local} and ~\ref{fig:explicit-nopeer-identical-s822lc-remote} show the same for S822LC.
Figure~\ref{fig:explicit-nopeer-identical-dgx} shows how GPU0-GPU1, GPU2-GPU3, GPU4-GPU5, and GPU6-GPU7 transfers are slower than all other local transfers (GPU0-GPU2, for example).
Table~\ref{tab:explicit-identical} summarizes the cases where differing performance is observed.

\begin{table}[ht]
	\centering
	\caption[Transfer rate on Identical Links]{Transfer Rate on Identical Links}
	\label{tab:explicit}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}                      \\ \hline 
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer enabled)  } & $\times$                                                            & $\times$                                                           & $\times$                                                  \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer enabled)  } & N/A                                                                 & $\times$                                                           & N/A                                                       \\ \hline
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-s822lc-local})  & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-ac922-local})  & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-dgx}) \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-s822lc-remote}) & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-ac922-remote}) & $\times$                                                  \\ \hline
	\end{tabular}
\end{table}

\section{Summary}

The performance of \texttt{cudaMemcpy} transfers is highly dependent on device affinity, CPU allocation type, transfer direction, and underlying hardware performance.
Transfers involving pageable host allocations are particularly unpredictable, with bandwidth peaking at intermediate transfer sizes, possibly due to pageable-to-pinned transfers occurring wholly in the CPU cache.
In general, constraining communication to pinned buffers and local devices offers the best performance, though direction of the transfer still has a large impact.
