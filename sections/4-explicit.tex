\chapter{Explicit Memory Performance}
\label{ch:explicit}

This chapter examines the performance of explicit data transfers over logical communication links presented in a system with NUMA and CUDA interfaces.
In particular, it focuses on CPU/CPU transfers, CPU/GPU transfers, and GPU/GPU transfers.
It highlights cases where the observed logical communication performance deviates significantly from the symmetries present in the CUDA API, numactl API, and hardware.
Those deviations take the form of different performance on identical links, anisotropic link performance, or performance affected by device affinity.

The microbenchmarks used in this section are available in the \texttt{microbench} project~\cite{pearson2018microbench}.
That project also includes benchmarks of other aspects of CUDA performance, including CUDA primitives like kernel launches, and CUDA libraries such as cuBLAS and cuDNN.

\section{CPU / CPU Transfers}
\label{sec:explicit-cpu-cpu}

This section begins by examining CPU-CPU transfer performance through \texttt{cudaMemcpy}.
This attempts to provide insight into the CUDA performance when sending data from one CPU socket to another.
Such a transfer would occur when data is sent from a CPU A to a GPU attached to another CPU B.
The data would traverse the SMP bus between CPU A and CPU B, and the bandwidth of that bus could limit the overall performance of the transfer.
Algorithm~\ref{alg:explicit-cpu-cpu} describes the measurement approach.
First, an allocation is created on the source CPU and destination CPU.
Then, \texttt{cudaMemcpy} is invoked to transfer data between those allocations.
CUDA events are used to 

\begin{algorithm}
	\caption[Measure \texttt{cudaMemcpy} CPU-CPU Bandwidth]{
		Algorithm to measure \texttt{cudaMemcpy} CPU-CPU Bandwidth.
		AllocPinned and AllocPageable are defined in Algorithm~\ref{alg:host-allocators}.
	}
	\label{alg:explicit-cpu-cpu}
	\begin{algorithmic}[1]
        \Statex
		\Function{bandwidth}{$dst$, $src$, $transfer\_size$}
		\State \texttt{numa\_bind($src$)}
        \State $srcPtr \gets$ \texttt{AllocPageable($transfer\_size$)}
		\State \texttt{memset($srcPtr$, 0, $transfer\_size$)}
		\State \texttt{numa\_bind($dst$)}
        \State $dstPtr \gets$ \texttt{AllocPinned($transfer\_size$)}
		\State \texttt{memset($dstPtr$, 0, $transfer\_size$)}

		\Statex
		\For{state}
		\State $start \gets$ cudaEventRecord()
		\State \begin{varwidth}[t]{\linewidth}
			cudaMemcpy(\par
			\hskip\algorithmicindent $dstPtr$, $srcPtr$, $transfer\_size$, \par 
			\hskip\algorithmicindent cudaMemcpyHostToHost)
		\end{varwidth}
		\State $stop \gets$ cudaEventRecord()
		\State cudaEventSynchronize($stop$)
		\State $millis \gets$ cudaEventElapsedTime($start$, $stop$)
		\State state.SetIterationTime($\frac{millis}{1000}$)
        \EndFor
		\EndFunction

		
	\end{algorithmic}
\end{algorithm}


Figure~\ref{fig:explicit-cpu0-cpu0} shows intra-CPU0 memcpy performance on S822LC, AC922, and DGX-1.
``1T RD'' and ``8T RD'' refer to single-thread and eight-threaded invocations of Algorithm~\ref{alg:explicit-cpu-cpu}.
``cudaMemcpy'' refers to a \texttt{cudaMemcpy} between a pageable allocation and pinned allocation on CPU0.
``cudaMemcpy'' typically achieves higher bandwidth than the single-thread implementation, probably thanks to an optimized copy implementation.
For large sizes, the ``8T'' invocation's multiple threads are better able to saturate the CPU memory and cache controllers.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_cpu-cpu.pdf}
		\caption{}
		\label{fig:s822lc-cpu0-cpu1-dst}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_cpu-cpu.pdf}
		\caption{}
		\label{fig:s822lc-cpu0-cpu1-dst}
	\end{subfigure}
	~
	% \begin{subfigure}[b]{0.31\textwidth}
	% 	\includegraphics[width=\textwidth]{figures/generated/s822lc_cpu_cpu.pdf}
	% 	\caption{}
	% 	\label{fig:s822lc-cpu0-cpu1-dst}
	% \end{subfigure}
	% ~
	\caption[CPU to CPU transfer bandwidth]{
		CPU to CPU transfer bandwidth vs. transfer size.
		Each transfer is measured using Algorithm~\ref{alg:explicit-cpu-cpu}.
		Whiskers at each point show the standard deviation measured over 5 repetitions.
	}
	\label{fig:cpu0-cpu1}
\end{figure}

Figure~\ref{fig:cpu0-cpu1} shows the inter-CPU performance achieved by the \texttt{rd\_8} and \texttt{wr\_8} workloads.
The measurement is taken multiple times with varying $num\_workers$, for each $transfer\_size$, and the maximum observed bandwidth at each transfer size is reported.
The highest performing number of workers is highlighted at each transfer size.
For smaller transfers, fewer workers may result in a faster transfer time as the overhead of entering the parallel region is not amortized effectively.

For both workloads, the achievable bandwidth for low transfer sizes is limited by the overhead introduced by the workload function calls and OpenMP runtime.
For large transfer sizes, the performance flattens out as the elapsed time is dominated by the actual data transfer over the SMP bus, achieving $30$ GB/s in the read workload and $25$ GB/s in the write workload on S822LC.
This is consistent with the theoretical 38.4 GB/s performance of the X bus on S822LC.


\section{CPU / GPU Transfers}
\label{sec:explicit-cpu-gpu}

Explicit CPU-GPU transfers are caused by the \texttt{cudaMemcpy} family of functions being invoked on one pointer to a host allocation and one pointer to a device allocation.
The host allocation may be pageable (created by \texttt{malloc} or \texttt{new}), or be created by \texttt{cudaMallocHost} or \texttt{cudaHostAlloc}.
The device allocation is created by \texttt{cudaMalloc}.
This section compares bandwidth achievable from pinned, pageable, and write-combining host allocations, with particular emphasis on how device affinity affects transfer-performance and cases where transfers are anisotropic.

Algorithm~\ref{alg:host-allocators} shows the different kind of host allocation strategies used in microbenchmarks that need CUDA host allocations on particular NUMA nodes.
\texttt{AllocPageable} simply defers to \texttt{malloc}, which will return memory allocation on a previously-pinned NUMA node.
\texttt{AllocPinned} defers to \texttt{malloc} to get a NUMA allocations, and then uses \texttt{cudaHostRegister} to pin that memory.
\texttt{AllocWriteCombined} uses the \texttt{cudaHostAlloc} CUDA library call with the \texttt{cudaHostAllocWriteCombined} flag to request that CUDA allocate write-combining memory.

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Statex
		\Function{AllocPageable}{$bytes$}
		\State $ptr \gets 0$
		\State malloc($ptr$, $bytes$)
		\Return $ptr$
		\EndFunction
		\Statex
		\Function{AllocPinned}{$bytes$}
		\State $ptr \gets 0$
		\State malloc($ptr$, $bytes$)
		\State cudaHostRegister($ptr$, $bytes$, cudaHostRegisterPortable)
		\Return $ptr$
		\EndFunction
		\Statex
		\Function{AllocWriteCombined}{$bytes$}
		\State $ptr \gets 0$
		\State cudaHostAlloc($ptr$, $bytes$, cudaHostAllocWriteCombined)
		\Return $ptr$
		\EndFunction
		
	\end{algorithmic}
	\caption[Pageable, Pinned, and Write-Combining Host Allocators.]{
        Pageable, Pinned, and Write-Combining Host Allocators.
	}
	\label{alg:host-allocators}
\end{algorithm}

Algorithm~\ref{alg:explicit-cpu-gpu} is used to evaluate the achievable bandwidth for \texttt{cudaMemcpy} transfers between a GPU allocation and a pageable, pinned, or write-combining host allocation. 
The same algorithm can be used for these cases, because the same \texttt{cudaMemcpy} CUDA API call to transfer data can be used on a pointer pointing to any of the allocation types.
Depending on the source and destination types $src$ and $dst$, and the desired host allocation type, the corresponding CUDA or numactl APIs are called to bind later activities to the desired GPU or CPU.
Then, the CUDA or host allocators are invoked to produce $devPtr$ (a pointer to the device allocation) and $hostPtr$ (a pointer to the CPU allocation).
The main benchmark loop uses the \texttt{cudaMemcpy} time as the iteration time that should be reported.

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $bytes$, $num\_iters$}
		\If{$src$ is GPU}
		\State \texttt{cudaSetDevice($src$)}
		\Else \Comment{$src$ is CPU}
		\State \texttt{numa\_bind($src$)}
		\EndIf
		\If{$dst$ is GPU}
		\State \texttt{cudaSetDevice($dst$)}
		\Else \Comment{$dst$ is CPU}
		\State \texttt{numa\_bind($dst$)}
		\EndIf
		
		\State $devPtr \gets$ \texttt{cudaMalloc($bytes$)} \Comment{device allocation}
		\State $hostPtr \gets$ hostAllocate($bytes$) \Comment{appropriate host allocator}

		\If{$src$ is GPU}
		\State $srcPtr \gets devPtr$
		\State $dstPtr \gets hostPtr$
		\Else \Comment{$src$ is CPU}
		\State $srcPtr \gets hostPtr$
		\State $dstPtr \gets devPtr$
		\EndIf

		\State $start \gets$ \texttt{cudaEventCreate()}
		\State $end \gets$ \texttt{cudaEventCreate()}

		\For{state}
		\State \texttt{cudaEventRecord($start$)}
		\State \texttt{cudaMemcpy($dstPtr$, $srcPtr$, $bytes$, cudaMemcpyDefault)}
		\State \texttt{cudaEventRecord($stop$)}
		\State $millis \gets$ cudaEventElapsedTime($start$, $stop$)
		\State state.SetIterationTime($millis$ / $1000$)
        \EndFor
		
		\Return $elapsed$
		\EndFunction
		
	\end{algorithmic}
	\caption[Measuring CPU/GPU bandwidth with \texttt{cudaMemcpy}.]{
		Measuring CPU/GPU bandwidth with \texttt{cudaMemcpy}.
		Host allocators are described in Algorithm~\ref{alg:host-allocators}.
	}
	\label{alg:explicit-cpu-gpu}
\end{algorithm}

\subsection{Comparison of Pageable, Pinned, and Write-Combining Host Allocations}
\label{sec:explicit-pageable-pinned-wc}

To contextualize other results presented in this chapter, Figure~\ref{fig:pageable-pinned-wc} shows the transfer performance from pageable, pinned, and write-combined allocations on CPU0 to device allocations on GPU0.
These performance curves exhibit features common throughout this chapter:
\begin{itemize}
	\item For small transfer sizes, the time is dominated by overhead introduced by the abstraction layer.
	\item For large transfer sizes, the time is dominated by bandwidth limits on the exercised physical link.
	\item The performance may vary smoothly across intermediate transfer sizes, or exhibit more complicated behavior.
	For example, in the AC922 transfer show in Figure~\ref{fig:pageable-cpu0-gpu0}, there is a peak in bandwidth for intermediate transfer sizes before the performance drops for larger transfers.
	Other curves do not show this behavior.
\end{itemize}

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pageable_cpu0-to-gpu0.pdf}
		\caption{}
		\label{fig:pageable-cpu0-gpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pinned_cpu0-to-gpu0.pdf}
		\caption{}
		\label{fig:pinned-cpu0-gpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/wc_cpu0-to-gpu0.pdf}
		\caption{}
		\label{fig:wc-cpu0-gpu0}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pageable_gpu0-to-cpu0.pdf}
		\caption{}
		\label{fig:pageable-gpu0-cpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/pinned_gpu0-to-cpu0.pdf}
		\caption{}
		\label{fig:pinned-gpu0-cpu0}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/wc_gpu0-to-cpu0.pdf}
		\caption{}
		\label{fig:wc-gpu0-cpu0}
	\end{subfigure}
	\caption[\texttt{cudaMemcpy} bandwidth for CPU0-GPU0 transfers.]{
		\texttt{cudaMemcpy} bandwidth vs transfer size for CPU0 to GPU0 transfers from 
		(a) pageable allocations,
		(b) pinned allocations, and
		(c) write-combining allocations.
		Results for S822LC, AC922, and DGX-1 systems are shown.
	}
	\label{fig:pageable-pinned-wc}
\end{figure}


The curves for transfers from a pinned allocation on CPU0 to GPU0 shown in Figure~\ref{fig:pinned-cpu0-gpu0} share a similar shape: the transfer bandwidth is low for small sizes, and eventually saturates once transfers become large enough.
For small transfer sizes, the elapsed time is dominated by overhead from invoking the transfer (and the device synchronization for the pageable transfer).
This causes the upward slope as transfer size increases.
For larger transfers, the time is dominated by the actual cost of moving the data, and the performance saturates.
Both S822LC and AC922 achieve \mytilde 75\% of the theoretical performance allowed by the relevant interconnects, and DGX-1 achieves around 50\%.

In contrast, Figure~\ref{fig:pageable-cpu0-gpu0} shows transfers from pageable host allocations to GPU0.
The achievable bandwidth for large transfer sizes on S822LC is reduced to approximately $50\%$ of the theoretical $40$ GB/s bandwidth provided by the link.
On AC922 the performance is actually even lower, at only $15$ GB/s or $20\%$ of the theoretical bandwidth.
AC922 also shows a transfer bandwidth peak at intermediate sizes that is not present on the other machines.
Similar peaks show up during other pageable transfers later in this section, 
For large transfers, DGX-1 drops by $20\%$ to $8$ GB/s out of a theoretical maximum $20$ GB/s for one-lane NVLink 1.0.
A comparison of Figure~\ref{fig:pinned-cpu0-gpu0}~and~\ref{fig:wc-cpu0-gpu0} shows that write-combining pinned allocations do not have a measurable performance impact on the testest systems.

Section~\ref{sec:pinned-memory} describes how \texttt{cudaMemcpy} from a pageable allocation to the GPU actually causes two data copies: one from the pageable allocation application to a pinned buffer, and a second copy, a DMA from the pinned buffer to the GPU.
When pinned memory transfers are faster than pageable memory, we can infer that the CPU memory copy from pageable allocation to pinned buffer is limiting the performance.
For comparison, consider Figure~\ref{fig:explicit-cpu-cpu}, which shows the performance of using \texttt{cudaMemcpy} to only do a copy from a pageable allocation to a pinned allocation.
For S822LC, the pageable-to-GPU transfer show in Figure~\ref{fig:pageable-cpu0-gpu0} is approximately the same performance as the pageable-to-pinned transfer show in Figure~\ref{fig:explicit-cpu-cpu}.
Surprisingly, on AC922, the pageable-to-GPU transfer for large transfer sizes is substantially faster than the pageable-to-pinned transfer that is should be limited by.
This suggests that there is a different implementation for the two cases.
Bandwidth spikes at intermediate sizes suggest that the GPU DMA may directly access data from the CPU cache when the transfer can fit in the cache.
This is further reinforced by the lack of difference between Pinned and write-combining transfer bandwidth, which suggests that caching or lack thereof on these systems does not influence the DMA engine.

\subsection{Measurements}

Figure~\ref{fig:cpu-gpu-affinity-direction} shows CPU/GPU bandwidth on a variety of logical paths for S822LC, AC922, and DGX-1.
Transfers involving pinned and pageable allocations are shown.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_pageable_affinity.pdf}
		\caption{}
		\label{fig:minsky_pageable_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_pageable_affinity.pdf}
		\caption{}
		\label{fig:hal_pageable_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_pageable_affinity.pdf}
		\caption{}
		\label{fig:dgx_pageable_affinity}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_pinned_affinity_cpu0.pdf}
		\caption{}
		\label{fig:minsky_pinned_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_pinned_affinity.pdf}
		\caption{}
		\label{fig:hal_pinned_affinity}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_pinned_affinity.pdf}
		\caption{}
		\label{fig:dgx_pinned_affinity}
		
	\end{subfigure}
	\caption[CPU-GPU affinity and \texttt{cudaMemcpy} bandwidth]{
		Transfer bandwidth vs transfer size for local and remote transfers from pageable and pinned host allocations on S822LC, AC922, and DGX-1.
		(a-c) show transfers from pageable allocations to GPUs.
		(d-f) show transfers from pinned allocations to GPUs.
		(a) and (d) are for S822LC, (b) and (e) for AC922, and (c) and (f) for DGX-1.
	}
	\label{fig:cpu-gpu-affinity-direction}
\end{figure}

In general, the bandwidth follows the same outline described in Section~\ref{sec:explicit-pageable-pinned-wc}, with overhead-dominated time for small transfers, bandwidth-dominated time for large transfers, and some other behavior between.
There are some distinctive reoccurring patterns in Figures~\ref{fig:cpu-gpu-affinity-direction} (a)-(c).

CPU-to-GPU pageable transfers on all systems exhibit peaks in transfer bandwidth at intermediate transfer sizes.
The shape of this curve suggests some insight into the copy implementation.
For example, consider the S822LC CPU0 to GPU0 curve.
This is a local transfer, so the expected process is that a pageable allocation on CPU0 is copied to one or more pinned allocations on CPU0, which are then accessed by GPU0's DMA engine.
The fact that the peak bandwidth at intermediate transfer sizes surpasses the measured bandwidth for single-threaded inter-CPU transfers (Figure~\ref{fig:cpu0-cpu1}) suggests that the pageable-to-pinned copy is indeed occurring within a single CPU, and not across CPUs.
When the transfer occurs within the same CPU, the bandwidth will be higher when that transfer can fit within the CPU cache.
The same shape is present in the CPU0 to GPU2 curve, where it would be plausible for the pageable allocation to be on CPU0 and the pinned allocation on CPU1.
\todo{compare to CPU-CPU intra-node bandwidth when available}.
For S822LC and DGX, as the transfer grows larger, the bandwidth reaches a steady-state value that is the same as the single-threaded inter-CPU memory access bandwidth.
In AC922, we see a large change in the transfer bandwidth, suggesting that some other implementation is chosen at those sizes.
The drop before reaching that bandwidth suggests some system performance bug, like imbalance in the number and size of the pinned transfer buffers that prevents good overlapping of the host-device DMA and the host-host memory copy.

Figures~\ref{fig:cpu-gpu-affinity-direction} (d)-(f) show the same transfers, but from pinned allocations.
These transfers are all ultimately limited by the interconnect bandwidth, and do not show the same peaks.
These transfers typically achieve \mytilde70\% or more of the theoretical link bandwidth.

\subsection{Affinity}
\label{sec:explicit-cpu-gpu-affinity}

On systems with high-performance interconnects, transfers from GPU allocations to pageable CPU allocations are strongly correlated with device affinity.
Transfers involving pinned CPU allocations demonstrate a strong effect in both CPU-to-GPU and GPU-to-CPU directions.
The presence of high-performance interconnects further exposes any performance differences, since bandwidth is less likely to be limited by underlying link bandwidth and more likely to be limited by performance bugs or single-threaded memory copies.
Table~\ref{tab:cpu-gpu-affinity} summarizes the effects.

Figure~\ref{fig:minsky_pageable_affinity} shows that affinity has a small effect on bandwidth for pageable transfers on S822LC for transfers in both directions.
For transfers larger than 4 MB, the GPU to CPU remote transfer is faster than the GPU to CPU local transfer.
Transfer bandwidths fall well below the NVLink and X bus transfer capacities (40 GB/s and 38.4 GB/s respectively), so this likely reflects a driver or firmware performance bug.
Figure~\ref{fig:hal_pageable_affinity} local GPU to CPU pageable transfers on AC922 are much faster than their remote counterparts, except for large transfers, where remote performance is higher.
Figure~\ref{fig:dgx_pageable_affinity} shows that GPU-to-CPU pageable transfers on DGX are affected by affinity.
Figure~\ref{fig:minsky_pinned_affinity} shows on S822LC, pinned bandwidth is affected strongly by affinity, particularly in the GPU to CPU direction.
Figure~\ref{fig:hal_pinned_affinity} shows for pinned transfers on AC922, affinity has a much stronger effect.
Figure~\ref{fig:dgx_pinned_affinity} shows no effect from affinity on pinned transfers for DGX-1.

\begin{table}[ht]
	\centering
	\caption[Affinity and Logical Communication Bandwidth]{Effect of Device Affinity on Logical Transfer Bandwidth}
	\label{tab:cpu-gpu-affinity}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}     & \textbf{S822LC}                                      & \textbf{AC922}                                    & \textbf{DGX-1}                                  \\ \hline 
		Pageable $\rightarrow$ GPU & $\times$   (Fig.~\ref{fig:minsky_pageable_affinity}) & $\times$   (Fig.~\ref{fig:hal_pageable_affinity}) & $\times$ (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pageable $\leftarrow$ GPU  & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pinned $\rightarrow$ GPU   & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & $\times$  (Fig.~\ref{fig:dgx_pinned_affinity})  \\ \hline
		Pinned $\leftarrow$ GPU    & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & $\times$ (Fig.~\ref{fig:dgx_pinned_affinity})   \\ \hline
	\end{tabular}
\end{table}

\subsection{Anisotropy}
\label{sec:explicit-cpu-gpu-anisotropy}
\textit{Anisotropy} refers to the property of being directionally-dependent, e.g., CPU/GPU link bandwidth is anisotropic if CPU-to-GPU bandwidth is different than GPU-to-CPU bandwidth. 
Figure~\ref{fig:cpu-gpu-affinity-direction} highlights that link bandwidth exhibits significant anisotropy in systems with high-performance interconnects.
Table~\ref{tab:explicit-anisotropy} summarizes the effects.
Particularly for the pageable transfers shown in Figures~\ref{fig:minsky_pageable_affinity} and \ref{fig:hal_pageable_affinity}, corresponding transfers are shown to be highly anisotropic.
In the pinned transfers on S822LC (Fig.~\ref{fig:minsky_pinned_affinity}), all transfers show some degree of anisotropy, with a larger effect over remote transfers.
For transfers where there is a difference, CPU $\rightarrow$ GPU transfers tend to be faster.
\todo{Can this be explained by cache effects? Check write-combined bandwidth}.
For AC922, (Fig.~\ref{fig:hal_pinned_affinity}), local transfers show anisotropy only for intermediate transfer sizes, and remote transfers do generally.
For DGX-1, pinned transfers all show 2 GB/s of anisotropy (Fig.~\ref{fig:dgx_pinned_affinity}), while the degree of anisotropy for transfers involving pageable allocations depends on the affinity (Fig.~\ref{fig:dgx_pageable_affinity}).

\begin{table}[H]
	\centering
	\caption[Host-Device Transfer Anisotropy]{Host-Device Transfer Anisotropy}
	\label{tab:explicit-anisotropy}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}                  & \textbf{S822LC}                                      & \textbf{AC922}                                    & \textbf{DGX-1}                                    \\ \hline 
		Pageable $\leftrightarrow$ GPU (local)  & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pageable $\leftrightarrow$ GPU (remote) & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
		Pinned $\leftrightarrow$ GPU (local)    & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & \checkmark (Fig.~\ref{fig:dgx_pinned_affinity}) \\ \hline
		Pinned $\leftrightarrow$ GPU (remote)   & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & \checkmark (Fig.~\ref{fig:dgx_pinned_affinity})   \\ \hline
	\end{tabular}
\end{table}

\subsection{Differences between Identical Transfers}
\label{sec:explicit-cpu-gpu-identical}

Figure~\ref{fig:minsky_pageable_cpu1-gpu01} shows transfer performance on two different identical links: CPU1-GPU0 and CPU1-GPU1.
Both of the scenarios presented involve identical hardware and logical links, yet there is a 10\% bandwidth different at intermediate sizes.
Table~\ref{tab:explicit-identical} summarizes scenarios where the transfer performance differs on identical links.
S822LC is the only system where this behavior is observed for CPU-GPU transfers.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/generated/minsky_pageable_cpu1-gpu01.pdf}
	\caption[Different Performance on Identical S822LC Links]{
        S822LC transfer bandwidth vs. transfer size for a pageable allocation on CPU1 to an allocation on GPU0 or GPU1.
	}
	\label{fig:minsky_pageable_cpu1-gpu01}
\end{figure}

\begin{table}[ht]
	\centering
	\caption[Transfer Rate Variability on Identical Links]{Transfer Rate Variability on Identical Links}
	\label{tab:explicit-identical}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}     & \textbf{S822LC}                                        & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
		Pageable $\rightarrow$ GPU & \checkmark (Fig.~\ref{fig:minsky_pageable_cpu1-gpu01}) & $\times$       & $\times$       \\ \hline
		Pageable $\leftarrow$ GPU  & $\times$                                               & $\times$       & $\times$       \\ \hline
		Pinned $\rightarrow$ GPU   & $\times$                                               & $\times$       & $\times$       \\ \hline
		Pinned $\leftarrow$ GPU    & $\times$                                               & $\times$       & $\times$       \\ \hline
	\end{tabular}
\end{table}

\section{GPU / GPU Transfers}
\label{sec:explicit-gpu-gpu}

Explicit GPU-GPU transfers are caused by the \texttt{cudaMemcpy} family of functions being invoked on pointers to device allocations created with \texttt{cudaMalloc}.
Unlike the different types of host allocations CPU-GPU transfers described in Section~\ref{sec:explicit-cpu-gpu}, this section only refers to a single kind of device allocation.
Device allocations come with the concept of peer access, discussed in Section~\ref{sec:cuda-peer}.
This section focuses on the effect of peer access on transfer bandwidth, and cases where transfers are anisotropic or have different performance on identical links.

Algorithm~\ref{alg:explicit-gpu-gpu} is used to evaluate the achievable GPU-GPU transfer bandwidth.
First, peer access is enabled or disabled depending on the experimental configuration.
Then, \texttt{cudaMalloc} is used to create allocations of $transfer\_size$ bytes pointed to by $srcPtr$ and $dstPtr$.
Finally, the achievable bandwidth is measured using the wall time for $num\_iters$ iterations, and the minimum elapsed time is reported, to help remove jitter from the results.

\begin{algorithm}[H]
	\caption[Measuring GPU-GPU \texttt{cudaMemcpy} Bandwidth]{Measuring GPU-GPU \texttt{cudaMemcpy} Bandwidth}
	\label{alg:explicit-gpu-gpu}
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$, $peer\_access$}
		\If{$peer\_access$}
		\State \texttt{cudaSetDevice($src$)}
		\State \texttt{cudaDeviceEnablePeerAccess($dst$)}
		\State \texttt{cudaSetDevice($dst$)}
		\State \texttt{cudaDeviceEnablePeerAccess($src$)}
		\Else
		\State \texttt{cudaSetDevice($src$)}
		\State \texttt{cudaDeviceDisablePeerAccess($dst$)}
		\State \texttt{cudaSetDevice($dst$)}
		\State \texttt{cudaDeviceDisablePeerAccess($src$)}        
		\EndIf
		
		\State \texttt{cudaSetDevice($src$)} \Comment{Source allocation}
		\State $srcPtr \gets$ \texttt{cudaMalloc($transfer\_size$)}
		
		\State \texttt{cudaSetDevice($dst$)} \Comment{Destination allocation}
		\State $dstPtr \gets$ \texttt{cudaMalloc($transfer\_size$)}
		
		\State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
		\For{$i \gets 1 \textrm{ to } num\_iters$}
		\State $start \gets$ walltime()
		\State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
		\State $end \gets$ walltime()
		\State $elapsed \gets$ min($elapsed$, $end-start$)
		\EndFor
		
		\Return $elapsed$
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}

\subsection{Transfer Rate and Peer Access}
\label{sec:explicit-peer-bandwidth}

Figure~\ref{fig:explicit-peer} shows the performance of a variety of GPU-GPU transfers with and without peer access.
Generally, peer access has a large effect on the bandwidth of local GPU-GPU transfers.
With peer access enabled, GPUs may do DMAs directly with their peer memory instead of copying through the host.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_memcpy_local.pdf}
		\caption{}
		\label{fig:explicit-s822lc-peer}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_memcpy_local.pdf}
		\caption{}
		\label{fig:explicit-hal-peer-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_memcpy_remote.pdf}
		\caption{}
		\label{fig:explicit-hal-peer-remote}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_peer_nopeer_local.pdf}
		\caption{}
		\label{fig:explicit-dgx-peer-nopeer-local}
	\end{subfigure}
	
	\caption[GPU-GPU \texttt{cudaMemcpy} bandwidth and peer access]{
		GPU-GPU \texttt{cudaMemcpy} transfer bandwidth vs. transfer size for various scenarios with peer access enabled or disabled.
		(a) shows a GPU0 to GPU1 transfer on S822LC with and without peer access,
		(b) shows GPU to GPU local transfers on AC922.
		(c) shows GPU to GPU remote transfers on AC922.
		(d) shows GPU to GPU local transfers on DGX-1.
		S822LC and DGX-1 do not support peer access for remote GPU-GPU transfers.
	}
	\label{fig:explicit-peer}
\end{figure}

Figure~\ref{fig:explicit-s822lc-peer} shows that enabling peer access on S822LC improves the performance of local GPU-GPU transfers by \mytilde40\%, to over 75\% of the theoretical link bandwidth.
Likewise, Figure~\ref{fig:explicit-dgx-peer-nopeer-local} shows peer access roughly doubling performance on DGX-1 and Figure~\ref{fig:explicit-hal-peer-local} shows similar behavior for AC922, with the much higher performance ceiling due to the increased bandwidth of NVLink 2.0.
On AC922, GPU2 and GPU3 show lower performance than GPU0 and GPU1 when peer access is disabled.
This may be because the CUDA staging buffers needed during this transfer are allocated on a NUMA node remote to the GPUs, so the transfer must copy over the X bus unnecessarily.
For remote transfers on AC922, shown in Figure~\ref{fig:explicit-hal-peer-remote}, enabling peer access actually reduces the performance, though the effect is not very large.
More interestingly, the remote GPU/GPU performance with peer disabled is well below both the pinned GPU-to-CPU and CPU-to-GPU remote bandwidth shown in Figure~\ref{fig:hal_pageable_affinity}.
Together, those operations should be able to implement the GPU-to-GPU transfer.

Table~\ref{tab:explicit-peer-rate} summarizes the scenarios where peer access affects transfer bandwidth.

\begin{table}[H]
	\centering
	\caption[Effect of Peer Access on Transfer Rate]{Effect of Peer Access on Transfer Rate}
	\label{tab:explicit-peer-rate}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}       & \textbf{S822LC}                                   & \textbf{AC922}                                       & \textbf{DGX-1}                                             \\ \hline 
		GPU $\rightarrow$ Local GPU  & \checkmark  (Fig.~\ref{fig:explicit-s822lc-peer}) & \checkmark (Fig.~\ref{fig:explicit-hal-peer-local})  & \checkmark (Fig.~\ref{fig:explicit-dgx-peer-nopeer-local}) \\ \hline
		GPU $\rightarrow$ Remote GPU & N/A                                               & \checkmark (Fig.~\ref{fig:explicit-hal-peer-remote}) & N/A                                                        \\ \hline
	\end{tabular}
\end{table}


\subsection{Transfer Anisotropy with Peer Access Disabled}
\label{sec:explicit-peer-direction}

Disabling peer access causes anisotropy to be observed in GPU-to-GPU transfers on S822LC and AC922.
Figure~\ref{fig:explicit-peer-anisotropy} highlights anisotropic remote GPU-GPU transfers on S822LC and AC922.
No anisotropy is observed on DGX-1.
This anisotropy is not consistent with anisotropy observed on the intervening links.
For example, consider Figure~\ref{fig:explicit-peer-anisotropy}a-c.
Figure~\ref{fig:minsky-explicit-nopeer-remote} shows anisotropy along the remote GPU0-GPU2 transfer.
Figures~\ref{fig:minsky-explicit-path-gpu0-gpu2}~and~\ref{fig:minsky-explicit-path-gpu2-gpu0} show pinned transfer speeds along GPU0-CPU0-CPU1-GPU2 and GPU2-CPU1-CPU0-GPU0 paths.
The observed GPU0-GPU2 bandwidth is sometimes higher than the higher observed bandwidth on the path components, and sometimes lower than the lowest path component.


\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_memcpy_remote.pdf}
		\caption{}
		\label{fig:minsky-explicit-nopeer-remote}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_path_gpu0-gpu2.pdf}
		\caption{}
		\label{fig:minsky-explicit-path-gpu0-gpu2}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_path_gpu2-gpu0.pdf}
		\caption{}
		\label{fig:minsky-explicit-path-gpu2-gpu0}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_nopeer_remote.pdf}
		\caption{}
		\label{fig:explicit-ac922-nopeer-remote}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_path_gpu0-gpu2.pdf}
		\caption{}
		\label{fig:ac922-explicit-path-gpu0-gpu2}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/ac922_path_gpu2-gpu0.pdf}
		\caption{}
		\label{fig:ac922-explicit-path-gpu2-gpu0}
	\end{subfigure}
	\caption[Peer Access and GPU-GPU Transfer Bandwidth]{
		Transfer bandwidth vs transfer size when peer access is disabled.
		(a) shows anisotropy on transfers between GPU0 and GPU2 on S822LC.
		(b) and (c) show transfer rates along the GPU0-CPU0 and CPU1-GPU2 components of the links.
		Although (b) and (c) are the same, the anisotropy still exists in the aggregated logical link.
		(d-c) show the same for AC922.
	}
	\label{fig:explicit-peer-anisotropy}
\end{figure}

\begin{table}[ht]
	\centering
	\caption[GPU-GPU Transfer Anisotropy]{GPU-GPU Transfer Anisotropy}
	\label{tab:explicit-peer-direction}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}                           & \textbf{S822LC}                                           & \textbf{AC922}                                           & \textbf{DGX-1} \\ \hline 
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer enabled)  } & $\times$                                                  & $\times$                                                 & $\times$       \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer enabled)  } & N/A                                                       & $\times$                                                 & N/A            \\ \hline
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer disabled) } & $\times$                                                  & $\times$                                                 & $\times$       \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:minsky-explicit-nopeer-remote}) & \checkmark (Fig.~\ref{fig:explicit-ac922-nopeer-remote}) & $\times$       \\ \hline
	\end{tabular}
\end{table}

\subsection{Transfer Rate on Identical Transfers}
\label{sec:explicit-peer-identical}

Like CPU-GPU transfers, different performance is observed on identical GPU-GPU transfers when peer access is disabled.
Figure~\ref{fig:explicit-nopeer-identical} show some example scenarios.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_nopeer_identical-local.pdf}
		\caption{}
		\label{fig:explicit-nopeer-identical-s822lc-local}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/s822lc_nopeer_identical-remote.pdf}
		\caption{}
		\label{fig:explicit-nopeer-identical-s822lc-remote}
	\end{subfigure}
    \\
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/ac922_nopeer_identical-local.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-ac922-local}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/ac922_nopeer_identical-remote.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-ac922-remote}
    \end{subfigure}
	\\
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_nopeer_identical.pdf}
		\caption{}
		\label{fig:explicit-nopeer-identical-dgx}
	\end{subfigure}
	\caption[GPU-GPU \texttt{cudaMemcpy} Bandwidth on Identical Links]{
		Transfer bandwidth vs transfer size for AC922 and S822LC.
		(a) and (b) show differing transfer bandwidth on logically-identical local and remote transfers for S822LC.
		(c) and (d) show the same for AC922.
		(e) shows the same for DGX-1.
	}
	\label{fig:explicit-nopeer-identical}
\end{figure}

Figure~\ref{fig:explicit-nopeer-identical-ac922-local} compares the bandwidth of transfers from GPU0 to GPU1 and GPU2 to GPU3 on AC922.
These transfers are logically and physically identical: they have the same direction, between identical sub-topologies, and use the same CUDA APIs.
Surprisingly, the achieved bandwidth varies by nearly a factor of two.
This may be due to intermediate buffers being allocated on a CPU remote to the GPU pair, forcing transfers to cross the X-bus.
Figure~\ref{fig:explicit-nopeer-identical-ac922-remote} shows similar behavior for remote transfers, though with a smaller magnitude of effect.
Figures~\ref{fig:explicit-nopeer-identical-s822lc-local} and ~\ref{fig:explicit-nopeer-identical-s822lc-remote} show the same for S822LC.
Figure~\ref{fig:explicit-nopeer-identical-dgx} shows how GPU0-GPU1, GPU2-GPU3, GPU4-GPU5, and GPU6-GPU7 transfers are slower than all other local transfers (GPU0-GPU2, for example).
Table~\ref{tab:explicit-identical} summarizes the cases where differing performance is observed.

\begin{table}[ht]
	\centering
	\caption[Transfer rate on Identical Links]{Transfer Rate on Identical Links}
	\label{tab:explicit}
	\begin{tabular}{cccc}
		\hline
		\textbf{Transfer Kind}                      \\ \hline 
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer enabled)  } & $\times$                                                            & $\times$                                                           & $\times$                                                  \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer enabled)  } & N/A                                                                 & $\times$                                                           & N/A                                                       \\ \hline
		\makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-s822lc-local})  & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-ac922-local})  & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-dgx}) \\ \hline
		\makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-s822lc-remote}) & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-ac922-remote}) & $\times$                                                  \\ \hline
	\end{tabular}
\end{table}

\section{Summary}

The performance of \texttt{cudaMemcpy} transfers is highly dependent on device affinity, CPU allocation type, transfer direction, and underlying hardware performance.
Transfers involving pageable host allocations are particularly unpredictable, with bandwidth peaking at intermediate transfer sizes, possibly due to pageable-to-pinned transfers occurring wholly in the CPU cache.
In general, constraining communication to pinned buffers and local devices offers the best performance, though direction of the transfer still has a large impact.

\begin{itemize}
	\item S822LC GPU to CPU pageable transfers are faster for remote than local.
	\item AC922 GPU to CPU pageable transfers: local is faster than remove for small, remote is faster than local for large.
\end{itemize}
