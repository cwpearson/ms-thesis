\chapter{Explicit Memory Performance}
\label{ch:explicit}

This chapter examines the performance of explicit data transfers over logical communication links presented in a system with NUMA and CUDA interfaces.
\begin{itemize}
    \item Section~\ref{sec:explicit-cpu-cpu} describes CPU-to-CPU transfers.
    \item Section~\ref{sec:explicit-cpu-gpu} describes transfers between CPUs and GPUs.
    \item Section~\ref{sec:explicit-gpu-gpu} describes GPU-to-GPU transfers.
\end{itemize}
It highlights cases where the observed logical communication performance deviates significantly from the symmetries present in the CUDA API, numactl API, and hardware.
Those deviances take the form of different performance on identical links, anisotropic link performance, or performance affected by device affinity.
Full results can be found in Appendix~\ref{ch:data}.

\section{CPU / CPU Transfers}
\label{sec:explicit-cpu-cpu}

This section begins by examining CPU-CPU transfer performance by using multiple threads to read remote data in an attempt to saturate the SMP bus and memory controllers.
This attempts to simulate the maximum possible performance that the CUDA API could achieve while sending data between sockets for CPU-to-GPU transfers.
Algorithm~\ref{alg:explicit-cpu-cpu} describes the approach.

Two types of traffic are generated.
In the \texttt{read} operation (Listing~\ref{lst:explicit-cpu-cpu-cpuread}), source-to-destination bandwidth is measured by a set of worker threads on the destination CPU generating loads for an allocation on the source CPU.
In the \texttt{write} operation (Listing~\ref{lst:explicit-cpu-cpu-cpuwrite}), bandwidth is measured by worker threads on the source CPU writing to an allocation on the destination CPU.
First, an allocation is created on the source CPU
Then, the worker threads are bound to the desired CPU, and the \texttt{cpu\_read} or \texttt{cpu\_write} workload is executed.
The read and write operations are executed on data members 8 bytes in size, to reflect the 8-byte payload on the X bus~\cite{caldeira2016s822lc}.

\begin{lstlisting}[language=C++, caption=CPU-CPU Read Function, label=lst:explicit-cpu-cpu-cpuread]
void cpu_read(double *__restrict__ dummy,
                double *__restrict__ ptr,
                const size_t count,
                const size_t stride) {
    const size_t numElems = count / sizeof(double);
    const size_t elemsPerStride = stride / sizeof(double);
    double acc = 0;
#pragma omp parallel for schedule(static)
    for (size_t i = 0; i < numElems; i += elemsPerStride) {
        acc += ptr[i];
    }
    *dummy += acc;
}
\end{lstlisting}


\begin{lstlisting}[language=C++, caption=CPU-CPU Write Function, label=lst:explicit-cpu-cpu-cpuwrite]
void cpu_write(double *__restrict__ ptr, 
                 const size_t count, 
                 const size_t stride) {
    const size_t numElems = count / sizeof(double);
    const size_t elemsPerStride = stride / sizeof(double);
#pragma omp parallel for schedule(static)
    for (size_t i = 0; i < numElems; i += elemsPerStride) {
        ptr[i] = i * 31ul + 7ul;
    }
}
\end{lstlisting}

For both the \texttt{cpu\_read} and \texttt{cpu\_write} operations, \texttt{ptr} is the allocation that is being read or written,
\texttt{count} is the number of bytes in that allocation, and \texttt{stride} is the number of bytes between every read or write.
Since memory loads in isolation are side-effect free, \texttt{cpu\_read} is designed to avoid being optimized away by the compiler.
The array values are accumulated and the sum is written out through the \texttt{dummy} argument.
This takes a small amount of time that is incorrectly attributed to the elapsed bandwidth time.
For both operations, OpenMP is used to evenly distribute the read or write operations amongst the cores of the executing CPU.
Entering and exiting the parallel region incurrs some cost, which is counted in the bandwidth computation.

\begin{algorithm}
    \caption[test]{
        Algorithm to measure CPU-CPU Bandwidth.
        \texttt{cpu\_read} is defined in Listing~\ref{lst:explicit-cpu-cpu-cpuread} and \texttt{cpu\_write} is defined in Listing~\ref{lst:explicit-cpu-cpu-cpuwrite}.
        \texttt{numa\_bind} binds the executing thread to the provided NUMA node.
        \texttt{walltime} returns the current wall time.
    }
    \label{alg:explicit-cpu-cpu}
    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_workers$}
        \If{$op$ is RD}
            \State \texttt{numa\_bind($src$)}
        \Else
            \State \texttt{numa\_bind($dst$)}
        \EndIf
        \State $srcPtr \gets$ \texttt{malloc($transfer\_size$)}
        \State \texttt{memset($srcPtr$, 0, $transfer\_size$)} \Comment force pages to be allocated

        \If{$op$ is RD}
            \State \texttt{numa\_bind($dst$)}
        \Else
            \State \texttt{numa\_bind($src$)}
        \EndIf
        \State \texttt{omp\_set\_num\_threads($num\_workers$)}
        \Statex
        
        \For{all worker threads} \Comment bind workers to $src$ or $dst$
            \If{$op$ is RD}
                \State \texttt{numa\_bind($dst$)}
            \Else
                \State \texttt{numa\_bind($src$)}
            \EndIf
        \EndFor

        \State $start \gets$ walltime()
        \If{$op$ is RD}
            \State \texttt{cpu\_read($dst$)}
        \Else
            \State \texttt{cpu\_write($src$}
        \EndIf
        \State $end \gets$ walltime()
        \State $elapsed \gets$ min($elapsed$, $end-start$)

        \State \Return $elapsed$
    \EndFunction

    \end{algorithmic}
\end{algorithm}


Figure~\ref{fig:cpu0-cpu1} shows the performance achieved by the \texttt{cpu\_read} and \texttt{cpu\_write} workloads.
The measurement is taken multiple times with varying $num\_workers$, for each $transfer\_size$, and the maximum observed bandwidth
at each transfer size is reported.
The highest performing number of workers is highlighted at each transfer size.
For smaller transfers, fewer workers may result in a faster transfer time as the overhead of entering the parallel region is not amortized effectively.

For both workloads, the achievable bandwidth for low transfer sizes is limited by the overhead introduced by the workload function calls and OpenMP runtime.
For large transfer sizes, the performance flattens out as the elapsed time is dominated by the actual data transfer over the SMP bus, achieving $30$ GB/s in the read workload and $25$ GB/s in the write workload on S822LC.
This is consistent with the theoretical 38.4 GB/s performance of the X bus on S822LC.

\todo{Data for DGX-1, AC922}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/s822lc_cpu0-cpu1-dst.pdf}
        \caption{}
        \label{fig:s822lc-cpu0-cpu1}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/s822lc_cpu0-cpu1-src.pdf}
        \caption{}
        \label{fig:s822lc-cpu0-cpu1}
    \end{subfigure}
    \caption[\todo{short}]{
        CPU0 to CPU1 transfer bandwidth vs. transfer size.
        The bandwidth achieved by one through 16 simultaneous worker threads is shown, with the maximum observed bandwidth across the number of threads highlighted.
        (a) shows multi-threaded reads from CPU1 of an allocation on CPU0's NUMA Node on S822LC.
        (b) shows multi-threaded writes from CPU0 to an allocation on CPU1's NUMA Node on S822LC.
    }
    \label{fig:cpu0-cpu1}
\end{figure}



\section{CPU / GPU Transfers}
\label{sec:explicit-cpu-gpu}

Explicit CPU-GPU transfers are caused by the \texttt{cudaMemcpy} family of functions being invoked on one pointer to a host allocation and one pointer to a device allocation.
The host allocation may be pageable (created by \texttt{malloc} or \texttt{new}), or be created by \texttt{cudaMallocHost} or \texttt{cudaHostAlloc}.
The device allocation is created by \texttt{cudaMalloc}.
This section covers
\begin{itemize}
\item Comparing bandwidth achievable from pinned, pageable, and write-combining host allocations (Section~\ref{sec:explicit-pageable-pinned-wc})
\item The effect of device affinity on transfer performance (Section~\ref{sec:explicit-cpu-gpu-affinity})
\item Cases of observed anisotropic transfer properties (Section~\ref{sec:explicit-cpu-gpu-anisotropy})
\item Cases of differing performance on identical links (Section~\ref{sec:explicit-cpu-gpu-identical})
\end{itemize}

Algorithm~\ref{alg:explicit} is used to evaluate the achievable bandwidth when \texttt{cudaMemcpy} is used to transfer data between a GPU allocation and a pageable, pinned, or write-combining host allocation. 
The same algorithm can be used for these cases, because the same \texttt{cudaMemcpy} CUDA API call to transfer data can be used on a pointer pointing to any of the allocation types.
Depending on the source and destination types $src$ and $dst$, and the desired host allocation type, the corresponding CUDA or numactl APIs are called to bind later activities to the desired GPU or CPU.
Then, the CUDA and system allocators are invoked to produce $devPtr$ (a pointer to the device allocation) and $srcPtr$ (a pointer to the CPU allocation).
Then, \texttt{cudaMemcpy} is invoked $num\_iters$ times, and the fastest result is reported.
This helps remove any jitter from the results.
Time is recorded by wrapping \texttt{cudaMemcpy} in a call to the operating system clock.
Consequently, the reported time includes the actual transfers, as well as any overhead for invoking the transfer.
For pageable transfers, a \texttt{cudaDeviceSynchronize} is also included in the time, as cudaMemcpy may return before the device DMA is finished.


\begin{algorithm}

    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$}
        \If{$src$ is GPU}
            \State \texttt{cudaSetDevice($src$}
        \Else \Comment{$src$ is CPU}
            \State \texttt{numa\_bind($src$}
        \EndIf
        \If{$dst$ is GPU}
        \State \texttt{cudaSetDevice($dst$)}
        \Else \Comment{$dst$ is CPU}
        \State \texttt{numa\_bind($dst$)}
        \EndIf

        \State $devPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{device allocation}
        \State $srcPtr \gets$ \texttt{malloc($transfer\_size$)} \Comment{or cudaHostAlloc()}

        \State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
        \For{$i \gets 1 \textrm{ to } num\_iters$}
            \State $start \gets$ walltime()
            \State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
            \State \texttt{cudaDeviceSynchronize()}
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor

        \Return $elapsed$
    \EndFunction

    \end{algorithmic}
    \caption[Measuring CPU/GPU bandwidth with \texttt{cudaMemcpy}]{
        Measuring CPU/GPU bandwidth with \texttt{cudaMemcpy}
    }
    \label{alg:explicit}
\end{algorithm}

\subsection{Comparison of Pageable, Pinned, and Write-Combining Host Allocations}
\label{sec:explicit-pageable-pinned-wc}

To contextualize other results presented in this chapter, Figure~\ref{fig:pageable-pinned-wc} shows the transfer performance from pageable, pinned, and write-combined allocations on CPU0 to device allocations on GPU0.
These performance curves exhibit features common throughout this chapter:
\begin{itemize}
\item For small transfer sizes, the time is dominated by overhead introduced by the abstraction layer.
\item For large transfer sizes, the time is dominated by bandwidth limits on the exercised physical link.
\item The performance may vary smoothly across intermediate transfer sizes, or exhibit more complicated behavior.
\end{itemize}

The curves for transfers from a pinned allocation on CPU0 to GPU0 shown in Figure~\ref{fig:pinned-cpu-gpu0} share a similar shape: the transfer bandwidth is low for small sizes, and eventually saturates once transfers become large enough.
For small transfer sizes, the elapsed time is dominated by overhead from invoking the transfer (and the device synchronization for the pageable transfer).
This causes the upward slope as transfer size increases.
For larger transfers, the time is dominated by the actual cost of moving the data, and the performance saturates.
Both S822LC and AC922 achieve \mytilde 75\% of the theoretical performance allowed by the relevant interconnects, and DGX-1 achieves around 50\%.


In contrast, Figure~\ref{fig:pageable-cpu0-gpu0} shows transfers from pageable host allocations to GPU0.
The achievable bandwidth for large transfer sizes on S822LC and AC922 is reduced to approximately 50\% of the theoretical bandwidth provided by the link: $20$ GB/s for S822LC vs $40$ GB/s on for two-lane NVLink 1.0, $30$ GB/s on AC922 vs $75$ GB/s for three-lane NVLink 2.0.
DGX-1 remains around the same performance level at $10$ GB/s vs $20$ GB/s for one-lane NVLink 1.0 on DGX-1.

Section~\ref{sec:pinned-memory} describes how \texttt{cudaMemcpy} actually causes two copies for pageable allocations: one from the pageable allocation application to a pinned buffer, and a second DMA from the pinned buffer to the GPU.
When pinned memory transfers are faster than pageable memory, we can infer that the CPU memory copy from pageable allocation to pinned buffer is limiting the performance.
Figure~\ref{fig:explicit-pageable-cpu} shows the performance achieved by using \texttt{cudaMemcpy} to only do a copy from a pageable allocation to a pinned allocation.

\todo{any conclusions to draw?}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/generated/page-to-pin.pdf}
    \caption[\todo{short}]{\todo{long}}
    \label{fig:explicit-pageable-cpu}
\end{figure}

Furthermore, the topologically-similar S822LC and AC922 machines do not have similar shaped curves.
Figure~\ref{fig:wc-cpu0-gpu0} shows that write-combining allocations do not affect CPU-to-GPU performance on the tested systems.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/pinned_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:pinned-cpu0-gpu0}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/pageable_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:pageable-cpu0-gpu0}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/wc_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:wc-cpu0-gpu0}
    \end{subfigure}
    \caption[\todo{short}]{
        \texttt{cudaMemcpy} bandwidth vs transfer size for CPU0 to GPU0 transfers from 
        (a) pageable allocations,
        (b) pinned allocations, and
        (c) write-combining allocations.
        Results for S822LC, AC922, and DGX-1 systems are shown.
    }
    \label{fig:pageable-pinned-wc}
\end{figure}

\subsection{Affinity}
\label{sec:explicit-cpu-gpu-affinity}

There is a distinct performance difference due to affinity, or transfers between directly-connected components, and components that must traverse multiple hardware links.
Figure~\ref{fig:cpu-gpu-affinity-direction} shows some cases where the device physical affinity affects the performance of the logical communication path.
Table~\ref{tab:cpu-gpu-affinity} summarizes the effects.
Figures~\ref{fig:minsky_pageable_affinity} and \ref{fig:hal_pageable_affinity} show that affinity has a negligable effect on bandwidth for pagable transfers on the IBM machines.
Figure~\ref{fig:dgx_pageable_affinity} shows that GPU-to-CPU pageable transfers on DGX are affected by affinity.
Figure~\ref{fig:minsky_pinned_affinity} shows on S822LC, pinned bandwidth is affected both by affinity with local transfers at least $5$ GB/s faster than remote transfers at large sizes.
Figure~\ref{fig:hal_pinned_affinity} shows for pinned transfers on AC922, affinity has a much stronger effect.
Figure~\ref{fig:dgx_pinned_affinity} shows no effect from affinity on pinned transfers for DGX-1.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pageable_affinity.pdf}
        \caption{}
        \label{fig:minsky_pageable_affinity}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_pageable_affinity.pdf}
        \caption{}
        \label{fig:hal_pageable_affinity}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/dgx_pageable_affinity.pdf}
        \caption{}
        \label{fig:dgx_pageable_affinity}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_affinity_cpu0.pdf}
        \caption{}
        \label{fig:minsky_pinned_affinity}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_pinned_affinity.pdf}
        \caption{}
        \label{fig:hal_pinned_affinity}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/dgx_pinned_affinity.pdf}
        \caption{}
        \label{fig:dgx_pinned_affinity}

    \end{subfigure}
    \caption[\todo{short}]{
    Transfer bandwidth vs transfer size for local and remote transfers from pageable and pinned host allocations on S822LC, AC922, and DGX-1.
    (a-c) show transfers from pageable allocations to GPUs.
    (d-f) show transfers from pinned allocations to GPUs.
    (a) and (d) are for S822LC, (b) and (e) for AC922, and (c) and (f) for DGX-1.
    }
    \label{fig:cpu-gpu-affinity-direction}
\end{figure}



\begin{table}[ht]
    \centering
    \caption[Affinity and Logical Communication Bandwidth]{Effect of Device Affinity on Logical Transfer Bandiwdth}
    \label{tab:cpu-gpu-affinity}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}     & \textbf{S822LC}                                    & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\rightarrow$ GPU & $\times$ (Fig.~\ref{fig:minsky_pageable_affinity}) & $\times$   (Fig.~\ref{fig:hal_pageable_affinity}) & $\times$ (Fig.~\ref{fig:dgx_pageable_affinity})\\ \hline
    Pageable $\leftarrow$ GPU  & \makecell{ for intermediate \\ sizes (Fig.~\ref{fig:minsky_pageable_affinity}) } & \makecell{ for intermediate \\ sizes (Fig.~\ref{fig:hal_pageable_affinity}) } & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
    Pinned $\rightarrow$ GPU   & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity}) & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & $\times$  (Fig.~\ref{fig:dgx_pinned_affinity})\\ \hline
    Pinned $\leftarrow$ GPU    & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity}) & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & $\times$ (Fig.~\ref{fig:dgx_pinned_affinity}) \\ \hline
    \end{tabular}
\end{table}

\subsection{Anisotropy}
\label{sec:explicit-cpu-gpu-anisotropy}

Figure~\ref{fig:cpu-gpu-affinity-direction} also highlights that direction of transfer can affect performance.
Table~\ref{tab:explicit-anisotropy} summarizes the effects.
Particularly for the pageable transfers shown in Figures~\ref{fig:minsky_pageable_affinity} and \ref{fig:hal_pageable_affinity}, corresponding transfers are shown to be highly anisotropic.
In the pinned transfers on S822LC (Fig.~\ref{fig:minsky_pinned_affinity}), all transfers show some degree of anisotropy, with a larger effect over remote transfers.
For AC922, (Fig.~\ref{fig:hal_pinned_affinity}), local transfers show anisotropy only for intermediate transfer sizes, and remote transfers do generally.
For DGX-1, pinned transfers all show 2 GB/s of anisotropy (Fig.~\ref{fig:dgx_pinned_affinity}), while the degree of anisotropy in pageable allocations depends on the affinity (Fig.~\ref{fig:dgx_pageable_affinity}).

\begin{table}[ht]
    \centering
    \caption[Summary of Host-Device Transfer Anisotropy]{Summary of Host-Device Transfer Anisotropy}
    \label{tab:explicit-anisotropy}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                         & \textbf{S822LC}     & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\leftrightarrow$ GPU (local)         & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity})                          & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity}) \\ \hline
    Pageable $\leftrightarrow$ GPU (remote)        & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity})                          & \checkmark (Fig.~\ref{fig:dgx_pageable_affinity})\\ \hline
    Pinned $\leftrightarrow$ GPU (local)           & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \makecell{for intermediate \\ sizes (Fig.~\ref{fig:hal_pinned_affinity}) } & \checkmark (Fig.~\ref{fig:dgx_pinned_affinity}) \\ \hline
    Pinned $\leftrightarrow$ GPU (remote)          & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})                            & \checkmark (Fig.~\ref{fig:dgx_pinned_affinity}) \\ \hline
    \end{tabular}
\end{table}

\subsection{Differences between Identical Transfers}
\label{sec:explicit-cpu-gpu-identical}

Figure~\ref{fig:minsky_pageable_cpu1-gpu01} shows transfer performance on two different identical links: CPU1-GPU0 and CPU1-GPU1.
Both of the scenarios presented involve identical hardware and logical links, yet there is a 10\% bandwidth different at intermediate sizes.
Table~\ref{tab:explicit-identical} summarizes scenarios where the transfer performance differs on identical links.
S822LC is the only system where this behavior is observed for CPU-GPU transfers.

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.45\textwidth]{figures/generated/minsky_pageable_cpu1-gpu01.pdf}
    \caption[\todo{short}]{
        Transfer bandwidth vs size for a pageable allocation on CPU1 to an allocation on GPU0 or GPU1.
    }
    \label{fig:minsky_pageable_cpu1-gpu01}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate vary on identical links]{Scenarios where bandwidth is observed to differ on identical links}
    \label{tab:explicit-identical}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}     & \textbf{S822LC}     & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\rightarrow$ GPU & fot intermediate sizes (Fig.~\ref{fig:minsky_pageable_cpu1-gpu01}) & $\times$ & $\times$ \\ \hline
    Pageable $\leftarrow$ GPU  & $\times$ & $\times$ & $\times$ \\ \hline
    Pinned $\rightarrow$ GPU   & $\times$ & $\times$ & $\times$ \\ \hline
    Pinned $\leftarrow$ GPU    & $\times$ & $\times$ & $\times$ \\ \hline
    \end{tabular}
\end{table}

\section{GPU / GPU Transfers}
\label{sec:explicit-gpu-gpu}

Explicit GPU-GPU transfers are caused by the \texttt{cudaMemcpy} family of functions being invoked on pointers to device allocations created with \texttt{cudaMalloc}.
Unlinke the different types of host allocations CPU-GPU transfers described in Section~\ref{sec:explicit-cpu-gpu}, this section only refers to a single kind of device allocation.
Device allocations come with the concept of peer access, discussed in Section~\ref{sec:cuda-peer}.
This section covers
\begin{itemize}
\item The effect of peer access on transfer bandwidth (Section~\ref{sec:explicit-peer-bandwidth})
\item Cases of observed anisotropic transfer properties (Section~\ref{sec:explicit-peer-direction})
\item Cases of differing performance on identical links (Section~\ref{sec:explicit-peer-identical})
\end{itemize}

Algorithm~\ref{alg:explicit} is used to evaluate the achievable GPU-GPU transfer bandwidth.
First, peer access is enabled or disabled depending on the experimental configuration.
Then, \texttt{cudaMalloc} is used to create allocations of $transfer\_size$ bytes pointed to by $srcPtr$ and $dstPtr$.
Finally, the achievable bandwidth is measured using the wall time for $num\_iters$ iterations, and the minimum elapsed time is reported, to help remove jitter from the results.

\begin{algorithm}
    \caption{Measuring explicit \texttt{cudaMemcpy} performance}
    \label{alg:explicit}
    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$, $peer\_access$}
        \If{$peer\_access$}
            \State \texttt{cudaSetDevice($src$)}
            \State \texttt{cudaDeviceEnablePeerAccess($dst$)}
            \State \texttt{cudaSetDevice($dst$)}
            \State \texttt{cudaDeviceEnablePeerAccess($src$)}
        \Else
            \State \texttt{cudaSetDevice($src$)}
            \State \texttt{cudaDeviceDisablePeerAccess($dst$)}
            \State \texttt{cudaSetDevice($dst$)}
            \State \texttt{cudaDeviceDisablePeerAccess($src$)}        
        \EndIf

        \State \texttt{cudaSetDevice($src$)} \Comment{Source allocation}
        \State $srcPtr \gets$ \texttt{cudaMalloc($transfer\_size$)}

        \State \texttt{cudaSetDevice($dst$)} \Comment{Destination allocation}
        \State $dstPtr \gets$ \texttt{cudaMalloc($transfer\_size$)}

        \State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
        \For{$i \gets 1 \textrm{ to } num\_iters$}
            \State $start \gets$ walltime()
            \State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor

    \Return $elapsed$
    \EndFunction

    \end{algorithmic}
\end{algorithm}

\subsection{Transfer Rate and Peer Access}
\label{sec:explicit-peer-bandwidth}

Figure~\ref{fig:explicit-peer} shows the performance of a variety of GPU-GPU transfers with and without peer access.
Generally, peer access has a large effect on the bandwidth of local GPU-GPU transfers.
Figure~\ref{fig:explicit-s822lc-peer} shows that enabling peer access on S822LC improves the performance of GPU-GPU transfers by \mytilde30\%.
Likewise, ~\ref{fig:explicit-dgx-peer-nopeer-local} shows peer access roughly doubling performance on DGX-1.
With peer access enabled, GPUs may do DMAs directly with their peer memory instead of copying through the host.
Likewise, Figure~\ref{fig:explicit-hal-peer-local} shows similar behavior for AC922, with the much higher performance ceiling due to the increased bandwidth of NVLink 2.0.
For remote transfers on AC922, shown in Figure~\ref{fig:explicit-hal-peer-remote}, enabling peer access actually reduces the performance, though the effect is not very large.
On remote transfers, with or without peer access, the data must traverse several hardware links with limited bandwidth.
Table~\ref{tab:explicit-peer} summarizes the scenarios where peer access affects transfer performance.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_memcpy_local.pdf}
        \caption{}
        \label{fig:explicit-s822lc-peer}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_peer_local.pdf}
        \caption{}
        \label{fig:explicit-hal-peer-local}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_peer_remote.pdf}
        \caption{}
        \label{fig:explicit-hal-peer-remote}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/dgx_peer_nopeer_local.pdf}
        \caption{}
        \label{fig:explicit-dgx-peer-nopeer-local}
    \end{subfigure}

    \caption[\todo{short}]{
        Transfer bandwidth vs transfer size for various scenarios with peer access enabled or disabled.
        (a) shows a GPU0 to GPU1 transfer on S822LC with and without peer access,
        (b) shows GPU to GPU local transfers on AC922.
        (c) shows GPU to GPU remote transfers on AC922.
        (d) shows GPU to GPU local transfers on DGX-1.
        S822LC and DGX-1 do not support peer access for remote GPU-GPU transfers.
    }
    \label{fig:explicit-peer}
\end{figure}


\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by peer access]{Is transfer rate affected by peer access?}
    \label{tab:explicit-peer-rate}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}       & \textbf{S822LC} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    GPU $\rightarrow$ Local GPU  & \checkmark      & \checkmark     & \checkmark \\ \hline
    GPU $\rightarrow$ Remote GPU & N/A             & \checkmark     & N/A \\ \hline
    \end{tabular}
\end{table}


\subsection{Transfer Anisotropy with Peer Access Disabled}
\label{sec:explicit-peer-direction}

Disabling peer access causes anisotropy to be observed in GPU-to-GPU transfers on S822LC and AC922.
No anisotropy is oberved on DGX-1.
This anisotropy is not consistent with anisotpropy observed on the intervening links.
For example, consider Figure~\ref{fig:explicit-peer-anisotropy}a-c.
Figure~\ref{fig:minsky-explicit-nopeer-remote} shows anisotropy along the remote GPU0-GPU2 transfer.
Figures~\ref{fig:minsky-explicit-path-gpu0-gpu2}~and~\ref{fig:minsky-explicit-path-gpu2-gpu0} show pinned transfer speeds along GPU0-CPU0-CPU1-GPU2 and GPU2-CPU1-CPU0-GPU0 paths.
The observed GPU0-GPU2 bandwidth is sometimes higher than the higher observed bandwidth on the path components, and sometimes lower than the lowest path component.
Figure~\ref{fig:explicit-peer-anisotropy} highlights anisotropic remote GPU-GPU transfers on S822LC and AC922.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_remote.pdf}
        \caption{}
        \label{fig:minsky-explicit-nopeer-remote}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_path_gpu0-gpu2.pdf}
        \caption{}
        \label{fig:minsky-explicit-path-gpu0-gpu2}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_path_gpu2-gpu0.pdf}
        \caption{}
        \label{fig:minsky-explicit-path-gpu2-gpu0}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_nopeer_remote.pdf}
        \caption{}
        \label{fig:explicit-ac922-nopeer-remote}
    \end{subfigure}
    \caption[\todo{short}]{
        Transfer bandwidth vs transfer size when peer access is disabled.
        (a) shows anisotropy on transfers between GPU0 and GPU2 on S822LC.
        (b) and (c) show transfer rates along the GPU0-CPU0 and CPU1-GPU2 components of the links.
        Although (b) and (c) are the same, the anisotropy still exists in the aggregated logical link.
        (d) shows anisotropy on AC922.
    }
    \label{fig:explicit-peer-anisotropy}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[GPU-GPU Transfer Anisotropy]{GPU-GPU Transfer Anisotropy}
    \label{tab:explicit-peer-direction}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                           & \textbf{S822LC} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    GPU $\leftrightarrow$ Local GPU  (peer enabled)  & $\times$        & $\times$       & $\times$ \\ \hline
    GPU $\leftrightarrow$ Remote GPU (peer enabled)  & N/A             & $\times$       & N/A      \\ \hline
    GPU $\leftrightarrow$ Local GPU  (peer disabled) & $\times$        & $\times$       & $\times$ \\ \hline
    GPU $\leftrightarrow$ Remote GPU (peer disabled) & \checkmark (Fig.~\ref{fig:minsky-explicit-nopeer-remote})  & \checkmark (Fig.~\ref{fig:explicit-ac922-nopeer-remote})    & $\times$ \\ \hline
    \end{tabular}
\end{table}

\subsection{Transfer Rate on Identical Transfers}
\label{sec:explicit-peer-identical}

Like CPU-GPU transfers, different performance is observed on identical GPU-GPU transfers when peer access is disabled.
Figure~\ref{fig:explicit-nopeer-identical} show some example scenarios.
Figure~\ref{fig:explicit-nopeer-identical-ac922-local} compares the bandwidth of transfers from GPU0 to GPU1 and GPU2 to GPU3 on AC922.
These transfers are logically and physically identical: they have the same direction, between identical sub-topologies, and use the same CUDA APIs.
Surprisingly, the achieved bandwidth varies by nearly a factor of two.
Figure~\ref{fig:explicit-nopeer-identical-ac922-remote} shows similar behavior for remote transfers, though with a smaller magnitude of effect.
Figures~\ref{fig:explicit-nopeer-identical-s822lc-local} and ~\ref{fig:explicit-nopeer-identical-s822lc-remote} show the same for S822LC.
Figure~\ref{fig:explicit-nopeer-identical-dgx} shows how GPU0-GPU1, GPU2-GPU3, GPU4-GPU5, and GPU6-GPU7 transfers are slower than all other local transfers (GPU0-GPU2, for example).
Table~\ref{tab:explicit-identical} summarizes the cases where differing performance is observed.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_nopeer_identical-local.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-ac922-local}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_nopeer_identical-remote.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-ac922-remote}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_identical-local.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-s822lc-local}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_identical-remote.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-s822lc-remote}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/dgx_nopeer_identical.pdf}
        \caption{}
        \label{fig:explicit-nopeer-identical-dgx}
    \end{subfigure}
    \caption[\todo{short}]{
        Transfer bandwidth vs transfer size for AC922 and S822LC.
        (a) and (b) show differeing transfer bandwidth on logically-identical local and remote transfers for AC922.
        (c) and (d) show the same for S822LC.
        (e) shows the same for DGX-1.
    }
    \label{fig:explicit-nopeer-identical}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate on Identical Links]{Does the GPU transfer rate vary on identical links?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                           & \textbf{S822LC} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    \makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer enabled)  } & $\times$                                                            & $\times$                                                           & $\times$                                                  \\ \hline
    \makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer enabled)  } & N/A                                                                 & $\times$                                                           & N/A                                                       \\ \hline
    \makecell{ GPU $\leftrightarrow$ Local GPU  \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-s822lc-local})  & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-ac922-local})  & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-dgx}) \\ \hline
    \makecell{ GPU $\leftrightarrow$ Remote GPU \\ (peer disabled) } & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-s822lc-remote}) & \checkmark (Fig.~\ref{fig:explicit-nopeer-identical-ac922-remote}) & $\times$                                                  \\ \hline
    \end{tabular}
\end{table}

\section{Summary}

