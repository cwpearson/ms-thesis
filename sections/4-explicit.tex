\chapter{Explicit Memory Performance}
\label{ch:explicit}

This chapter is broken into two sections, describing CPU-GPU transfers (Section~\ref{sec:explicit-cpu-gpu}) and GPU-GPU transfers (Section~\ref{sec:explicit-gpu-gpu}).
It highlights results that show surprising behaviors of CUDA data transfers.
Full results can be found in Appendix~\ref{ch:data}.

\section{CPU / GPU Transfers}
\label{sec:explicit-cpu-gpu}

Algorithm~\ref{alg:explicit} is used to evaluate the achievable bandwidth for the logical endpoints and methods shown in Table~\ref{tab:explicit-cpu-gpu}.
The same algorithm can be used for these cases, because the same \texttt{cudaMemcpy} CUDA API call to transfer data can be used on a pointer pointing to any of the allocation types.
Depending on the source and destination types $src$ and $dst$, and the desired host allocation type, the corresponding CUDA or numactl APIs are called to bind later activities to the desired GPU or CPU.
Then, the CUDA and system allocators are invoked to produce $devPtr$ (a pointer to the device allocation) and $srcPtr$ (a pointer to the CPU allocation).
Then, \texttt{cudaMemcpy} is invoked $num\_iters$ times, and the fastest result is reported.
This helps remove any jitter from the results.

\begin{table}[ht]
    \centering
    \caption[\todo{short}]{\todo{long}}
    \label{tab:explicit-cpu-gpu}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Source} & \textbf{Transfer Methods} \\ \hline 
    \makecell{Host Pinned Allocation \\ Host Pageable Allocation \\ Host Write-Combining Allocation \\ Device Allocation} & cudaMemcpy \\ \hline
    \end{tabular}
\end{table}

\begin{algorithm}
    \caption{Measuring explicit \texttt{cudaMemcpy} performance}
    \label{alg:explicit}
    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$}
        \If{$src$ is GPU}
            \State \texttt{cudaSetDevice($src$}
        \Else \Comment{$src$ is CPU}
            \State \texttt{numa\_bind($src$}
        \EndIf
        \If{$dst$ is GPU}
        \State \texttt{cudaSetDevice($dst$)}
        \Else \Comment{$dst$ is CPU}
        \State \texttt{numa\_bind($dst$)}
        \EndIf

        \State $devPtr \gets$ \texttt{cudaMalloc($transfer\_size$)} \Comment{device allocation}
        \State $srcPtr \gets$ \texttt{malloc($transfer\_size$)} \Comment{or cudaHostAlloc()}

        \State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
        \For{$i \gets 1 \textrm{ to } num\_iters$}
            \State $start \gets$ walltime()
            \State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor

        \Return $elapsed$
    \EndFunction

    \end{algorithmic}
\end{algorithm}

\subsection{Comparison of Pageable, Pinned, and Write-Combining Host Allocations}

To contextualize other results presented in this chapter, Figure~\ref{fig:pageable-pinned-wc} shows the transfer performance from pageable, pinned, and write-combined allocations on CPU0 to device allocations on GPU0.
These performance curves exhibit features common throughout this chapter:
\begin{itemize}
\item For small transfer sizes, the time is dominated by overhead introduced by the abstraction layer.
\item For large transfer sizes, the time is dominated by bandwidth limits on the exercised physical link.
\item The performance may vary smoothly across intermediate transfer sizes, or exhibit more complicated behavior.
\end{itemize}

For transfers from pageable host allocations to the GPU (Figure~\ref{fig:pageable-cpu0-gpu0}), the observed performance is substantially lower than the hardware specifications of the system suggest, and furthermore, the topologically-similar Minsky and AC922 machines do not have similar shaped curves.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/pageable_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:pageable-cpu0-gpu0}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/pinned_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/wc_cpu0-gpu0.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    \caption[\todo{short}]{
        \texttt{cudaMemcpy} performance for CPU0 to GPU0 transfers from 
        (a) pageable allocations,
        (b) pinned allocations, and
        (c) write-combining allocations.
    }
    \label{fig:pageable-pinned-wc}
\end{figure}

\subsection{Effect of Direction and Affinity}

There is a distinct performance difference due to affinity, or transfers between components within a triad (CPU0-GPU0-GPU1 or CPU1-GPU2-GPU3) and components across triads.
There is also a performance affect from direction, where performance of a transfer between a CPU and GPU depends on the direction of the transfer.
Figure~\ref{fig:cpu-gpu-affinity-direction} highlights some cases that are impacted by affinity or direction.
Table~\ref{tab:cpu-gpu-affinity} summarizes the effects.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pageable_affinity.pdf}
        \caption{}
        \label{fig:minsky_pageable_affinity}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_pageable_affinity.pdf}
        \caption{}
        \label{fig:hal_pageable_affinity}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_affinity_cpu0.pdf}
        \caption{}
        \label{fig:minsky_pinned_affinity}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_pinned_affinity.pdf}
        \caption{}
        \label{fig:hal_pinned_affinity}
    \end{subfigure}
    \caption[\todo{short}]{Effect of Direction and Affinity on data transfers.
    (a) shows transfers involving pageable allocations on CPU0, to device allocations on GPU0 and GPU2 in both directions. The direction of the transfer has a large performance impact, with transfers between 2-24 MB being the fastest. Affinity plays a small role.
    (b) is the same, except for AC922. Again, direction is a stronger effect than affinity.
    (c) and (d) are the same as (a) and (b), except with pinned transfers. 
    For AC922, affinity has a stronger effect, and for Minsky, both affinity and direction impact performance.
    }
    \label{fig:cpu-gpu-affinity-direction}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by affinity]{Does affinity affect transfer rate?}
    \label{tab:cpu-gpu-affinity}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind} & \textbf{Minsky} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\rightarrow$ GPU        & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & $\times$   (Fig.~\ref{fig:hal_pageable_affinity}) & \\ \hline
    Pageable $\leftarrow$ GPU         & \checkmark (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \\ \hline
    Pinned $\rightarrow$ GPU          & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & \\ \hline
    Pinned $\leftarrow$ GPU           & \checkmark (Fig.~\ref{fig:minsky_pinned_affinity})   & \checkmark (Fig.~\ref{fig:hal_pinned_affinity})   & \\ \hline
    \end{tabular}
\end{table}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_affinity_cpu0.pdf}
        \caption{}
        \label{fig:minsky_pinned_affinity}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_affinity_cpu1.pdf}
        \caption{}
        \label{fig:minsky_pinned_affinity}
    \end{subfigure}
    \caption[\todo{short}]{
        (a) shows pinned transfers between CPU0 and local or remote GPUs.
        For local and remote transfers, the direction affects the bandwidth.
        (b) shows pinned transfers between CPU1 and local or remote GPUs.
        For remote transfers, the direction does not affect the bandwidth.

    }
    \label{fig:minsky-direction}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by direction]{Does direction affect transfer rate?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                         & \textbf{Minsky}     & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\leftrightarrow$ GPU (local)         & \checkmark     (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \\ \hline
    Pageable $\leftrightarrow$ GPU (remote)        & \checkmark     (Fig.~\ref{fig:minsky_pageable_affinity}) & \checkmark (Fig.~\ref{fig:hal_pageable_affinity}) & \\ \hline
    Pinned $\leftrightarrow$ GPU (local)           & \checkmark     (Fig.~\ref{fig:minsky-direction})         & for intermediate sizes (Fig.~\ref{fig:hal_pinned_affinity})  & \\ \hline
    Pinned $\leftrightarrow$ GPU (remote)          & depends on CPU (Fig.~\ref{fig:minsky-direction})         & \checkmark             (Fig.~\ref{fig:hal_pinned_affinity})& \\ \hline
    \end{tabular}
\end{table}

\subsection{Effect of Write-Combining}

On the Minsky machine, whether or not the allocation is write-combining influences the performance of transfers involving CPU0.
Figure~\ref{fig:minsky-wc} shows the behavior.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_pinned_cpu0-local.pdf}
        \caption{}
        \label{fig:minsky_pinned_cpu0-local}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_wc_cpu0-local.pdf}
        \caption{}
        \label{fig:minsky_wc_cpu0-local}
    \end{subfigure}
    \caption[\todo{short}]{
        (a) and (b) compare observed bandwidth when transferring from GPU0 or GPU1 to a pinned or write-combined allocation on CPU0.
        This difference in performance is not observed for transfers involving CPU1.
    }
    \label{fig:minsky-wc}
\end{figure}

Minsky local wc is different from minsky pinned

\subsection{Differences between Identical Transfers}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/generated/minsky_pageable_cpu1-gpu01.pdf}
    \caption[\todo{short}]{\todo{long}}
    \label{fig:minsky_pageable_cpu1-gpu01}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate vary on identical links]{Does transfer rate vary on identical links?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                & \textbf{Minsky}     & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    Pageable $\rightarrow$ GPU        & $\times$   & $\times$ & \\ \hline
    Pageable $\leftarrow$ GPU         & \checkmark & $\times$ & \\ \hline
    Pinned $\rightarrow$ GPU          & $\times$   & $\times$ & \\ \hline
    Pinned $\leftarrow$ GPU           & $\times$   & $\times$ & \\ \hline
    \end{tabular}
\end{table}

Figure~\ref{fig:minsky_pageable_cpu1-gpu01} shows the performance of \texttt{cudaMemcpy} moving data between pageable host allocations on CPU1 to GPU1 on Minsky.

\section{GPU / GPU Transfers}
\label{sec:explicit-gpu-gpu}

Algorithm~\ref{alg:explicit} is used to evaluate the achievable bandwidth for the logical endpoints and methods shown in Table~\ref{tab:explicit}.

\begin{table}[ht]
    \centering
    \caption[\todo{short}]{\todo{long}}
    \label{tab:explicit}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Endpoints} & \textbf{Transfer Methods} \\ \hline 
    Device Allocation & cudaMemcpy (peer access disabled) \\ \hline
    Device Allocation & cudaMemcpy (peer access enabled) \\ \hline
    \end{tabular}
\end{table}

\begin{algorithm}
    \caption{Measuring explicit \texttt{cudaMemcpy} performance}
    \label{alg:explicit}
    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_iters$, $peer\_access$}
        \If{$peer\_access$}
            \State \texttt{cudaSetDevice($src$)}
            \State \texttt{cudaDeviceEnablePeerAccess($dst$)}
            \State \texttt{cudaSetDevice($dst$)}
            \State \texttt{cudaDeviceEnablePeerAccess($src$)}
        \Else
            \State \texttt{cudaSetDevice($src$)}
            \State \texttt{cudaDeviceDisablePeerAccess($dst$)}
            \State \texttt{cudaSetDevice($dst$)}
            \State \texttt{cudaDeviceDisablePeerAccess($src$)}        
        \EndIf

        \State \texttt{cudaSetDevice($src$)} \Comment{Source allocation}
        \State $srcPtr \gets$ \texttt{cudaMalloc($transfer\_size$)}

        \State \texttt{cudaSetDevice($dst$)} \Comment{Destination allocation}
        \State $dstPtr \gets$ \texttt{cudaMalloc($transfer\_size$)}

        \State $elapsed \gets infinity$ \Comment{minimum of $num\_iters$ observations}
        \For{$i \gets 1 \textrm{ to } num\_iters$}
            \State $start \gets$ walltime()
            \State \texttt{cudaMemcpy($dst$,$src$,$transfer\_size$)}
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor

    \Return $elapsed$
    \EndFunction

    \end{algorithmic}
\end{algorithm}

\subsection{Transfer Rate and Peer Access}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_memcpy_local.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_peer_local.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_peer_remote.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    \caption[\todo{short}]{\todo{long}}
    \label{fig:}
\end{figure}


\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by peer access]{Is transfer rate affected by peer access?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}       & \textbf{Minsky} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    GPU $\rightarrow$ Local GPU  & \checkmark      & \checkmark     & \\ \hline
    GPU $\rightarrow$ Remote GPU & N/A             & \checkmark     & \\ \hline
    \end{tabular}
\end{table}


\subsection{Transfer Rate and Direction}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_remote.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_nopeer_remote.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    \caption[\todo{short}]{\todo{long}}
    \label{fig:}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate affected by direction]{Is GPU-GPU transfer rate affected by direction?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                           & \textbf{Minsky} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    GPU $\leftrightarrow$ Local GPU  (peer enabled)  & $\times$        & $\times$       & \\ \hline
    GPU $\leftrightarrow$ Remote GPU (peer enabled)  & N/A             & $\times$       & \\ \hline
    GPU $\leftrightarrow$ Local GPU  (peer disabled) & $\times$        & $\times$       & \\ \hline
    GPU $\leftrightarrow$ Remote GPU (peer disabled) & \checkmark      & \checkmark     & \\ \hline
    \end{tabular}
\end{table}



\subsection{Transfer Rate on Identical Links}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_nopeer_identical-local.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_nopeer_identical-remote.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_identical-local.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/minsky_nopeer_identical-remote.pdf}
        \caption{}
        \label{fig:}
    \end{subfigure}
    \caption[\todo{short}]{\todo{long}}
    \label{fig:}
\end{figure}

\begin{table}[ht]
    \centering
    \caption[Matrix: Transfer rate on Identical Links]{Does the GPU transfer rate vary on identical links?}
    \label{tab:explicit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind}                           & \textbf{Minsky} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    GPU $\leftrightarrow$ Local GPU  (peer enabled)  & $\times$        & $\times$       & \\ \hline
    GPU $\leftrightarrow$ Remote GPU (peer enabled)  & N/A             & $\times$       & \\ \hline
    GPU $\leftrightarrow$ Local GPU  (peer disabled) & \checkmark      & \checkmark     & \\ \hline
    GPU $\leftrightarrow$ Remote GPU (peer disabled) & \checkmark      & \checkmark     & \\ \hline
    \end{tabular}
\end{table}
