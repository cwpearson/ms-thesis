\chapter{Future Work: Application Characterization and Combined Modeling}
\label{ch:future}

\section{Mapping Logical Communication to Underlying Links}

Previous sections described the approach and results of creating a performance model of the logical communication paths.
Those models took some knowledge of the underlying system configuration as \textit{a priori} knowledge, but in general this process should be automated.
Application developers who want to understand the performance of the system may not have the architecture background to select appropriate performance models.
System architects who develop system capabilties may not understand the implications their choices have on performance at the top abstraction layer.
This work presents an initial step towards automation with automatic hardware enumeration described in Section~\ref{sec:hardware-enumeration}.

The next step is to systematically instrument all possible links for observation during the logical workloads.
If the instrumentation is low-overhead, this could be done jointly with logical performance measurements.
If not, known, separate workloads could be sent over logical communication paths to observe hardware links.
For NVLink monitoring, NVML provides the ability to query the link utilization counters~\cite{nvidia2017nvmlreference}.
For PCIe monitoring, the Performance Counter Monitor~\cite{opcm2018pcm} project provides the ability to query PCIe link counters.

For each enumerated link, the utilized hardware components could be associated and used to select the appropriate performance model.

\section{Application Model}

After a model of system communication performance is established, the next step is to establish an understanding of how applications use the logical communication paths.
This would encapsulate information about how an application produces, moves, and consumes data.
The application can be modeled as a dynamic value dependence graph (DVDG) $G_a = \{E_a,V_a\}$ where $E_a$ is a set edges representing data transfer, and $V_a$ is a set of vertices representing data values.
Each value represents a contiguous region of memory that the program interacts with.
Each edge is either an explicit memory copy or a CUDA kernel launch.
The edges can be tagged with observed timestamps to facilitate program performance analysis.

\todo{include an example graph from a cuda snippet?}

Through it is generally possible to generate these dependence graphs by hand, it is not feasible for complicated allocations.
Furthermore, as applications are updated, new models would have to be manually generated.
The proposed method described in this section would automate that process.

\section{Constructing the Dynamic Value Dependence Graph for Unmodified Applications}

Ideally, the DVDG would be generated from an unmodified application execution.
This ensures that the tool is as accessible to users as possible, and that it can work on closed-source applications
Unfortunately, the DVDG cannot rely on the application to advertise any helpful information about its behavior, and any relevant application state must be observed through its interaction with the operating system.
The proposed system (``apptracer'') leverages two tools available on the Linux platform: the CUDA Profiling Tools Interface(Section~\ref{sec:cupti} (CUPTI) and the Linux \texttt{LD\_PRELOAD}(Section~\ref{sec:ldpreload}) mechanism.

\texttt{apptracer} would use CUPTI to capture most CUDA-related information, and LD\_PRELOAD for everything else.
CUPTI allows The Tools to provide a callback function that is invoked at every CUDA runtime or driver call, and also allows \texttt{apptracer} to collect any performance metrics the GPU exposes.
The callback function records relevant information, including the wall time when the CUDA runtime function is invoked, its arguments, and the device and stream associated with the call.
In this way, detailed information about data transfers from runtime functions can be reconstructed.
For example, allocations from \texttt{cudaMalloc} can be associated with pointers passed to \texttt{cudaMemcpy} to discover data transfers from host to device.

\todo{hwcomm-tracer} uses LD\_PRELOAD mechanism to intercept known API calls made by the application to shared libraries.
Specifically, LD\_PRELOAD is used to watch for \todo{filesystem access}, CUDA's cuDNN library, CUDA's cuBLAS library, \todo{network access}, and \todo{system memory allocations}.
The various kernel launches and allocations used by cuDNN and cuBLAS are already visible through CUPTI, but the known semantics of the higher-level cuBLAS and cuDNN calls allow for a more detailed application model.

One challenge of \texttt{apptracer} is handling implicit data movement from three scenarios:
data moved from GPU global memory to arbitrary GPU kernels, implicit data movement between remote mappings, and implicit data movement through the unified memory system.
On supported systems, GPUs can directly access data that is on the host without making any CUDA runtime calls.
On supported systems, GPUs can directly access data that is resident on other GPUs without making any CUDA runtime calls.
Although \todo{hwcomm-apptracer} can record pointer kernel arguments, that alone does not provide information about the size of the data transfer, only the source and destination of that data.
CUPTI allows the GPU to record detailed profiling information, but this affects program execution time and distorts the timeline.
A two-pass approach, once to collect accurate timeline information and another to capture more detatiled information may be a solution.

Another challenge is for \texttt{apptracer} to infer which kernel arguments are pointers to allocations.
It may be possible for \texttt{apptracer} to examine the \tocite{PTX} embedded in most CUDA binaries, and deduce some information about function signatures.

Once CUDA is observed, the next step would be to extend the graph to some view of activity on the CPU as well.
This could be acomplished through a variety of techniques.
The \texttt{LD\_PRELOAD} mechanism could be used to instrument widespread libraries such as \tocite{BLAS} or \tocite{MPI}.
The operating system trace facilities strace~\cite{strace2018} for Linux, DTrace~\cite{dtrace2008} for MacOS, NtTrace~\cite{orr2014nttrace}, or Dr. Memory~\cite{bruening2001design}\cite{drmemory2018} for Windows could be used to track system calls and profile things such as file I/O or network interaction.
Tracking arbitrary function calls within an unmodified application may not be possible in the case of a binary without debug symbols.
Dynamic tracing tools like \tocite{Intel PIN} can insert instrumentation code, but further work would be needed to determine the number of function arguments and their location to recover their values.
For a cluster environment, it may be possible to generate distributed dependence graphs through each node, and then join them together by linking together information recorded about MPI calls.

\section{Combined Modeling}
\label{sec:modeling}

Finally, once system performance modeling is established, and application demands are recorded, joint performance modeling is possible, to tackle questions like
\begin{itemize}
    \item For a particular DVDG, what performance could we expect on a particular system?
    \item For a particular DVDG and a particular budget, what system configuration would perform best?
    \item For a particular DVDG, can the execution be rescheduled on a system to improve performance?
    \item How would changing link parameters or topology on a particular system affect application performance?
\end{itemize}
Answering these questions may require additional effort in open research challenges, such as task scheduling with placement-dependent communication costs, compute kernel performance estimation, and design space exploration.
