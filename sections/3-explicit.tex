\chapter{Explicit Memory Performance}
\label{ch:explicit-char}

\section{CPU / GPU Transfers}

This characterization method outlined in Algorithm~\ref{alg:cuda-h2d} is available for paths terminated by a CPU socket and a CUDA GPU.
A pinned memory allocation on the host and corresponding GPU memory allocation on the GPU are established.
Bandwidth betwene host and devices achieved at various transfer sizes is established by copying various amounts of data between the memory allocations.


\begin{algorithm}[ht]
    \SetAlgoLined
    \KwResult{bandwidth vs. transfer size between socket $s$ and CUDA GPU $g$}
    bind CPU thread to $s$\;
    bind memory allocation to $s$\;
    poolSize $\gets$ $\frac{gpuMemory}{2}$\;
    socketPool $\gets$ \texttt{cudaMallocHost(poolSize)}\;
    gpuPool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    \For{transferSize := $1$ to poolSize}{
        start $\gets$ wall\_time()\;
        \eIf{direction == deviceToHost} {
            \texttt{cudaMemcpy(socketPool, gpuPool, transferSize, cudaMemcpyHostToDevice)}\;
        }{
            \texttt{cudaMemcpy(gpuPool, socketPool, transferSize, cudaMemcpyHostToDevice)}\;
        }
        stop $\gets$ wall\_time()\;
        bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
    }
    \caption{CUDA cudaMemcpy with pinned memory.}
    \label{alg:cuda-h2d}
\end{algorithm}

\section{GPU / GPU Transfers}


\subsubsection{CUDA \texttt{cudaMemcpy} between CUDA GPUs}

This characterization method outlined in Algorithm~\ref{alg:cuda-d2d} is available for paths terminated by a CUDA GPU on both ends.
A GPU memory allocation is established on each GPU.
Bandwidth achieved at various transfer sizes between GPUs is established by copying various amounts of data between the memory allocations.

\begin{algorithm}[ht]
    \SetAlgoLined
    \KwResult{bandwidth vs. transfer size between CUDA GPUs $g_0$ and $g_1$}
    poolSize $\gets$ $\frac{gpuMemory}{2}$\;
    gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    \For{transferSize := $1$ to poolSize}{
        start $\gets$ wall\_time()\;
        \texttt{cudaMemcpy(gpu1Pool, gpu0Pool, transferSize, cudaMemcpyDeviceToDevice)}\;
        stop $\gets$ wall\_time()\;
        bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
    }
    \caption{CUDA cudaMemcpy between CUDA GPUs}
    \label{alg:cuda-d2d}
\end{algorithm}

\subsubsection{CUDA \texttt{memcpyPeer}}

This characterization method outlined in Algorithm~\ref{alg:cuda-d2d} is available for paths terminated by a CUDA GPU on both ends.
A GPU memory allocation is established on each GPU.
Bandwidth between GPUs achieved at various transfer sizes is established by copying various amounts of data between the memory allocations.

\begin{algorithm}[ht]
    \SetAlgoLined
    \KwResult{bandwidth vs. transfer size between CUDA GPUs $g_0$ and $g_1$}
    poolSize $\gets$ $\frac{gpuMemory}{2}$\;
    gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    \For{transferSize := $1$ to poolSize}{
        start $\gets$ wall\_time()\;
        \texttt{cudaMemcpyPeer(gpu1Pool, gpu0Pool, transferSize)}\;
        stop $\gets$ wall\_time()\;
        bandwidth $\gets$ $\frac{copySize}{stop - start}$\;
    }
    \caption{CUDA cudaMemcpy between CUDA GPUs}
    \label{alg:cuda-p2p}
\end{algorithm}


This characterization method outlined in Algorithm~\ref{alg:gpu-gpu-peer} is available for paths terminated by a CUDA GPU on both ends.
If the GPUs support peer access

A GPU memory allocation is established on each GPU.
Bandwidth achieved at various transfer sizes between GPUs is established by copying various amounts of data between the memory allocations.


\begin{algorithm}[ht]
    \SetAlgoLined
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{$b$, whether peer transfers between CUDA GPUs should be enabled}
    \Output{a set of (bandwidth, transfer size) tuples $t$ for transfers between CUDA GPUs $g_0$ and $g_1$}
    $t$ $\gets$ \{\}\;
    poolSize $\gets$ $\frac{gpuMemory}{2}$\;
    gpu0Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    gpu1Pool $\gets$ \texttt{cudaMalloc(poolSize)}\;
    \If{$b$}{
        \texttt{cudaSetDevice($g_0$)\;}
        \texttt{cudaDeviceEnablePeerAccess($g_1$)\;}
        \texttt{cudaSetDevice($g_1$)\;}
        \texttt{cudaDeviceEnablePeerAccess($g_0$)\;}
    }
    \For{transferSize := $1$ to poolSize}{
        start $\gets$ wall\_time()\;
        \texttt{cudaMemcpyPeer(gpu1Pool, gpu0Pool, transferSize)}\;
        stop $\gets$ wall\_time()\;
        t $\gets$ t $\cup$ \{ ( transferSize, $\frac{transferSize}{stop - start}$ ) \}\;
    }
    \caption{Characterizing data transfer performance between CUDA GPUs.}
    \label{alg:gpu-gpu-peer}
\end{algorithm}