\chapter{Application Characterization}
\label{ch:app-char}

Directly analogous to the hardware model, this work models applications as a collection of communicating data sources and sinks (endpoints).
This facilitates the performance modeling described in Chapter~\ref{ch:performance}.
Through it is possible to generate such application models by hand, in general, this approach is not feasible for complicated applications.
Furthermore, applications are frequently updated, which could require a new model.
This work proposes an approach for automatically generating these application models.

\section{Application Model}

The goal of the application record is to track how an application produces, moves, and consumes data.
To that end, the application is broken down into data sources and sinks.
Memory allocations, file creation/access, network activity, and computation kernels all represent possible producers or consumers of data.

Whenever data is moved, it moves from a source to a sink.
Table~\ref{tab:source-sink-example} gives some examples of data movement and the corresponding sources and sinks.

\begin{table}[h]
    \centering
    \caption{\todo{caption}}
    \label{tab:source-sink-example}
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Application Activity} & \textbf{Data Source} & \textbf{Data Sink} \\ \hline
    File system read  & file & calling function \\ \hline
    File system write  & file & calling function \\ \hline
    \textbf{cudaMemcpy()} & allocation & allocation \\ \hline
    function call & pointer arguments & pointer arguments \\ \hline
    \end{tabular}
\end{table}

The application can therefore be represented by a graph $G_a = \{E_a,V_a\}$ where $E_a$ is a set of edges representing data transfer, and $V_a$ is a set of vertices representing data endpoints.

\section{Application Monitoring}

A custom application trace is generated during an application execution to build the application graph.
\todo{apptracer} is built on top of CUDA Profiling Tools Interface(Section~\ref{sec:cupti} (CUPTI) and the Linux \texttt{LD\_PRELOAD}(Section~\ref{sec:ldpreload}) mechanism.

A design goal for \todo{hwcomm-apptracer} is that it should work without modifying or recompiling existing applications.
This ensures that the tool is as accessible to users as possible, and that it can work on closed-source applications.
This limits \todo{hwcomm-apptracer} to profiling techniques that observe how the application interacts with the operating system and runtime libraries.

\todo{hwcomm-tracer} uses CUPTI to capture most CUDA-related information, and LD\_PRELOAD for everything else.
CUPTI allows \todo{apptracer} to provide a callback function that is invoked at every CUDA runtime or driver call, and also allows \todo{apptracer} to collect any performance metrics the GPU exposes.
The callback function records relevant information, including the wall time when the CUDA runtime function is invoked, its arguments, and the device and stream associated with the call.
In this way, detailed information about data transfers from runtime functions can be reconstructed.
For example, allocations from \texttt{cudaMalloc} can be associated with pointers passed to \texttt{cudaMemcpy} to discover data transfers from host to device.

One challenge is to determine the amount of data consumed or produced by arbitrary kernel functions.
Although \todo{hwcomm-apptracer} can record pointer kernel arguments, that alone does not provide information about the size of the data transfer, only the source and destination of that data.

Another challenge is to capture implicit peer access between CUDA devices.
On supported systems, GPUs can directly access data that is resident on other GPUs without making any CUDA runtime calls.

Another challenge is to capture CUDA unified memory page transfers.
On supported systems, GPUs can directly access data that is on the host without making any CUDA runtime calls.
The driver is responsible for transparently moving the data to the GPU.


\todo{hwcomm-tracer} uses LD\_PRELOAD mechanism to intercept known API calls made by the application to shared libraries.
Specifically, LD\_PRELOAD is used to watch for \todo{filesystem access}, CUDA's cuDNN library, CUDA's cuBLAS library, \todo{network access}, and \todo{system memory allocations}.
The various kernel launches and allocations used by cuDNN and cuBLAS are already visible through CUPTI, but the known semantics of the higher-level cuBLAS and cuDNN calls allow for a more detailed application model.


Additionally, conservative data consumption and generation from CUDA kernels can be inferred.
Any arguments that point to previously-recorded allocations are assumed to modify that allocation.
\todo{through monitoring the device metrics, it is possible to understand how much data is read and written through those pointers in aggregate.}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth,draft]{figures/example-app-char.png}
    \caption{\todo{caption}}
    \label{fig:example-app-char}
\end{figure}


\subsection{Communication Link Traffic}
\todo{nothing yet}

\outline{PCI traffic}

\outline{NVLink traffic}

\section{Application Modeling Case Studies}
\subsection{Chai}
\subsection{Caffe}
\subsection{mxnet}
\subsection{Graph Challenge}
\subsection{Lulesh}
\subsection{MLFMM}
\subsection{PlasComCM}