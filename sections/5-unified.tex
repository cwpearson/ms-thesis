\chapter{Unified Memory Performance}
\label{ch:unified}

The unified memory system (Sections~\ref{sec:unified-cc3} and \ref{sec:unified-cc6}) greatly simplifies the programmer interaction with CUDA memories and data transfer.
Data tranfers in the unified memory system are created in two ways:
\begin{itemize}
    \item \textit{coherence} transfers, where data is migrated to ensure that the CPU and GPU have a consistent view of memory.
    \item \textit{prefetch} transfers, where data is moved ahead of time, with the purpose of reducing future access times. 
\end{itemize}
This chapter comprises two sections, detailing performance of CPU-to-GPU transfers (Section~\ref{sec:um-cpu-gpu}), and GPU-GPU transfers (Section~\ref{sec:um-gpu-gpu}).
In each section, transfer bandwidth for coherence and prefetch transfers is examined, as well as page-fault latency for demand transfers, where applicable.

Algorithm~\ref{alg:um-coherence-bw} describes the approach to measure coherence bandwidth between any two CUDA devices in the unified memory system.
First, if the source or destination device is a CPU, the application and worker threads are bound to that CPU.
Then, if the destination is a GPU, that GPU is set as the active device, so future kernel executions will happen on that GPU.
$transfer\_size$ bytes are allocated for the unified memory system, and touched by the CPU to ensure the allocation is actually backed by pages.
Then, the CPU or GPU workload is executed on the destination device.
That workload, described in \texttt{cpu\_write} or \texttt{gpu\_write}, writes a single value to every $stride$ bytes in $transer_size$.
To minimize the extra work, $stride$ is set to the page size on the target system.
This forces the unified memory system to migrate each page to the destination.
\texttt{cudaDeviceSynchronize} is used to ensure the GPU kernel is finished.
The time for the CPU workload or GPU workload and synchronization is recorded, and the minimum time over $numIters$ is returned.

\begin{algorithm}
    \caption{Algorithm to measure CPU-CPU Bandwidth}
    \label{alg:um-coherence-bw}
    \begin{algorithmic}[1]
    \Statex
    \Function{cpu\_write}{$ptr$, $count$, $stride$}
        \State \Return 
    \EndFunction
    \Statex
    \Function{gpu\_write}{$ptr$, $count$, $stride$}
        \State \Return 
    \EndFunction
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_workers$}

        \If{$src$ is CPU}
            \State \texttt{numa\_bind($src$)} \Comment bind thread and children to $src$
        \EndIf
        \If{$dst$ is CPU}
        \State \texttt{numa\_bind($dst$)}
        \EndIf
        \State $ptr \gets$ \texttt{cudaMallocManaged($transfer\_size$)}
        \State \texttt{memset($srcPtr$, 0, $transfer\_size$)} \Comment force pages to be allocated
        

        \State $blockDim \gets [256,1,1]$
        \State $transferStrides \gets floor(\frac{transfer\_size + stride - 1}{stride})$
        \State $gridDim \gets [floor(\frac{transferStride + 255}{256}), 1, 1]$

        \If{$dst$ is GPU}
            \State \texttt{cudaSetDevice($dst$)}
        \EndIf
        \State $elapsed \gets \infty$
        \For{$i$ in $numIters$}

            \State \texttt{cudaMemPrefetchAsync($ptr$, $transfer\_size$, $src$)} \Comment pages on $src$
            \State \texttt{cudaDeviceSynchronize()}

            \State $start \gets$ walltime()
            \If{$dst$ is GPU}
                \State \texttt{gpu\_read\_8<<<$gridDim$,$blockDim$>>>($ptr$, $transfer\_size$, $stride$)}
                \State \texttt{cudaDeviceSynchronize()}
            \Else
                \State \texttt{cpu\_write\_8($ptr$, $transfer\_size$, $stride$)}
            \EndIf
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor
        \State \Return $elapsed$
    \EndFunction

    \end{algorithmic}
\end{algorithm}

\section{CPU to GPU Transfers}
\label{sec:um-cpu-gpu}

\subsection{Coherence vs Prefetch Bandwidth}
This section investigates the performance of CPU-GPU transfers in the unified memory system.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-bw_cpu0-gpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-bw_cpu0-gpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth,draft]{figures/generated/dgx_um-bw_cpu0-gpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-bw_gpu0-cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-bw_gpu0-cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.31\textwidth}
        \includegraphics[width=\textwidth,draft]{figures/generated/dgx_um-bw_gpu0-cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{
        Minsky, local CPU/GPU coherence and prefetch bandwidth.
    }
    \label{}
\end{figure}

\subsection{Page Fault Latency}

\section{GPU / GPU Transfers}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-bw_prefetch_gpu-local-remote.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-bw_coherence_gpu-local-remote.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{
        Minsky, GPU/GPU coherence and prefetch bandwidth
    }
    \label{}
\end{figure}


\section{GPU / GPU Transfers}
\label{sec:um-gpu-gpu}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-bw_prefetch_gpu-local-remote.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-bw_coherence_gpu-local-remote.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{Hal, GPU/GPU coherence and prefetch bandwidth}
    \label{}
\end{figure}


\section{Page Fault Latency}

Unified memory page fault latency is estimated by constructing a linked list in managed memory.
The stride between linked list elements is a large number, to avoid prefetching effects on page faults.
The managed memory allocation is prefetched to the source, and a single-threaded traversal function (shown in Listing~\ref{lst:traversal}) is executed on the destination.
Each access to the list incurs a page fault.
The incremental change in function execution time as the number of strides increases is therefore an approximate measure of the page fault latency.
Table~\ref{tab:page-fault-latency} summarizes the estimated page fault latencies.
Figures~\ref{fig:minsky-page-fault-latency}~and~\ref{fig:hal-page-fault-latency} show the raw traversal times.

\begin{lstlisting}[language=c++, caption=Linked List Traversal, label=lst:traversal]
    __global__ void gpu_traverse(size_t *ptr, const size_t steps)
    {
      size_t next = 0;
      for (int i = 0; i < steps; ++i)
      {
        next = ptr[next];
      }
      ptr[next] = 1;
    }
\end{lstlisting}
    

\begin{table}[h]
	\centering
	\caption[\todo{short}]{Page Fault latency}
	\label{tab:page-fault-latency}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Page Fault} & \textbf{Minsky Latency} & \textbf{Power9 Latency} \\ \hline
		CPU0 $\leftarrow$ GPU0  (local) & 17.9 & 24.4   \\ \hline
        CPU0 $\rightarrow$ GPU0 (local) & 15.2 & 23.6  \\ \hline
        CPU0 $\leftarrow$ GPU2  (remote) & 19.5 & 24.5   \\ \hline
        CPU0 $\rightarrow$ GPU2 (remote) & 16.6 & 23.2  \\ \hline
        GPU0 $\leftarrow$ GPU1  (local) & 26.9 & 35.9   \\ \hline
		GPU0 $\rightarrow$ GPU2 (remote) & 35.6 & 38.5  \\ \hline
	\end{tabular}
\end{table}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-latency_gpu0cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-latency_gpu2cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-latency_gpus.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{ 
        Total kernel execution times on IBM Minsky for linked-list traversal.
        Each additional stride incurs another page fault; the slope of the line estimates the page fault latency.
        For example, GPU0 to CPU0 means the managed allocation is on GPU0, and the traversal function is executing on CPU0.
    }
    \label{fig:minsky-page-fault-latency}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-latency_gpu0cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-latency_gpu2cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-latency_gpus.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{
        Total kernel execution times on IBM Power9 for linked-list traversal.
        Each additional stride incurs another page fault; the slope of the line estimates the page fault latency.
        For example, GPU0 to CPU0 means the managed allocation is on GPU0, and the traversal function is executing on CPU0.
        }
    \label{fig:hal-page-fault-latency}
\end{figure}

