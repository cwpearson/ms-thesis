\chapter{Unified Memory Performance}
\label{ch:unified}

The unified memory system (Sections~\ref{sec:unified-cc3} and \ref{sec:unified-cc6}) greatly simplifies the programmer interaction with CUDA memories and data transfer.
Data tranfers in the unified memory system are created in two ways:
\begin{itemize}
    \item \textit{coherence} transfers, where data is migrated to ensure that the CPU and GPU have a consistent view of memory.
    \item \textit{prefetch} transfers, where data is moved ahead of time, with the purpose of reducing future access times. 
\end{itemize}
This chapter comprises two sections, detailing performance of CPU-to-GPU transfers (Section~\ref{sec:um-cpu-gpu}), and GPU-GPU transfers (Section~\ref{sec:um-gpu-gpu}).
In each section, transfer bandwidth for coherence and prefetch transfers is examined, as well as page-fault latency for demand transfers, where applicable.

Algorithm~\ref{alg:um-coherence-bw} describes the approach to measure coherence bandwidth between any two CUDA devices in the unified memory system.
First, if the source or destination device is a CPU, the application and worker threads are bound to that CPU.
Then, if the destination is a GPU, that GPU is set as the active device, so future kernel executions will happen on that GPU.
$transfer\_size$ bytes are allocated for the unified memory system, and touched by the CPU to ensure the allocation is actually backed by pages.
Then, the CPU or GPU workload is executed on the destination device.
That workload, described in \texttt{cpu\_write} or \texttt{gpu\_write}, writes a single value to every $stride$ bytes in $transer_size$.
To minimize the extra work, $stride$ is set to the page size on the target system.
This forces the unified memory system to migrate each page to the destination.
\texttt{cudaDeviceSynchronize} is used to ensure the GPU kernel is finished.
The time for the CPU workload or GPU workload and synchronization is recorded, and the minimum time over $numIters$ is returned.

\begin{lstlisting}[language=c++, caption=CPU Write Function, label=lst:cpu-write]
    template <typename data_type>
    void cpu_write(data_type *ptr, const size_t count, const size_t stride)
    {
    
      const size_t numElems = count / sizeof(data_type);
      const size_t elemsPerStride = stride / sizeof(data_type);
    
    #pragma omp parallel for schedule(static)
      for (size_t i = 0; i < numElems; i += elemsPerStride)
      {
        ptr[i] = i * 31ul + 7ul;
      }
    }
\end{lstlisting}

\begin{lstlisting}[language=c++, caption=GPU Write Function, label=lst:cpu-write]
    template <typename data_type, bool NOOP = false>
    __global__ void gpu_write(data_type *ptr, const size_t count, const size_t stride)
    {
      if (NOOP)
      {
        return;
      }
    
      // global ID
      const size_t gx = blockIdx.x * blockDim.x + threadIdx.x;
      // lane ID 0-31
      const size_t lx = gx & 31;
      // warp ID
      size_t wx = gx / 32;
      // number of warps in the grid
      const size_t numWarps = (gridDim.x * blockDim.x + 32 - 1) / 32;
      // number of strides in N bytes
      const size_t numStrides = count / stride;
      const size_t numData = count / sizeof(data_type);
      // number of data_types in each
      const size_t dataPerStride = stride / sizeof(data_type);
    
      if (0 == lx)
      {
    
        for (; wx < numStrides; wx += numWarps)
        {
          const size_t id = wx * dataPerStride;
          if (id < numData)
          {
            ptr[id] = id * 31ul + 7ul;
          }
        }
      }
    }
\end{lstlisting}

\begin{algorithm}
    \caption{CPU-GPU Coherence Bandwidth}
    \label{alg:um-coherence-bw}
    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_workers$}

        \If{$src$ is CPU}
            \State \texttt{numa\_bind($src$)} \Comment bind thread and children to $src$
        \EndIf
        \If{$dst$ is CPU}
        \State \texttt{numa\_bind($dst$)}
        \EndIf
        \State $ptr \gets$ \texttt{cudaMallocManaged($transfer\_size$)}
        \State \texttt{memset($srcPtr$, 0, $transfer\_size$)} \Comment force pages to be allocated
        

        \State $blockDim \gets [256,1,1]$
        \State $transferStrides \gets floor(\frac{transfer\_size + stride - 1}{stride})$
        \State $gridDim \gets [floor(\frac{transferStride + 255}{256}), 1, 1]$

        \If{$dst$ is GPU}
            \State \texttt{cudaSetDevice($dst$)}
        \EndIf
        \State $elapsed \gets \infty$
        \For{$i$ in $numIters$}

            \State \texttt{cudaMemPrefetchAsync($ptr$, $transfer\_size$, $src$)} \Comment pages on $src$
            \State \texttt{cudaDeviceSynchronize()}

            \State $start \gets$ walltime()
            \If{$dst$ is GPU}
                \State \texttt{gpu\_read\_8<<<$gridDim$,$blockDim$>>>($ptr$, $transfer\_size$, $stride$)}
                \State \texttt{cudaDeviceSynchronize()}
            \Else
                \State \texttt{cpu\_write\_8($ptr$, $transfer\_size$, $stride$)}
            \EndIf
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor
        \State \Return $elapsed$
    \EndFunction

    \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:um-coherence-bw} describes the approach to measure prefetch bandwidth between two devices participating in the unified memory system.
First, if $src$ is a CPU, the application thread is bound to that CPU, or if $src$ is a GPU, that GPU is made active.
$transfer\_size$ bytes are then allocated through \texttt{cudaMallocManaged}, and are associated with the source device thanks to the previous step.
The benchmark loop is then
\begin{itemize}
    \item \texttt{cudaMemPrefetchAsync} is used to move the pages to the source device (if they are not already there).
    \item \texttt{cudaDeviceSynchronize} ensures the asynchronous prefetch completes.
    \item the wall time is recorded
    \item \texttt{cudaMemPrefetchAsync} to move the allocation to $dst$.
    \item \texttt{cudaDeviceSynchronize} ensures the asynchronous prefetch completes.
    \item the end wall time is recorded
\end{itemize}
The loop is run $num\_iters$ times, and the fastest observed time is reported.

\begin{algorithm}
    \caption{CPU-GPU Prefetch Bandwidth}
    \label{alg:um-prefetch-bw}
    \begin{algorithmic}[1]
    \Statex
    \Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_workers$}

        \If{$src$ is CPU}
            \State \texttt{cpu\_bind($src$)}
        \Else
            \State \texttt{cudaSetDevice($src$)}
        \EndIf

        \State $ptr \gets$ \texttt{cudaMallocManaged($transfer\_size$)}
        \State \texttt{memset($srcPtr$, 0, $transfer\_size$)} \Comment force pages to be allocated

        \State $elapsed \gets \infty$
        \For{$i$ in $numIters$}

            \State \texttt{cudaMemPrefetchAsync($ptr$, $transfer\_size$, $src$)} \Comment pages on $src$
            \State \texttt{cudaDeviceSynchronize()}

            \State $start \gets$ walltime()
            \State \texttt{cudaMemPrefetchAsync($ptr$, $transfer\_size$, $dst$)} \Comment pages on $dst$
            \State \texttt{cudaDeviceSynchronize()}
            \State $end \gets$ walltime()
            \State $elapsed \gets$ min($elapsed$, $end-start$)
        \EndFor
        \State \Return $elapsed$
    \EndFunction

    \end{algorithmic}
\end{algorithm}

\section{CPU to GPU Transfers}
\label{sec:um-cpu-gpu}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_prefetch_cpu-gpu.pdf}
        \caption{}
        \label{fig:um-prefetch-s822lc-cpu-gpu}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_coherence_cpu-gpu.pdf}
        \caption{}
        \label{fig:um-coherence-s822lc-cpu-gpu}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_prefetch_cpu-gpu.pdf}
        \caption{}
        \label{fig:um-prefetch-ac922-cpu-gpu}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_coherence_cpu-gpu.pdf}
        \caption{}
        \label{fig:um-coherence-ac922-cpu-gpu}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth,draft]{figures/generated/dgx_prefetch_cpu-gpu.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth,draft]{figures/generated/dgx_coherence_cpu-gpu.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{
        CPU-GPU Coherence and Prefetch Bandwidth
    }
    \label{fig:um-cpu-gpu}
\end{figure}



\subsection{Coherence vs Prefetch Bandwidth}
This section investigates the performance of CPU-GPU transfers in the unified memory system.
Figure~\ref{fig:um-cpu-gpu} shows transfer bandwidth vs transfer size between CPU0 and GPU0 on the three systems.

\subsection{Coherence Affinity}

Device affinity can affect the observed coherence transfer bandwidth.
Table~\ref{tab:um-coherence-affinity} shows some cases where affinity affects transfer bandwidth, with Figure~\ref{fig:um-coherence-affinity} showing the corresponding results.
On S822LC and AC922, GPU-GPU coherence traffic is affected by affinity, with local transfers exhibiting nearly double the bandwidth for intermediate transfer sizes.
On AC922, GPU-CPU bandwidth for large transfers is also larger for local CPUs.

\todo{any speculation about why?}

\begin{table}[ht]
    \centering
    \caption[\todo{short}]{Effect of Device Affinity on Coherence Bandwidth}
    \label{tab:um-coherence-affinity}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind} & \textbf{S822LC} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    CPU $\rightarrow$     GPU & $\times$                                                & $\times$                                               & (Fig.~\ref{}) \\ \hline
    CPU $\leftarrow$      GPU & $\times$                                                & \checkmark (Fig.~\ref{fig:um-coherence-ac922-cpu-gpu}) & (Fig.~\ref{}) \\ \hline
    GPU $\leftrightarrow$ GPU & \checkmark (Fig.~\ref{fig:um-coherence-s822lc-gpu-gpu}) & \checkmark (Fig.~\ref{fig:um-coherence-ac922-gpu-gpu}) & (Fig.~\ref{}) \\ \hline
    \end{tabular}
\end{table}


\subsection{Prefetch Affinity}

Device affinity can affect the observed prefetch bandwidth.
Table~\ref{tab:um-prefetch-affinity} shows some cases where affinity affects prefetch transfer bandwidth, with Figure~\ref{fig:um-prefetch-affinity} showing the corresponding results.
Figures \ref{fig:um-prefetch-s822lc-affinity-gpu-gpu} and \ref{fig:um-prefetch-ac922-affinity-gpu-gpu} show that GPU-GPU affinity has a very large effect for prefetch bandwidth, with local GPUs having double or more the prefetch performance of remote GPUs.
Figures \ref{fig:um-prefetch-s822lc-affinity-cpu-gpu} and \ref{fig:um-prefetch-ac922-affinity-cpu-gpu} show that CPU to GPU transfers are also affected, to a lesser degree.

\todo{speculation why?}

\begin{table}[ht]
    \centering
    \caption[\todo{short}]{Effect of Device Affinity on Prefetch Bandwidth}
    \label{tab:um-prefetch-affinity}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Transfer Kind} & \textbf{S822LC} & \textbf{AC922} & \textbf{DGX-1} \\ \hline 
    CPU $\rightarrow$     GPU \checkmark (Fig.~\ref{fig:um-prefetch-s822lc-cpu-gpu}) & \checkmark (Fig.~\ref{fig:um-prefetch-ac922-cpu-gpu}) & & \\ \hline
    CPU $\leftarrow$      GPU $\times$                                                        & $\times$ & & \\ \hline
    GPU $\leftrightarrow$ GPU \checkmark (Fig.~\ref{fig:um-prefetch-s822lc-gpu-gpu}) & \checkmark (Fig.~\ref{fig:um-prefetch-ac922-gpu-gpu}) & & \\ \hline
    \end{tabular}
\end{table}

\section{Coherence Direction}

\section{GPU / GPU Transfers}
\label{sec:um-gpu-gpu}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-bw_prefetch_gpu-local-remote.pdf}
        \caption{}
        \label{fig:um-prefetch-s822lc-gpu-gpu}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-bw_coherence_gpu-local-remote.pdf}
        \caption{}
        \label{fig:um-coherence-s822lc-gpu-gpu}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-bw_prefetch_gpu-local-remote.pdf}
        \caption{}
        \label{fig:um-prefetch-ac922-gpu-gpu}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-bw_coherence_gpu-local-remote.pdf}
        \caption{}
        \label{fig:um-coherence-ac922-gpu-gpu}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth,draft]{figures/generated/dgx_prefetch_gpu-gpu.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth,draft]{figures/generated/dgx_coherence_gpu-gpu.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{
        GPU-GPU Coherence and Prefetch Bandwidth
    }
    \label{fig:um-coherence-gpu-gpu}
\end{figure}




\section{Page Fault Latency}

Unified memory page fault latency is estimated by constructing a linked list in managed memory.
The stride between linked list elements is a large number, to avoid prefetching effects on page faults.
The managed memory allocation is prefetched to the source, and a single-threaded traversal function (shown in Listing~\ref{lst:traversal}) is executed on the destination.
Each access to the list incurs a page fault.
The incremental change in function execution time as the number of strides increases is therefore an approximate measure of the page fault latency.
Table~\ref{tab:page-fault-latency} summarizes the estimated page fault latencies.
Figures~\ref{fig:minsky-page-fault-latency}~and~\ref{fig:hal-page-fault-latency} show the raw traversal times.

\begin{lstlisting}[language=c++, caption=Linked List Traversal, label=lst:traversal]
    __global__ void gpu_traverse(size_t *ptr, const size_t steps)
    {
      size_t next = 0;
      for (int i = 0; i < steps; ++i)
      {
        next = ptr[next];
      }
      ptr[next] = 1;
    }
\end{lstlisting}

\begin{algorithm}
    \caption{CPU-GPU Coherence Bandwidth}
    \label{alg:um-coherence-bw}
    \begin{algorithmic}[1]
    \Statex
    \Function{latency}{$ptr$, $count$, $stride$}

    \EndFunction

    \end{algorithmic}
\end{algorithm}


\begin{table}[h]
	\centering
	\caption[\todo{short}]{Page Fault latency}
	\label{tab:page-fault-latency}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Page Fault} & \textbf{Minsky Latency} & \textbf{Power9 Latency} \\ \hline
		CPU0 $\leftarrow$ GPU0  (local) & 17.9 & 24.4   \\ \hline
        CPU0 $\rightarrow$ GPU0 (local) & 15.2 & 23.6  \\ \hline
        CPU0 $\leftarrow$ GPU2  (remote) & 19.5 & 24.5   \\ \hline
        CPU0 $\rightarrow$ GPU2 (remote) & 16.6 & 23.2  \\ \hline
        GPU0 $\leftarrow$ GPU1  (local) & 26.9 & 35.9   \\ \hline
		GPU0 $\rightarrow$ GPU2 (remote) & 35.6 & 38.5  \\ \hline
	\end{tabular}
\end{table}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-latency_gpu0cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-latency_gpu2cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/m2_um-latency_gpus.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{ 
        Total kernel execution times on IBM Minsky for linked-list traversal.
        Each additional stride incurs another page fault; the slope of the line estimates the page fault latency.
        For example, GPU0 to CPU0 means the managed allocation is on GPU0, and the traversal function is executing on CPU0.
    }
    \label{fig:minsky-page-fault-latency}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-latency_gpu0cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-latency_gpu2cpu0.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/generated/hal_um-latency_gpus.pdf}
        \caption{}
        \label{}
    \end{subfigure}
    \caption[\todo{short}]{
        Total kernel execution times on IBM Power9 for linked-list traversal.
        Each additional stride incurs another page fault; the slope of the line estimates the page fault latency.
        For example, GPU0 to CPU0 means the managed allocation is on GPU0, and the traversal function is executing on CPU0.
        }
    \label{fig:hal-page-fault-latency}
\end{figure}

