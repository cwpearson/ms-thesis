\chapter{Unified Memory Performance}
\label{ch:unified}

The unified memory system (Sections~\ref{sec:unified-cc3} and \ref{sec:unified-cc6}) greatly simplifies the programmer interaction with CUDA memories and data transfer.
Data tranfers in the unified memory system are created in two ways:
\begin{itemize}
	\item \textit{coherence} transfers, where data is migrated to ensure that the CPU and GPU have a consistent view of memory.
	\item \textit{prefetch} transfers, where data is moved ahead of time, with the purpose of reducing future access times. 
\end{itemize}
This chapter comprises two sections, detailing performance of CPU-to-GPU transfers (Section~\ref{sec:um-cpu-gpu}), and GPU-GPU transfers (Section~\ref{sec:um-gpu-gpu}).
In each section, transfer bandwidth for coherence and prefetch transfers is examined, as well as page-fault latency for demand transfers, where applicable.

Algorithm~\ref{alg:um-coherence-bw} describes the approach to measure coherence bandwidth between any two CUDA devices in the unified memory system.
First, if the source or destination device is a CPU, the application and worker threads are bound to that CPU.
Then, if the destination is a GPU, that GPU is set as the active device, so future kernel executions will happen on that GPU.
$transfer\_size$ bytes are allocated for the unified memory system, and touched by the CPU to ensure the allocation is actually backed by pages.
Then, the CPU or GPU workload is executed on the destination device.
That workload, shown in Listing~\ref{lst:cpu-write} and \ref{lst:gpu-write}, writes a single value to every $stride$ bytes in $transer_size$.
To minimize the extra work, $stride$ is set to the page size on the target system.
This causes a single access to each page, and forces the unified memory system to migrate each page to the destination.
\texttt{cudaDeviceSynchronize} is used to ensure the GPU kernel is finished.
The time for the CPU workload or GPU workload and synchronization is recorded, and the minimum time over $numIters$ is returned.

Listing~\ref{lst:cpu-write} shows a simple function to write \texttt{sizeof(data\_type)} bytes to every \texttt{stride} byte in a \texttt{count}-byte region starting at \texttt{ptr}.
OpenMP is used to statically partition the write loop, with each thread taking a contiguous chunk the loop iteration space.
This ensures that the CPU is running enough threads to maximize demand on the unified memory system.

\begin{lstlisting}[language=c++, caption=CPU Write Function, label=lst:cpu-write]
    template <typename data_type>
    void cpu_write(data_type *ptr, const size_t count,
                   const size_t stride)
    {
    
      const size_t numElems = count / sizeof(data_type);
      const size_t elemsPerStride = stride / sizeof(data_type);
    
    #pragma omp parallel for schedule(static)
      for (size_t i = 0; i < numElems; i += elemsPerStride)
      {
        ptr[i] = i * 31ul + 7ul;
      }
    }
\end{lstlisting}

Listing~\ref{lst:gpu-write} shows CUDA kernel to write \texttt{sizeof(data\_type)} bytes to every \texttt{stride} byte in a \texttt{count}-byte region starting at \texttt{ptr}.
It assigns consecutive warps in the grid to handle consecutive writes, with a single thread from each warp doing a write.
If there are not sufficient warps in the grid to cover all writes, the grid loops over the required writes.
Since warps execute in lockstep, this ensures the broadest simultaneous demands on the unified memory system without reduntant work within a warp.

\begin{lstlisting}[language=c++, caption=GPU Write Function, label=lst:cpu-write]
    template <typename data_type>
    __global__ void gpu_write(data_type *ptr,
                              const size_t count,
                              const size_t stride)
    {
    
      size_t gx = 
        blockIdx.x * blockDim.x + threadIdx.x;
      size_t lx = gx & 31;
      size_t wx = gx / 32;
      size_t numWarps = 
        (gridDim.x * blockDim.x + 32 - 1) / 32;
      size_t numStrides = count / stride;
      size_t numData = count / sizeof(data_type);
      size_t dataPerStride = 
        stride / sizeof(data_type);
    
      if (0 == lx)
      {
        for (; wx < numStrides; wx += numWarps)
        {
          const size_t id = wx * dataPerStride;
          if (id < numData)
          {
            ptr[id] = id * 31ul + 7ul;
          }
        }
      }
    }
\end{lstlisting}

\begin{algorithm}
	\caption{CPU-GPU Coherence Bandwidth}
	\label{alg:um-coherence-bw}
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_workers$}
				
		\If{$src$ is CPU}
		\State \texttt{numa\_bind($src$)} \Comment bind thread and children to $src$
		\EndIf
		\If{$dst$ is CPU}
		\State \texttt{numa\_bind($dst$)}
		\EndIf
		\State $ptr \gets$ \texttt{cudaMallocManaged($transfer\_size$)}
		\State \texttt{memset($srcPtr$, 0, $transfer\_size$)} \Comment force pages to be allocated
				        
				
		\State $blockDim \gets [256,1,1]$
		\State $transferStrides \gets floor(\frac{transfer\_size + stride - 1}{stride})$
		\State $gridDim \gets [floor(\frac{transferStride + 255}{256}), 1, 1]$
				
		\If{$dst$ is GPU}
		\State \texttt{cudaSetDevice($dst$)}
		\EndIf
		\State $elapsed \gets \infty$
		\For{$i$ in $numIters$}
				
		\State \texttt{cudaMemPrefetchAsync($ptr$, $transfer\_size$, $src$)} \Comment pages on $src$
		\State \texttt{cudaDeviceSynchronize()}
				
		\State $start \gets$ walltime()
		\If{$dst$ is GPU}
		\State \texttt{gpu\_read\_8<<<$gridDim$,$blockDim$>>>($ptr$, $transfer\_size$, $stride$)}
		\State \texttt{cudaDeviceSynchronize()}
		\Else
		\State \texttt{cpu\_write\_8($ptr$, $transfer\_size$, $stride$)}
		\EndIf
		\State $end \gets$ walltime()
		\State $elapsed \gets$ min($elapsed$, $end-start$)
		\EndFor
		\State \Return $elapsed$
		\EndFunction
				
	\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:um-coherence-bw} describes the approach to measure prefetch bandwidth between two devices participating in the unified memory system.
First, if $src$ is a CPU, the application thread is bound to that CPU, or if $src$ is a GPU, that GPU is made active.
$transfer\_size$ bytes are then allocated through \texttt{cudaMallocManaged}, and are associated with the source device thanks to the previous step.
The benchmark loop is then
\begin{itemize}
	\item \texttt{cudaMemPrefetchAsync} is used to move the pages to the source device (if they are not already there).
	\item \texttt{cudaDeviceSynchronize} ensures the asynchronous prefetch completes.
	\item the wall time is recorded
	\item \texttt{cudaMemPrefetchAsync} to move the allocation to $dst$.
	\item \texttt{cudaDeviceSynchronize} ensures the asynchronous prefetch completes.
	\item the end wall time is recorded
\end{itemize}
The loop is run $num\_iters$ times, and the fastest observed time is reported.

\begin{algorithm}
	\caption{CPU-GPU Prefetch Bandwidth}
	\label{alg:um-prefetch-bw}
	\begin{algorithmic}[1]
		\Statex
		\Function{Bandwidth}{$dst$, $src$, $transfer\_size$, $num\_workers$}
				
		\If{$src$ is CPU}
		\State \texttt{cpu\_bind($src$)}
		\Else
		\State \texttt{cudaSetDevice($src$)}
		\EndIf
				
		\State $ptr \gets$ \texttt{cudaMallocManaged($transfer\_size$)}
		\State \texttt{memset($srcPtr$, 0, $transfer\_size$)} \Comment force pages to be allocated
				
		\State $elapsed \gets \infty$
		\For{$i$ in $numIters$}
				
		\State \texttt{cudaMemPrefetchAsync($ptr$, $transfer\_size$, $src$)} \Comment pages on $src$
		\State \texttt{cudaDeviceSynchronize()}
				
		\State $start \gets$ walltime()
		\State \texttt{cudaMemPrefetchAsync($ptr$, $transfer\_size$, $dst$)} \Comment pages on $dst$
		\State \texttt{cudaDeviceSynchronize()}
		\State $end \gets$ walltime()
		\State $elapsed \gets$ min($elapsed$, $end-start$)
		\EndFor
		\State \Return $elapsed$
		\EndFunction
				
	\end{algorithmic}
\end{algorithm}

\section{Coherence vs Prefetch Bandwidth}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/m2_prefetch_cpu-gpu.pdf}
		\caption{}
		\label{fig:um-prefetch-s822lc-cpu-gpu}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/m2_coherence_cpu-gpu.pdf}
		\caption{}
		\label{fig:um-coherence-s822lc-cpu-gpu}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_prefetch_cpu-gpu.pdf}
		\caption{}
		\label{fig:um-prefetch-ac922-cpu-gpu}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_coherence_cpu-gpu.pdf}
		\caption{}
		\label{fig:um-coherence-ac922-cpu-gpu}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_prefetch_cpu-gpu.pdf}
		\caption{}
		\label{fig:um-prefetch-dgx-cpu-gpu}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_coherence_cpu-gpu.pdf}
		\caption{}
		\label{fig:um-coherence-dgx-cpu-gpu}
	\end{subfigure}
	\caption[\todo{short}]{
		CPU-GPU Coherence and Prefetch Bandwidth
	}
	\label{fig:um-cpu-gpu}
\end{figure}

The following figures compare prefetch and coherence bandwidth for CPU/GPU transfers;
\begin{itemize}
	\item Figures \ref{fig:um-prefetch-s822lc-cpu-gpu} and \ref{fig:um-coherence-s822lc-cpu-gpu} on S822LC,
	\item Figures \ref{fig:um-prefetch-ac922-cpu-gpu} and \ref{fig:um-coherence-ac922-cpu-gpu}   on AC922,
	\item Figures \ref{fig:um-prefetch-dgx-cpu-gpu} and \ref{fig:um-coherence-dgx-cpu-gpu}       on DGX,
\end{itemize}
In general, the CPU/GPU prefetch bandwidth saturates at a higher fraction of the theoretical underlying link bandwidth.
This is likely because the DMA associated with a prefetch is a simpler and higher-performance operation than the on-demand migration of pages.
Coherence migrations can provide higher transfer bandwidth at small and intermediate transfer sizes.
This suggests that the overhead of the prefetch transfer is higher than the coherence transfer.




\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/m2_prefetch_gpu-gpu.pdf}
		\caption{}
		\label{fig:um-prefetch-s822lc-gpu-gpu}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/m2_coherence_gpu-gpu.pdf}
		\caption{}
		\label{fig:um-coherence-s822lc-gpu-gpu}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_prefetch_gpu-gpu.pdf}
		\caption{}
		\label{fig:um-prefetch-ac922-gpu-gpu}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_coherence_gpu-gpu.pdf}
		\caption{}
		\label{fig:um-coherence-ac922-gpu-gpu}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_prefetch_gpu-gpu.pdf}
		\caption{}
		\label{fig:um-prefetch-dgx-gpu-gpu}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_coherence_gpu-gpu.pdf}
		\caption{}
		\label{fig:um-coherence-dgx-gpu-gpu}
	\end{subfigure}
	\caption[\todo{short}]{
		GPU-GPU Coherence and Prefetch Bandwidth
	}
	\label{fig:um-coherence-gpu-gpu}
\end{figure}


The following figures compare prefetch and coherence bandwidth for GPU/GPU transfers;
\begin{itemize}
	\item Figures \ref{fig:um-prefetch-s822lc-gpu-gpu} and \ref{fig:um-coherence-s822lc-gpu-gpu} on S822LC,
	\item Figures \ref{fig:um-prefetch-ac922-gpu-gpu} and \ref{fig:um-coherence-ac922-gpu-gpu}   on AC922,
	\item Figures \ref{fig:um-prefetch-dgx-gpu-gpu} and \ref{fig:um-coherence-dgx-gpu-gpu}       on DGX,
\end{itemize}
The GPU/GPU prefetch bandwidth also saturates at a higher bandwidth than the coherence transfers, and like the CPU/GPU transfers, the coherence bandwidth is higher for small and intermediate transfers.

\subsection{Effect of Device Affinity on Coherence Bandwidth}

Device affinity can affect the observed coherence transfer bandwidth.
Table~\ref{tab:um-coherence-affinity} shows some cases where affinity affects transfer bandwidth.
Figures \ref{fig:um-coherence-s822lc-cpu-gpu} and ~\ref{fig:um-coherence-ac922-cpu-gpu} shows no affinity effect for CPU to GPU coherence traffic.
On AC922, the same is not true for GPU-to-CPU traffic, where local transfers saturate above 40GB/s while remote transfers are limited to around 30 GB/s.
The CPUs on AC922 may be able to generate more coherence requests, or the system may be able to serve them faster, and ultimately the performance is limited by the slower CPU-CPU X bus.

Figures \ref{fig:um-coherence-s822lc-gpu-gpu} and ~\ref{fig:um-coherence-ac922-gpu-gpu} show that affinity influences the performance of GPU-GPU transfers, with local transfers being faster than remote transfers.
The performance difference is much greater for intermediate transfers than large transfers.
\todo{Can we figure out why?}

\begin{table}[ht]
	\centering
	\caption[\todo{short}]{Observations of Device Affinity affecting Coherence Bandwidth}
	\label{tab:um-coherence-affinity}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Transfer Kind}    & \textbf{S822LC}                                         & \textbf{AC922}                                         & \textbf{DGX-1}                            \\ \hline 
		CPU $\rightarrow$     GPU & $\times$   (Fig.~\ref{fig:um-coherence-s822lc-cpu-gpu}) & $\times$   (Fig.~\ref{fig:um-coherence-ac922-cpu-gpu}) & (Fig.~\ref{fig:um-coherence-dgx-cpu-gpu}) \\ \hline
		CPU $\leftarrow$      GPU & $\times$   (Fig.~\ref{fig:um-coherence-s822lc-cpu-gpu}) & \checkmark (Fig.~\ref{fig:um-coherence-ac922-cpu-gpu}) & (Fig.~\ref{fig:um-coherence-dgx-cpu-gpu}) \\ \hline
		GPU $\leftrightarrow$ GPU & \checkmark (Fig.~\ref{fig:um-coherence-s822lc-gpu-gpu}) & \checkmark (Fig.~\ref{fig:um-coherence-ac922-gpu-gpu}) & (Fig.~\ref{fig:um-coherence-dgx-gpu-gpu}) \\ \hline
	\end{tabular}
\end{table}


\subsection{Effect of Device Affinity on Prefetch Bandwidth}

Device affinity can affect the observed prefetch bandwidth.
Table~\ref{tab:um-prefetch-affinity} shows some cases where affinity affects prefetch transfer bandwidth.
Figures \ref{fig:um-prefetch-s822lc-cpu-gpu} and \ref{fig:um-prefetch-ac922-cpu-gpu} show that CPU-GPU prefetch bandwidth is influenced by device affinity.
For S822LC and AC922, the prefetch bandwidth from GPU2 to CPU0 is substantially higher than that of GPU0 to CPU0, even though GPU2 is remote from CPU0 and must traverse more links.
The CPU-to-GPU transfers show more expected behavior, with local transfer bandwidth being higher than remote.
On DGX-1, there is no effect, probably due to the low PCIe bandwidth hiding other factors that might introduce unepxected behavior.

Figures \ref{fig:um-prefetch-s822lc-gpu-gpu}, \ref{fig:um-prefetch-ac922-gpu-gpu}, and \ref{fig:um-prefetch-dgx-gpu-gpu} show that GPU-GPU affinity has a very large effect for prefetch bandwidth.
On all systems, local GPUs can prefetch data much faster than remote GPUs.
On S822LC, local GPUs enjoy $162\%$ of the transfer bandwidth of their remote companions.
On DGX-1, it is $170\%$, and on AC922, that number balloons to $230\%$.
Generally, local GPU-GPU transfers are able to saturate around 75\% of the theoretical underlying link bandwidth.

\begin{table}[ht]
	\centering
	\caption[\todo{short}]{Observations of Device Affinity affecting Prefetch Bandwidth}
	\label{tab:um-prefetch-affinity}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Transfer Kind}    & \textbf{S822LC}                                        & \textbf{AC922}                                        & \textbf{DGX-1}                                      \\ \hline 
		CPU $\rightarrow$     GPU & \checkmark (Fig.~\ref{fig:um-prefetch-s822lc-cpu-gpu}) & \checkmark (Fig.~\ref{fig:um-prefetch-ac922-cpu-gpu}) & $\times$   (Fig.~\ref{fig:um-prefetch-dgx-cpu-gpu}) \\ \hline
		CPU $\leftarrow$      GPU & \checkmark (Fig.~\ref{fig:um-prefetch-s822lc-cpu-gpu}) & \checkmark (Fig.~\ref{fig:um-prefetch-ac922-cpu-gpu}) & $\times$   (Fig.~\ref{fig:um-prefetch-dgx-cpu-gpu}) \\ \hline
		GPU $\leftrightarrow$ GPU & \checkmark (Fig.~\ref{fig:um-prefetch-s822lc-gpu-gpu}) & \checkmark (Fig.~\ref{fig:um-prefetch-ac922-gpu-gpu}) & \checkmark (Fig.~\ref{fig:um-prefetch-dgx-gpu-gpu}) \\ \hline
	\end{tabular}
\end{table}

\section{Observed Anisotropy in Coherence Bandwidth}

Table~\ref{tab:um-coherence-anisotropy} describes instances of observed anisotropy in coherence bandwidth.
Figure~\ref{fig:um-coherence-s822lc-cpu-gpu} shows that CPU-to-GPU coherence transfers are uniformly faster than GPU-to-CPU coherence transfers on S822LC.
On AC922, Figure~\ref{fig:um-coherence-ac922-cpu-gpu} shows that for small transfers, CPU-to-GPU transfers are faster, but the corresponding GPU-to-CPU transfers become larger for large transfers.
The Power9 CPUs may be able to generate more coherence requests once the OpenMP overhead of the benchmark is sufficiently amortized for large transfers.

Figure~\ref{fig:um-coherence-s822lc-gpu-gpu} shows that GPU-GPU S822LC transfers do exhibit anisotropy in large remote coherence transfers, but no other cases.
On AC922, Figure~\ref{fig:um-coherence-ac922-gpu-gpu} shows there is no anisotropic behavior in any GPU-GPU coherence transfers.

\begin{table}[ht]
	\centering
	\caption[\todo{short}]{Observed Coherence Bandwidth Anisotropy}
	\label{tab:um-coherence-anisotropy}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Transfer Kind}             & \textbf{S822LC}                                         & \textbf{AC922}                                         & \textbf{DGX-1}                            \\ \hline 
		CPU $\leftrightarrow$ GPU (local)  & \checkmark (Fig.~\ref{fig:um-coherence-s822lc-cpu-gpu}) & \checkmark (Fig.~\ref{fig:um-coherence-ac922-cpu-gpu}) & (Fig.~\ref{fig:um-coherence-dgx-cpu-gpu}) \\ \hline
		CPU $\leftrightarrow$ GPU (remote) & \checkmark (Fig.~\ref{fig:um-coherence-s822lc-cpu-gpu}) & \checkmark (Fig.~\ref{fig:um-coherence-ac922-cpu-gpu}) & (Fig.~\ref{fig:um-coherence-dgx-cpu-gpu}) \\ \hline
		GPU $\leftrightarrow$ GPU (local)  & $\times$   (Fig.~\ref{fig:um-coherence-s822lc-gpu-gpu}) & $\times$   (Fig.~\ref{fig:um-coherence-ac922-gpu-gpu}) & (Fig.~\ref{fig:um-coherence-dgx-gpu-gpu}) \\ \hline
		GPU $\leftrightarrow$ GPU (remote) & \checkmark (Fig.~\ref{fig:um-coherence-s822lc-gpu-gpu}) & $\times$   (Fig.~\ref{fig:um-coherence-ac922-gpu-gpu}) & (Fig.~\ref{fig:um-coherence-dgx-gpu-gpu}) \\ \hline
	\end{tabular}
\end{table}

\section{Observed Anisotropy in Prefetch Bandwidth}

Table~\ref{tab:um-prefetch-anisotropy} describes instances of observed anisotropy in prefetch bandwidth.

Figure~\ref{fig:um-prefetch-s822lc-cpu-gpu} and \ref{fig:um-prefetch-ac922-cpu-gpu} show CPU-GPU anisotropy on S822LC and AC922.
For local transfers, the CPU-GPU direction is faster.
For remote transfers, the GPU-to-CPU direction is faster.
Figure~\ref{fig:um-prefetch-dgx-cpu-gpu} shows negligable anisotropy on DGX-1, though the GPU-to-CPU direction is always faster.
The relatively low PCIe bandwidth may be hiding other influences.

Fig.~\ref{fig:um-prefetch-s822lc-gpu-gpu} shows anisotropy in remote GPU-GPU prefetch transfers on S822LC.
Neither of the other systems demonstrate any GPU-GPU prefetch anisotropy.

\begin{table}[ht]
	\centering
	\caption[\todo{short}]{Observed Prefetch Bandwidth Anisotropy}
	\label{tab:um-prefetch-anisotropy}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Transfer Kind}             & \textbf{S822LC}                                         & \textbf{AC922}                                         & \textbf{DGX-1}                            \\ \hline 
		CPU $\leftrightarrow$ GPU (local)  & \checkmark (Fig.~\ref{fig:um-prefetch-s822lc-cpu-gpu}) & \checkmark (Fig.~\ref{fig:um-prefetch-ac922-cpu-gpu}) & $\times$ (Fig.~\ref{fig:um-prefetch-dgx-cpu-gpu}) \\ \hline
		CPU $\leftrightarrow$ GPU (remote) & \checkmark (Fig.~\ref{fig:um-prefetch-s822lc-cpu-gpu}) & \checkmark (Fig.~\ref{fig:um-prefetch-ac922-cpu-gpu}) & $\times$ (Fig.~\ref{fig:um-prefetch-dgx-cpu-gpu}) \\ \hline
		GPU $\leftrightarrow$ GPU (local)  & $\times$   (Fig.~\ref{fig:um-prefetch-s822lc-gpu-gpu}) & $\times$   (Fig.~\ref{fig:um-prefetch-ac922-gpu-gpu}) & $\times$ (Fig.~\ref{fig:um-prefetch-dgx-gpu-gpu}) \\ \hline
		GPU $\leftrightarrow$ GPU (remote) & \checkmark (Fig.~\ref{fig:um-prefetch-s822lc-gpu-gpu}) & $\times$   (Fig.~\ref{fig:um-prefetch-ac922-gpu-gpu}) & $\times$ (Fig.~\ref{fig:um-prefetch-dgx-gpu-gpu}) \\ \hline
	\end{tabular}
\end{table}

\section{Page Fault Latency}

Unified memory page fault latency is estimated by constructing a linked list in managed memory and traversing it.
Algorithm~\ref{alg:um-latency} summarizes the measurement routine.
The stride between linked list elements is a large number, to avoid prefetching effects on page faults.
The managed memory allocation is prefetched to the source, and a single-threaded traversal function (shown in Listings \ref{lst:gpu-traversal} and \ref{lst:cpu-traversal}) is executed on the destination.
Each access to the list incurs a page fault.
The incremental change in function execution time as the number of strides increases is therefore an approximate measure of the page fault latency.


\begin{lstlisting}[language=c++, caption=GPU Linked List Traversal Kernel, label=lst:gpu-traversal]
__global__ void gpu_traverse(size_t *ptr, const size_t steps)
{
  size_t next = 0;
  for (int i = 0; i < steps; ++i)
  {
    next = ptr[next];
  }
  ptr[next] = 1;
}
\end{lstlisting}

\begin{lstlisting}[language=c++, caption=CPU Linked List Traversal Function, label=lst:cpu-traversal]
void cpu_traverse(size_t *ptr, const size_t steps)
{
  size_t next = 0;
  for (size_t i = 0; i < steps; ++i)
  {
    next = ptr[next];
  }
  ptr[next] = 1;
}
\end{lstlisting}

\todo{fix stride}
\todo{fix count}
\begin{algorithm}
	\caption{CPU-GPU Coherence Bandwidth}
	\label{alg:um-latency}
	\begin{algorithmic}[1]
		\Statex
		\Function{latency}{$dst$, $src$, $ptr$, $count$, $stride$}
				
        \If{$src$ is CPU}
            \State \texttt{bind\_cpu($src$)}
        \EndIf
        \If{$dst$ is CPU}
            \State \texttt{bind\_cpu($dst$)}
        \EndIf

        \State $blockDim \gets [1,1,1]$ 
        \State $gridDim \gets [1,1,1]$
        \State $stride \gets PAGE\_SIZE \times 2$
        \State $count \gets sizeof(size\_t) \times (steps + 1) \times stride$

        \State $ptr \gets \texttt{cudaMallocManaged(count))}$

        \For{ $i$ in $steps$}
            \State ptr[$i$] $\gets$ $(i+1)\times stride$
        \EndFor
        \State \texttt{cudaDeviceSynchronize()}

        \State $elapsed \gets \infty$
        \For{ $i$ in $numIters$}
            \State $\texttt{cudaMemPrefetchAsync(ptr, count, src))}$
            \State $\texttt{cudaDeviceSynchronize()}$

            \State $start \gets \texttt{wall\_time()}$

            \If{$dst$ is GPU}
                \State \texttt{gpu\_traverse<<<1,1>>>($ptr$, $steps$)}
            \Else
                \State \texttt{cpu\_traverse($ptr$, $steps$)}
            \EndIf

            \State $end \gets \texttt{wall\_time()}$
        \EndFor

		\EndFunction
				
	\end{algorithmic}
\end{algorithm}

Table~\ref{tab:page-fault-latency} summarizes the estimated page fault latencies.
Figure~\ref{fig:coherence-page-fault-latency} shows the raw traversal times.
There is no substantial difference in page fault latencies for different CPUs, so values for transfers to CPU0 are shown.

All three systems follow the same behavior, with GPU/GPU page fault latencies being higher than CPU/GPU page fault latencies.
The bottom section of Table~\ref{tab:page-fault-latency} shows that the cost of moving the page is a small portion of the observed transfer time.
These calculated values do not include link latencies, and represent a lower bound.
On S822LC and AC922, the CPU/GPU page fault latency is nearly identical in both directions.


\begin{table}[ht]
	\centering
	\caption[\todo{short}]{
        Page Fault latencies.
        Above the double-line are empirically-measured values.
        Below the double-line are computed values, for the system page size and system configuration.
        The value is computed by taking the link bandwidth and dividing it by the page size.
        e.g., NVLink for S822LC refers to two-lane NVLink 1.0.
    }
	\label{tab:page-fault-latency}
	\begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Page Fault}                  & \multicolumn{3}{c|}{\textbf{Latency ($\mu$s)}}    \\ \hline
		\textbf{Type}                        & \textbf{S822LC} & \textbf{AC922} & \textbf{DGX-1} \\ \hline
		CPU  $\leftarrow$  GPU               & 13.7            & 24.4           & 26.6           \\ \hline
		CPU  $\rightarrow$ GPU               & 16.3            & 22.3           & 32.2           \\ \hline
		GPU0 $\leftrightarrow$ GPU1 (local)  & 27.6            & 36.0           & 37.1           \\ \hline
        GPU0 $\leftrightarrow$ GPU2 (remote) & 28.3            & 36.5           & 50.3           \\ \hline
        \hline
        One Page, PCIe 3.0 x16               & N/A             & N/A            & 0.25           \\ \hline
        One Page, NVLink                     & 1.6             & 0.9            & 0.4            \\ \hline
	\end{tabular}
\end{table}


\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/m2_coherence_latency.pdf}
		\caption{}
		\label{}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/hal_coherence_latency.pdf}
		\caption{}
		\label{}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.31\textwidth}
		\includegraphics[width=\textwidth]{figures/generated/dgx_coherence_latency.pdf}
		\caption{}
		\label{}
	\end{subfigure}
	\caption[Page Fault Latencies for S822LC, AC922, and DGX-1]{
        Linked-list traversal time vs number of strides for S822LC, AC922, and DGX-1.
        For each system, CPU-to-GPU, GPU-to-CPU, and remote/local GPU-to-GPU times are shown.
        Each stride incurs a page fault, so the slope of these lines estimate the page fault cost.
    }
	\label{fig:coherence-page-fault-latency}
\end{figure}

\section{Quirks}

Coherence bandwidth is higher for small / intermediate transfers.

Remote GPU-CPU prefetch bandwidth higher than local on IBM systmes.

GPU-CPU prefetch on DGX-1 no effect of affinity.

shows anisotropy in remote GPU-GPU prefetch transfers on S822LC (no hter systems)