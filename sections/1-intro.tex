
\chapter{Introduction}

\outline{
    The first chapter should introduce the problem studied and describe the main results obtained in the thesis.
    In order to provide guidance to the reader, the first chapter should briefly describe the organization of the rest of the thesis.
    The first chapter can also give the background of previous work on the subject and the method used in attacking the problem.
}

With the end of Dennard scaling, computer architects have sough to satisfy demand for increasing performance by providing specialized hardware accelerators tuned to computation with particular characteristics.
Perhaps the most successful examples of this trend is the uniform adoption and IEEE 754 specification of floating-point hardware, and widespread adoption of graphics processing units (GPUs) for more general data-parallel compute tasks.
With the success of GPUs as a template, architects are moving forward with a wide variety of specialized processors, such as
SIMD extensions~\cite{intel2017avx, matz2013sse, arm2017neon},
AI accelerators (Google tensor processing unit~\cite{jouppi2017datacenter}, Huawei Neural Processing Unit~\cite{huawei2017kirin} IBM neuromorphic chips~\cite{merolla2014million}, Intel Nervana~\cite{intel2017nervana}),
motion coprocessors\footnote{M7,M8 separate chip, M9-M11 on-die}~\todo{cite apple},
field-programmable gate arrays (FPGAs)~\todo{cite Intel, Xilinx}
network processors~\todo{cite intel ixp, netronome},
digital signal processors~\todo{cite google pixel2},
vision processing units (Eyeriss~\cite{chen2017eyeriss} ~\todo{cite movidius, microsoft, mobilieye}),
and many others.

Thanks to \todo{fundamental physical challenges}, accelerators require memories close to their high-performance specialized compute elements.
Without these memories to ``feed the beast'', the performance of the accelerator would be severely limited by its link to the host memory.
Moving data into accelerator memory to support high-performance execution is a first-order design consideration for any accelerated application.
This work lays the foundation for mitigating the data-placement challenge and reducing developer effort.

\todo{cognitive computing apps as a motivating example}

The data-placement and data-movement challenge is exacerbated by the growing demand for data-driven applications.
Analytics and neural-network applications ingest huge amounts of data, and even if the amount of compute per data element is small, the aggregate required computation can be commensurately large.
That motivates developers to use accelerators for these applications.
To achieve high performance on accelerators, developers must marshal and coordinate their data movement and computation.

This work describes an automated approach to analyzing the performance of data movement in systems that use programmable accelerators.
Broadly, the approach consists of two components
\begin{enumerate}
    \item A system characterization tool, which enumerates and characterizes the performance of logical communication paths (Chapter~\ref{ch:sys-char}).
    \item An application characterization tool, which profiles unmodifed applications to record how they inteact with the system (Chapter~\ref{ch:app-char}).
\end{enumerate}
These tools are examined in the context of heterogeneous systems made of CPUs and NVIDIA GPUs and machine-learning workloads due to the maturity of that hardware/software ecosystem.
Together, these tools provide a foundation for automating analysis of the relationship between system design and application performance (Chapter~\ref{ch:modeling}).

The rest of this document is organized as follows:
Chapter~\ref{ch:background} describes background information on NVIDIA GPUs, the CUDA programming system, interconnects, and 
Chapter~\ref{ch:sys-char} describes the hardware system characterization approach;
Chapter~\ref{ch:app-char} describes the application characterization approach;
Chapter~\ref{ch:modeling} describes a methodology for combining the system and application characterization to understand application performance;
Chapter~\ref{ch:related} discusses related work;
and finally, Chapter~\ref{ch:conclusion} concludes.