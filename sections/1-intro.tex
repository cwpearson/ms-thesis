\chapter{Introduction}
\label{ch:introduction}

%The first chapter should introduce the problem studied and describe the main results obtained in the thesis.
%In order to provide guidance to the reader, the first chapter should briefly describe the organization of the rest of the thesis.
%The first chapter can also give the background of previous work on the subject and the method used in attacking the problem.

With the end of Dennard scaling, computer architects have sough to satisfy demand for increasing performance by providing specialized hardware accelerators tuned to computation with particular characteristics.
Perhaps the most successful example of this trend is the widespread adoption of graphics processing units (GPUs) for more general data-parallel compute tasks.
With the success of GPUs as a template, architects are moving forward with a wide variety of accelerators, such as
SIMD extensions~\cite{intel2017avx, matz2013sse, arm2017neon},
AI accelerators (Google tensor processing unit~\cite{jouppi2017datacenter}, Huawei Neural Processing Unit~\cite{huawei2017kirin} IBM neuromorphic chips~\cite{merolla2014million}, Intel Nervana~\cite{intel2017nervana}),
motion coprocessors (Apple M7-M8 \footnote{M7,M8 separate chip, M9-M11 on-die}~\todo{cite apple},
field-programmable gate arrays (FPGAs)~\todo{cite Intel, Xilinx}
network processors~\todo{cite intel ixp, netronome},
digital signal processors~\todo{cite google pixel2},
vision processing units (Eyeriss~\cite{chen2017eyeriss} ~\todo{cite movidius, microsoft, mobilieye}),
and many others.

The enormous compute capability of accelerators demands high-bandiwdth access to data to ``feed the beast.''
Without this access, the perform, ance potential of the accelerator is largely wasted waiting for data.
The trend of \textit{integration} (also motivated by reduction of total system cost) where semiconductor die-size or power limits allow, has provided one approach to solving this problem.
By integrating an accelerator onto the same die as the CPU, the accelerator more easily gets high-bandwidth low-power access to data shared with the CPU.
For accelerators with high memory demands, however, the system memory DRAM bandwidth may ultimately limit performance.

The second approach is to provide accelerators with their own high-performance memory.
Unfortunately, managing this memory then falls upon runtime systems or the application developer, and moving data into accelerator memory to support high-performance execution is a first-order design consideration for any accelerated application.
The data-placement and data-movement challenge is exacerbated by the growing demand for data-driven applications.
Analytics and neural-network applications ingest huge amounts of data, and even if the amount of compute per data element is small, the aggregate required computation can be commensurately large.
That motivates developers to use accelerators for these applications.
To achieve high performance on accelerators, developers must marshal and coordinate their data movement and computation.

This work describes an automated approach to analyzing the performance of data movement in systems that use discrete accelerators with local memories.
This approach consists of a
Broadly, the approach consists of two components: a system characterization tool, which enumerates and characterizes the performance of logical communication paths, and an application characterization tool, which profiles unmodifed applications to record how they inteact with the system.
These tools are examined in the context of heterogeneous systems made of CPUs and NVIDIA GPUs and machine-learning workloads due to the maturity of that hardware/software ecosystem.
Together, these tools provide a foundation for automating analysis of the relationship between system design and application performance.

To that end, this work makes the following contributions:
\begin{itemize}
 \item a detailed communication performance characterization of three multi-CPU/multi-GPU systems designed for data-driven applications
 \item an approach for combining this characterization with an application characterization to understand application performance on modern accelerator-heavy systems.
\end{itemize}

The rest of this document is organized as follows:
Chapter~\ref{ch:background} describes background information on the studied computers, the CUDA programming system, Linux NUMA system, OpenMP, and profiling tools proposed for the application characterization;
Chapter~\ref{ch:sys-char} describes the hardware system characterization approach;
Chapter~\ref{ch:explicit} describes performance characterization of explicit CUDA memory management.
Chapter~\ref{ch:unified} describes performance characterization of unified memory in CUDA systems.
Chapter~\ref{ch:future} describes future work of application characterization and combined modeling.
Chapter~\ref{ch:related} discusses related work;
and finally, Chapter~\ref{ch:conclusion} concludes.