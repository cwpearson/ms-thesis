
\chapter{Introduction}

\outline{
    The first chapter should introduce the problem studied and describe the main results obtained in the thesis.
    In order to provide guidance to the reader, the first chapter should briefly describe the organization of the rest of the thesis.
    The first chapter can also give the background of previous work on the subject and the method used in attacking the problem.
}

With the end of Dennard scaling, computer architects have sough to satisfy demand for increasing performance by providing specialized hardware accelerators tuned to computation with particular characteristics.
Perhaps the most successful examples of this trend is the uniform adoption and IEEE 754 specification of floating-point hardware, and widespread adoption of graphics processing units (GPUs) for more general data-parallel compute tasks.
With the success of GPUs as a template, architects are moving forward with a wide variety of specialized processors, such as
SIMD extensions~\cite{intel2017avx, matz2013sse, arm2017neon},
AI accelerators (Google tensor processing unit~\cite{jouppi2017datacenter}, Huawei Neural Processing Unit~\cite{huawei2017kirin} IBM neuromorphic chips~\cite{merolla2014million}, Intel Nervana~\cite{intel2017nervana}),
motion coprocessors\footnote{M7,M8 separate chip, M9-M11 on-die}~\todo{cite apple},
field-programmable gate arrays (FPGAs)~\todo{cite Intel, Xilinx}
network processors~\todo{cite intel ixp, netronome},
digital signal processors~\todo{cite google pixel2},
vision processing units (Eyeriss~\cite{chen2017eyeriss} ~\todo{cite movidius, microsoft, mobilieye}),
and many others.



Thanks to \todo{fundamental physical challenges}, accelerators require memories close to their high-performance specialized compute elements.
Without these memories to ``feed the beast'', the performance of the accelerator would be severely limited by its link to the host memory.
For discrete-type accelerators, moving data into accelerator memory to support high-performance execution is a first-order developer challenge.
This work lays the foundation for mitigating the data-placement challenge and reducing developer effort.

\todo{cognitive computing apps as a motivating example}

The data-placement and data-movement challenge is exacerbated by the growing demand for data-driven applications.
Analytics and neural-network applications ingest huge amounts of data, and even if the amount of compute per data element is small, the aggregate required computation can be commensurately large.
That motivates developers to use accelerators for these applications.
To achieve high performance on accelerators, developers must marshal and coordinate their data movement and computation.

This document describes an automated approach to modeling the performance of data movement in systems that use programmable accelerators.
The performance of logical communication paths available to the application is determined by the hardware and software present in the system, and can be empirically measured and understood by enumerating the system topology 

Any automated solution to this problem will require a detailed characterization of the underlying heterogeneous system and its data movement capabilities.
This work demonstrates a systematic heterogeneous system communication characterization.
Furthermore, it presents a systematic method for analyzing applications to determine how their performance is influenced by heterogeneous systems.

The rest of this document is organized as follows:
chapter 2 describes background and related work;
chapter 3 describes the hardware system characterization approach;
chapter 4 describes the application characterization approach;
chapter 5 describes the methodology for combining the system and application characterization to understand application performance;
chapter 6 discusses related work;
and finally, chapter 7 concludes.
